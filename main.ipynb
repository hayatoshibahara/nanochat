{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c39d5d",
   "metadata": {},
   "source": [
    "# nanochat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5884fe2",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa359a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: maturin in /opt/miniconda/envs/py312/lib/python3.12/site-packages (1.9.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "ğŸ“¦ Including license file `LICENSE`\n",
      "ğŸ”— Found pyo3 bindings\n",
      "ğŸ Found CPython 3.12 at /opt/miniconda/envs/py312/bin/python\n",
      "ğŸ“¡ Using build options bindings from pyproject.toml\n",
      "Ignoring torch: markers 'extra == \"cpu\"' don't match your environment\n",
      "Ignoring torch: markers 'extra == \"gpu\"' don't match your environment\n",
      "Requirement already satisfied: datasets>=4.0.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: fastapi>=0.117.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.121.0)\n",
      "Requirement already satisfied: files-to-prompt>=0.6 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.6)\n",
      "Requirement already satisfied: numpy==1.26.4 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: psutil>=7.1.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (7.1.3)\n",
      "Requirement already satisfied: regex>=2025.9.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (2025.11.3)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (80.9.0)\n",
      "Requirement already satisfied: tiktoken>=0.11.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: tokenizers>=0.22.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.22.1)\n",
      "Requirement already satisfied: torch>=2.8.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: uvicorn>=0.36.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.38.0)\n",
      "Requirement already satisfied: wandb>=0.21.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.22.3)\n",
      "Requirement already satisfied: filelock in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from datasets>=4.0.0) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (3.13.2)\n",
      "Requirement already satisfied: anyio in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=4.0.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=4.0.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=4.0.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=4.0.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=4.0.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=4.0.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=4.0.0) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from fastapi>=0.117.1) (0.49.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from fastapi>=0.117.1) (2.12.4)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from fastapi>=0.117.1) (0.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.117.1) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets>=4.0.0) (1.3.1)\n",
      "Requirement already satisfied: click in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from files-to-prompt>=0.6) (8.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from torch>=2.8.0) (3.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from wandb>=0.21.3) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from wandb>=0.21.3) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from wandb>=0.21.3) (6.33.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from wandb>=0.21.3) (2.43.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=4.0.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=4.0.0) (2.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=4.0.0) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.21.3) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.21.3) (5.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from jinja2->torch>=2.8.0) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pandas->datasets>=4.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pandas->datasets>=4.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pandas->datasets>=4.0.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=4.0.0) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[1m\u001b[92m    Finished\u001b[0m `release` profile [optimized] target(s) in 0.06s\n",
      "ğŸ“¦ Built wheel for CPython 3.12 to /tmp/.tmppJz61d/nanochat-0.1.0-cp312-cp312-linux_x86_64.whl\n",
      "âš ï¸ Warning: pip raised a warning running [\"-m\", \"pip\", \"--disable-pip-version-check\", \"install\", \"--no-deps\", \"--force-reinstall\", \"/tmp/.tmppJz61d/nanochat-0.1.0-cp312-cp312-linux_x86_64.whl\"]:\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "âœï¸ Setting installed package as editable\n",
      "ğŸ›  Installed nanochat-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "%pip install maturin\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"nanochat\"):\n",
    "    !git clone https://github.com/karpathy/nanochat\n",
    "\n",
    "try:\n",
    "    # Google Colabã®å ´åˆ\n",
    "    from google.colab import userdata\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Rustã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && . \"$HOME/.cargo/env\"\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # maturin build --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸwhlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # pip install nanochat/rustbpe/target/wheels/*.whl\n",
    "\n",
    "    if not os.path.exists(\"nanochat/rustbpe/target\"):\n",
    "        raise FileNotFoundError(\"rustbpeã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "except ImportError:\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
    "    !maturin develop --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "import rustbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb7b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "import logging as logging\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = 'ğŸŸ¦'\n",
    "        case logging.INFO:\n",
    "            level = 'ğŸŸ©'\n",
    "        case logging.WARNING:\n",
    "            level = 'ğŸŸ¨'\n",
    "        case logging.ERROR:\n",
    "            level = 'ğŸŸ¥'\n",
    "        case logging.CRITICAL:\n",
    "            level = 'ğŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f8a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/nanochat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "\n",
    "def get_base_dir():\n",
    "    \"\"\"\n",
    "    ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
    "    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€~/.cache/nanochat\n",
    "    NANOCHAT_BASE_DIRç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½\n",
    "\n",
    "    Returns:\n",
    "        str: ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"NANOCHAT_BASE_DIR\"):\n",
    "        nanochat_dir = os.environ.get(\"NANOCHAT_BASE_DIR\")\n",
    "    else:\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        cache_dir = os.path.join(home_dir, \".cache\")\n",
    "        nanochat_dir = os.path.join(cache_dir, \"nanochat\")\n",
    "    os.makedirs(nanochat_dir, exist_ok=True)\n",
    "    return nanochat_dir\n",
    "\n",
    "get_base_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00863e",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3e368",
   "metadata": {},
   "source": [
    "äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã¨ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "[karpathy/fineweb-edu-100b-shuffle][2]ã‚’ä½¿ç”¨:\n",
    "\n",
    "- [FineWeb-EDUãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ][1]ã‚’ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆæ–­ç‰‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã«å†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "- å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¯25ä¸‡æ–‡å­—ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã€ç´„100MB\n",
    "- åˆè¨ˆã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯1822\n",
    "- depth=20ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯240ã§ã€ç´„24GB\n",
    "\n",
    "[1]: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "[2]: https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4501892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®URL\n",
    "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¤§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "# fineweb-edu-100b-shuffleã®å ´åˆã€1823å€‹\n",
    "# æœ€å¾Œã®ã‚·ãƒ£ãƒ¼ãƒ‰ã¯shard_01822.parquet\n",
    "MAX_SHARD = 1822\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®æ•°\n",
    "num_files = 8\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "num_workers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6e117d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.cache/nanochat/base_data'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "\n",
    "base_dir = get_base_dir()\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2749ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shard_00000.parquet'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
    "index_to_filename = lambda index: f\"shard_{index:05d}.parquet\"\n",
    "\n",
    "index_to_filename(0) # shard_00000.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07b4a901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /root/.cache/nanochat/base_data/shard_00000.parquet (already exists)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_single_file(index):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã€ã„ãã¤ã‹ã®ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ã\n",
    "    ãƒãƒƒã‚¯ã‚ªãƒ•ã¨ã¯ã€å¤±æ•—ã—ãŸå ´åˆã«å†è©¦è¡Œã™ã‚‹å‰ã«å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™æ‰‹æ³•\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the local filepath for this file and skip if it already exists\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"Skipping {filepath} (already exists)\")\n",
    "        return True\n",
    "\n",
    "    # Construct the remote URL for this file\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    print(f\"Downloading {filename}...\")\n",
    "\n",
    "    # Download with retries\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            # Write to temporary file first\n",
    "            temp_path = filepath + f\".tmp\"\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            # Move temp file to final location\n",
    "            os.rename(temp_path, filepath)\n",
    "            print(f\"Successfully downloaded {filename}\")\n",
    "            return True\n",
    "\n",
    "        except (requests.RequestException, IOError) as e:\n",
    "            print(f\"Attempt {attempt}/{max_attempts} failed for {filename}: {e}\")\n",
    "            # Clean up any partial files\n",
    "            for path in [filepath + f\".tmp\", filepath]:\n",
    "                if os.path.exists(path):\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "            # Try a few times with exponential backoff: 2^attempt seconds\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt\n",
    "                print(f\"Waiting {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to download {filename} after {max_attempts} attempts\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "download_single_file(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec50c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 8 shards using 4 workers...\n",
      "Target directory: /root/.cache/nanochat/base_data\n"
     ]
    }
   ],
   "source": [
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹IDã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "\n",
    "num = MAX_SHARD + 1 if num_files == -1 else min(num_files, MAX_SHARD + 1)\n",
    "ids_to_download = list(range(num))\n",
    "print(f\"Downloading {len(ids_to_download)} shards using {num_workers} workers...\")\n",
    "print(f\"Target directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a9cace6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping /root/.cache/nanochat/base_data/shard_00000.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00001.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00002.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00003.parquet (already exists)\n",
      "\n",
      "\n",
      "\n",
      "Skipping /root/.cache/nanochat/base_data/shard_00004.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00005.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00006.parquet (already exists)Skipping /root/.cache/nanochat/base_data/shard_00007.parquet (already exists)\n",
      "\n",
      "\n",
      "\n",
      "Done! Downloaded: 8/8 shards to /root/.cache/nanochat/base_data\n"
     ]
    }
   ],
   "source": [
    "# ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "with Pool(processes=num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "successful = sum(1 for success in results if success)\n",
    "print(f\"Done! Downloaded: {successful}/{len(ids_to_download)} shards to {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcf16b",
   "metadata": {},
   "source": [
    "## RustBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d486f",
   "metadata": {},
   "source": [
    "RustBPETokenizerã¯ã€rustbpeã‚’ãƒ©ãƒƒãƒ—ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e88a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rustbpe\n",
    "import tiktoken\n",
    "import os\n",
    "import copy\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb6407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\n",
    "    # every document begins with the Beginning of Sequence (BOS) token that delimits documents\n",
    "    \"<|bos|>\",\n",
    "    # tokens below are only used during finetuning to render Conversations into token ids\n",
    "    \"<|user_start|>\", # user messages\n",
    "    \"<|user_end|>\",\n",
    "    \"<|assistant_start|>\", # assistant messages\n",
    "    \"<|assistant_end|>\",\n",
    "    \"<|python_start|>\", # assistant invokes python REPL tool\n",
    "    \"<|python_end|>\",\n",
    "    \"<|output_start|>\", # python REPL outputs back to assistant\n",
    "    \"<|output_end|>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c66fda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this split pattern deviates from GPT-4 in that we use \\p{N}{1,2} instead of \\p{N}{1,3}\n",
    "# I did this because I didn't want to \"waste\" too many tokens on numbers for smaller vocab sizes.\n",
    "# I haven't validated that this is actually a good idea, TODO.\n",
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b40d80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RustBPETokenizer:\n",
    "    \"\"\"Light wrapper around tiktoken (for efficient inference) but train with rustbpe\"\"\"\n",
    "\n",
    "    def __init__(self, enc, bos_token):\n",
    "        self.enc = enc\n",
    "        self.bos_token_id = self.encode_special(bos_token)\n",
    "\n",
    "    @classmethod\n",
    "    def train_from_iterator(cls, text_iterator, vocab_size):\n",
    "        # 1) train using rustbpe\n",
    "        tokenizer = rustbpe.Tokenizer()\n",
    "        # the special tokens are inserted later in __init__, we don't train them here\n",
    "        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n",
    "        assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n",
    "        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n",
    "        # 2) construct the associated tiktoken encoding for inference\n",
    "        pattern = tokenizer.get_pattern()\n",
    "        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n",
    "        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n",
    "        tokens_offset = len(mergeable_ranks)\n",
    "        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n",
    "        enc = tiktoken.Encoding(\n",
    "            name=\"rustbpe\",\n",
    "            pat_str=pattern,\n",
    "            mergeable_ranks=mergeable_ranks, # dict[bytes, int] (token bytes -> merge priority rank)\n",
    "            special_tokens=special_tokens, # dict[str, int] (special token name -> token id)\n",
    "        )\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, tokenizer_dir):\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            enc = pickle.load(f)\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tiktoken_name):\n",
    "        # https://github.com/openai/tiktoken/blob/eedc8563/tiktoken_ext/openai_public.py\n",
    "        enc = tiktoken.get_encoding(tiktoken_name)\n",
    "        # tiktoken calls the special document delimiter token \"<|endoftext|>\"\n",
    "        # yes this is confusing because this token is almost always PREPENDED to the beginning of the document\n",
    "        # it most often is used to signal the start of a new sequence to the LLM during inference etc.\n",
    "        # so in nanoChat we always use \"<|bos|>\" short for \"beginning of sequence\", but historically it is often called \"<|endoftext|>\".\n",
    "        return cls(enc, \"<|endoftext|>\")\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.enc.n_vocab\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        return self.enc.special_tokens_set\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        return self.enc.decode([id])\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def encode_special(self, text):\n",
    "        return self.enc.encode_single_token(text)\n",
    "\n",
    "    def get_bos_token_id(self):\n",
    "        return self.bos_token_id\n",
    "\n",
    "    def encode(self, text, prepend=None, append=None, num_threads=8):\n",
    "        # text can be either a string or a list of strings\n",
    "\n",
    "        if prepend is not None:\n",
    "            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n",
    "        if append is not None:\n",
    "            append_id = append if isinstance(append, int) else self.encode_special(append)\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            ids = self.enc.encode_ordinary(text)\n",
    "            if prepend is not None:\n",
    "                ids.insert(0, prepend_id) # TODO: slightly inefficient here? :( hmm\n",
    "            if append is not None:\n",
    "                ids.append(append_id)\n",
    "        elif isinstance(text, list):\n",
    "            ids = self.enc.encode_ordinary_batch(text, num_threads=num_threads)\n",
    "            if prepend is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.insert(0, prepend_id) # TODO: same\n",
    "            if append is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.append(append_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.enc.decode(ids)\n",
    "\n",
    "    def save(self, tokenizer_dir):\n",
    "        # save the encoding object to disk\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(self.enc, f)\n",
    "        print(f\"Saved tokenizer encoding to {pickle_path}\")\n",
    "\n",
    "    def render_conversation(self, conversation, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        Tokenize a single Chat conversation (which we call a \"doc\" or \"document\" here).\n",
    "        Returns:\n",
    "        - ids: list[int] is a list of token ids of this rendered conversation\n",
    "        - mask: list[int] of same length, mask = 1 for tokens that the Assistant is expected to train on.\n",
    "        \"\"\"\n",
    "        # ids, masks that we will return and a helper function to help build them up.\n",
    "        ids, mask = [], []\n",
    "        def add_tokens(token_ids, mask_val):\n",
    "            if isinstance(token_ids, int):\n",
    "                token_ids = [token_ids]\n",
    "            ids.extend(token_ids)\n",
    "            mask.extend([mask_val] * len(token_ids))\n",
    "\n",
    "        # sometimes the first message is a system message...\n",
    "        # => just merge it with the second (user) message\n",
    "        if conversation[\"messages\"][0][\"role\"] == \"system\":\n",
    "            # some conversation surgery is necessary here for now...\n",
    "            conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "            messages = conversation[\"messages\"]\n",
    "            assert messages[1][\"role\"] == \"user\", \"System message must be followed by a user message\"\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        else:\n",
    "            messages = conversation[\"messages\"]\n",
    "        assert len(messages) >= 1, f\"Conversation has less than 1 message: {messages}\"\n",
    "\n",
    "        # fetch all the special tokens we need\n",
    "        bos = self.get_bos_token_id()\n",
    "        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n",
    "        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n",
    "        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n",
    "        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n",
    "\n",
    "        # now we can tokenize the conversation\n",
    "        add_tokens(bos, 0)\n",
    "        for i, message in enumerate(messages):\n",
    "\n",
    "            # some sanity checking here around assumptions, to prevent footguns\n",
    "            must_be_from = \"user\" if i % 2 == 0 else \"assistant\"\n",
    "            assert message[\"role\"] == must_be_from, f\"Message {i} is from {message['role']} but should be from {must_be_from}\"\n",
    "\n",
    "            # content can be either a simple string or a list of parts (e.g. containing tool calls)\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            if message[\"role\"] == \"user\":\n",
    "                assert isinstance(content, str), \"User messages are simply expected to be strings\"\n",
    "                value_ids = self.encode(content)\n",
    "                add_tokens(user_start, 0)\n",
    "                add_tokens(value_ids, 0)\n",
    "                add_tokens(user_end, 0)\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "                add_tokens(assistant_start, 0)\n",
    "                if isinstance(content, str):\n",
    "                    # simple string => simply add the tokens\n",
    "                    value_ids = self.encode(content)\n",
    "                    add_tokens(value_ids, 1)\n",
    "                elif isinstance(content, list):\n",
    "                    for part in content:\n",
    "                        value_ids = self.encode(part[\"text\"])\n",
    "                        if part[\"type\"] == \"text\":\n",
    "                            # string part => simply add the tokens\n",
    "                            add_tokens(value_ids, 1)\n",
    "                        elif part[\"type\"] == \"python\":\n",
    "                            # python tool call => add the tokens inside <|python_start|> and <|python_end|>\n",
    "                            add_tokens(python_start, 1)\n",
    "                            add_tokens(value_ids, 1)\n",
    "                            add_tokens(python_end, 1)\n",
    "                        elif part[\"type\"] == \"python_output\":\n",
    "                            # python output => add the tokens inside <|output_start|> and <|output_end|>\n",
    "                            # none of these tokens are supervised because the tokens come from Python at test time\n",
    "                            add_tokens(output_start, 0)\n",
    "                            add_tokens(value_ids, 0)\n",
    "                            add_tokens(output_end, 0)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown part type: {part['type']}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown content type: {type(content)}\")\n",
    "                add_tokens(assistant_end, 1)\n",
    "\n",
    "        # truncate to max_tokens tokens MAX (helps prevent OOMs)\n",
    "        ids = ids[:max_tokens]\n",
    "        mask = mask[:max_tokens]\n",
    "        return ids, mask\n",
    "\n",
    "    def visualize_tokenization(self, ids, mask, with_token_id=False):\n",
    "        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n",
    "        RED = '\\033[91m'\n",
    "        GREEN = '\\033[92m'\n",
    "        RESET = '\\033[0m'\n",
    "        GRAY = '\\033[90m'\n",
    "        tokens = []\n",
    "        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n",
    "            token_str = self.decode([token_id])\n",
    "            color = GREEN if mask_val == 1 else RED\n",
    "            tokens.append(f\"{color}{token_str}{RESET}\")\n",
    "            if with_token_id:\n",
    "                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n",
    "        return '|'.join(tokens)\n",
    "\n",
    "    def render_for_completion(self, conversation):\n",
    "        \"\"\"\n",
    "        Used during Reinforcement Learning. In that setting, we want to\n",
    "        render the conversation priming the Assistant for a completion.\n",
    "        Unlike the Chat SFT case, we don't need to return the mask.\n",
    "        \"\"\"\n",
    "        # We have some surgery to do: we need to pop the last message (of the Assistant)\n",
    "        conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "        messages = conversation[\"messages\"]\n",
    "        assert messages[-1][\"role\"] == \"assistant\", \"Last message must be from the Assistant\"\n",
    "        messages.pop() # remove the last message (of the Assistant) inplace\n",
    "\n",
    "        # Now tokenize the conversation\n",
    "        ids, mask = self.render_conversation(conversation)\n",
    "\n",
    "        # Finally, to prime the Assistant for a completion, append the Assistant start token\n",
    "        assistant_start = self.encode_special(\"<|assistant_start|>\")\n",
    "        ids.append(assistant_start)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551ac0",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa89209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8368dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n",
    "\n",
    "# è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10Bï¼ˆ100å„„ï¼‰æ–‡å­—\n",
    "# å¤šã„ã®ã§2Bï¼ˆ20å„„ï¼‰æ–‡å­—ã«è¨­å®š\n",
    "max_chars = 2_000_000_000\n",
    "\n",
    "# 1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10,000æ–‡å­—\n",
    "doc_cap = 10_000\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯65,536ï¼ˆ2ã®16ä¹—ï¼‰\n",
    "vocab_size = 65_536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618d1f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/root/.cache/nanochat/base_data/shard_00000.parquet',\n",
       " '/root/.cache/nanochat/base_data/shard_00001.parquet',\n",
       " '/root/.cache/nanochat/base_data/shard_00002.parquet',\n",
       " '/root/.cache/nanochat/base_data/shard_00003.parquet',\n",
       " '/root/.cache/nanochat/base_data/shard_00004.parquet']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_parquet_files(data_dir=None):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™\n",
    "    Args:\n",
    "        data_dir (str, optional): ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®DATA_DIRã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "    Returns:\n",
    "        List[str]: parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    # ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "    data_dir = DATA_DIR if data_dir is None else data_dir\n",
    "\n",
    "    # parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    parquet_files = sorted([\n",
    "        f for f in os.listdir(data_dir)\n",
    "        if f.endswith('.parquet') and not f.endswith('.tmp')\n",
    "    ])\n",
    "\n",
    "    # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]\n",
    "\n",
    "    return parquet_paths\n",
    "\n",
    "list_parquet_files()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08792b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row group has 1024 documents.\n",
      "First document length: 8657 characters.\n",
      "First document: Shipment & Transport-Sea, Air, Rail, Road, Pipelin...\n"
     ]
    }
   ],
   "source": [
    "def parquets_iter_batched(split, start=0, step=1):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒƒãƒã§èª­ã¿è¾¼ã‚€ãŸã‚ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "\n",
    "    splitã¯ã€\"train\"ã¾ãŸã¯\"val\"ã‚’æŒ‡å®š\n",
    "    \"train\"ã®å ´åˆã€æœ€åˆã®N-1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    \"val\"ã®å ´åˆã€æœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    startã¨stepã¯ã€åˆ†æ•£ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆDistributed Data Processingï¼‰ã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹éš›ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        split (str): \"train\"ã¾ãŸã¯\"val\"\n",
    "        start (int, optional): é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "        step (int, optional): ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "    Yields:\n",
    "        List[str]: å„ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    # ã™ã¹ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    parquet_paths = list_parquet_files()\n",
    "\n",
    "    # splitã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ\n",
    "    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n",
    "\n",
    "    for filepath in parquet_paths:\n",
    "\n",
    "        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        pf = pq.ParquetFile(filepath)\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¡Œã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆrow groupsï¼‰ã§ãƒ«ãƒ¼ãƒ—\n",
    "        # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã¯ã€æ•°åƒã‹ã‚‰æ•°ä¸‡è¡Œã®ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯\n",
    "        for rg_idx in range(start, pf.num_row_groups, step):\n",
    "\n",
    "            # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’èª­ã¿è¾¼ã‚€\n",
    "            rg = pf.read_row_group(rg_idx)\n",
    "\n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—ã—ã¦yield\n",
    "            texts = rg.column('text').to_pylist()\n",
    "\n",
    "            # 1ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™\n",
    "            yield texts\n",
    "\n",
    "text = parquets_iter_batched(\"train\")\n",
    "first_row_group = next(text)\n",
    "print(f\"First row group has {len(first_row_group)} documents.\")\n",
    "print(f\"First document length: {len(first_row_group[0])} characters.\")\n",
    "print(f\"First document: {first_row_group[0][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326e4c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 length: 8657 characters.\n",
      "Document 0 content: Shipment & Transport-Sea, Air, Rail, Road, Pipelin...\n",
      "Document 1 length: 9058 characters.\n",
      "Document 1 content: 12. Definition â€” In this Part, unless the context ...\n",
      "Document 2 length: 800 characters.\n",
      "Document 2 content: GÃºthwinÃ« was the sword that belonged to Ã‰omer.\n",
      "It ...\n",
      "Document 3 length: 10000 characters.\n",
      "Document 3 content: The robot in the picture above is called YOLO, whi...\n",
      "Document 4 length: 1824 characters.\n",
      "Document 4 content: Metal additive manufacturing (AM) is growing at a ...\n"
     ]
    }
   ],
   "source": [
    "# Text iterator\n",
    "\n",
    "def text_iterator():\n",
    "    \"\"\"\n",
    "    1) Flatten the batches into a single iterator\n",
    "    2) Crop every document to args.doc_cap characters\n",
    "    3) Break when we've seen args.max_chars characters\n",
    "    \"\"\"\n",
    "    nchars = 0\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "        for doc in batch:\n",
    "            doc_text = doc\n",
    "            if len(doc_text) > doc_cap:\n",
    "                doc_text = doc_text[:doc_cap]\n",
    "            nchars += len(doc_text)\n",
    "            yield doc_text\n",
    "            if nchars > max_chars:\n",
    "                return\n",
    "\n",
    "text_iter = text_iterator()\n",
    "first_5_docs = [next(text_iter) for _ in range(5)]\n",
    "for i, doc in enumerate(first_5_docs):\n",
    "    print(f\"Document {i} length: {len(doc)} characters.\")\n",
    "    print(f\"Document {i} content: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "743bb00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 28.18s\n"
     ]
    }
   ],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "\n",
    "t0 = time.time()\n",
    "text_iter = text_iterator()  # reset the iterator\n",
    "tokenizer = RustBPETokenizer.train_from_iterator(text_iter, vocab_size)\n",
    "t1 = time.time()\n",
    "train_time = t1 - t0\n",
    "print(f\"Training time: {train_time:.2f}s\")\n",
    "\n",
    "# 26.33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb1d6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer encoding to /root/.cache/nanochat/tokenizer/tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenizer to disk\n",
    "base_dir = get_base_dir()\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "tokenizer.save(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "285dd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inline sanity check\n",
    "test_text = \"\"\"Hello world! This is a test.\n",
    "Numbers: 123, 4567, 89\n",
    "Contractions: I'm, you're, it's\n",
    "Special chars: @#$%^&*()\n",
    "Unicode: ä½ å¥½ä¸–ç•Œ ğŸŒ\"\"\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "assert decoded == test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d82bc0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved token_bytes to /root/.cache/nanochat/tokenizer/token_bytes.pt\n"
     ]
    }
   ],
   "source": [
    "# One more thing: we wish to cache a mapping from token id to number of bytes of that token\n",
    "# for efficient evaluation of bits per byte. Unlike the typical mean loss, this\n",
    "# allows us to report a loss that is invariant to the vocab size of the tokenizer.\n",
    "# The bits per byte on the validation set is then one of the primary metrics we care about.\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "special_set = set(tokenizer.get_special_tokens())\n",
    "token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n",
    "token_bytes = []\n",
    "for token_id in range(vocab_size):\n",
    "    token_str = token_strings[token_id] # the Python string representation of this token\n",
    "    if token_str in special_set:\n",
    "        token_bytes.append(0) # special characters are not counted\n",
    "    else:\n",
    "        id_bytes = len(token_str.encode(\"utf-8\")) # number of bytes that make up this token\n",
    "        token_bytes.append(id_bytes)\n",
    "token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n",
    "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "with open(token_bytes_path, \"wb\") as f:\n",
    "    torch.save(token_bytes, f)\n",
    "print(f\"Saved token_bytes to {token_bytes_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd031a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to report\n",
    "# from nanochat.report import get_report\n",
    "# token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)\n",
    "# get_report().log(section=\"Tokenizer training\", data=[\n",
    "#     vars(args), # argparse command line arguments\n",
    "#     {\"train_time\": train_time},\n",
    "#     {\"num_special_tokens\": len(special_set)},\n",
    "#     {\n",
    "#         \"token_bytes_min\": int(token_bytes_nonzero.min().item()),\n",
    "#         \"token_bytes_max\": int(token_bytes_nonzero.max().item()),\n",
    "#         \"token_bytes_mean\": token_bytes_nonzero.mean().item(),\n",
    "#         \"token_bytes_std\": token_bytes_nonzero.std().item(),\n",
    "#     }\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1f17",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3d9cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random text I got from a random website this morning\n",
    "news_text = r\"\"\"\n",
    "(Washington, D.C., July 9, 2025)- Yesterday, Mexicoâ€™s National Service of Agro-Alimentary Health, Safety, and Quality (SENASICA) reported a new case of New World Screwworm (NWS) in Ixhuatlan de Madero, Veracruz in Mexico, which is approximately 160 miles northward of the current sterile fly dispersal grid, on the eastern side of the country and 370 miles south of the U.S./Mexico border. This new northward detection comes approximately two months after northern detections were reported in Oaxaca and Veracruz, less than 700 miles away from the U.S. border, which triggered the closure of our ports to Mexican cattle, bison, and horses on May 11, 2025.\n",
    "\n",
    "While USDA announced a risk-based phased port re-opening strategy for cattle, bison, and equine from Mexico beginning as early as July 7, 2025, this newly reported NWS case raises significant concern about the previously reported information shared by Mexican officials and severely compromises the outlined port reopening schedule of five ports from July 7-September 15. Therefore, in order to protect American livestock and our nationâ€™s food supply, Secretary Rollins has ordered the closure of livestock trade through southern ports of entry effective immediately.\n",
    "\n",
    "â€œThe United States has promised to be vigilant â€” and after detecting this new NWS case, we are pausing the planned port reopeningâ€™s to further quarantine and target this deadly pest in Mexico. We must see additional progress combatting NWS in Veracruz and other nearby Mexican states in order to reopen livestock ports along the Southern border,â€ said U.S. Secretary of Agriculture Brooke L. Rollins. â€œThanks to the aggressive monitoring by USDA staff in the U.S. and in Mexico, we have been able to take quick and decisive action to respond to the spread of this deadly pest.â€\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aeeca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Korean text (to test non-English compression)\n",
    "korean_text = r\"\"\"\n",
    "ì •ì§í•œ ì‚¬ì‹¤ ìœ„ì—, ê³µì •í•œ ì‹œì„ ì„ ë”í•˜ë‹¤\n",
    "Herald Korea Times\n",
    "\n",
    "í—¤ëŸ´ë“œì½”ë¦¬ì•„íƒ€ì„ì¦ˆëŠ” ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™” ë“± í•œêµ­ ì‚¬íšŒ ì „ë°˜ì˜ ì£¼ìš” ì´ìŠˆë¥¼ ì‹¬ë„ ìˆê²Œ ë‹¤ë£¨ëŠ” ì¢…í•© ì˜¨ë¼ì¸ ì‹ ë¬¸ì‚¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë‹¨ìˆœíˆ ë‰´ìŠ¤ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ì‹¤(Fact)ì— ê¸°ë°˜í•œ ì–‘ì¸¡ì˜ ì‹œê°ì„ ê· í˜• ìˆê²Œ ì¡°ëª…í•˜ë©°, ë…ì ì—¬ëŸ¬ë¶„ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ” â€˜ì •ë³´ì˜ ê· í˜•â€™ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "í•œêµ­ ì–¸ë¡ ì˜ ì˜¤ëœ ë¬¸ì œë¡œ ì§€ì ë˜ì–´ ì˜¨ ì •ì¹˜ì  í¸í–¥, ì´ë…ì  ì™œê³¡ì—ì„œ ë²—ì–´ë‚˜\n",
    "ì˜¤ì§ ì •ì§í•¨ê³¼ ê³µì •í•¨ì„ ì›ì¹™ìœ¼ë¡œ ì‚¼ëŠ” ì–¸ë¡ ì„ ì§€í–¥í•©ë‹ˆë‹¤.\n",
    "ì–´ëŠ í•œìª½ì˜ ì£¼ì¥ë§Œì„ í™•ëŒ€í•˜ê±°ë‚˜ ê°ì¶”ì§€ ì•Šê³ ,\n",
    "**ëª¨ë“  ìŸì ì— ëŒ€í•´ â€˜ë¬´ì—‡ì´ ìŸì ì¸ì§€â€™, â€˜ëˆ„ê°€ ë¬´ì—‡ì„ ì£¼ì¥í•˜ëŠ”ì§€â€™, â€˜ì‚¬ì‹¤ì€ ë¬´ì—‡ì¸ì§€â€™**ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6465ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random piece of code\n",
    "code_text = r\"\"\"\n",
    "class BasicTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # input text preprocessing\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count up the number of times every consecutive pair appears\n",
    "            stats = get_stats(ids)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e86a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_text = r\"\"\"\n",
    "\\documentclass[12pt]{article}\n",
    "\\usepackage{amsmath,amsthm,amssymb}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\n",
    "\\newtheorem{theorem}{Theorem}\n",
    "\\newtheorem*{remark}{Remark}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\begin{center}\n",
    "{\\Large A Cute Identity: The Sum of Cubes is a Square}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{theorem}\n",
    "For every integer $n \\ge 1$,\n",
    "\\[\n",
    "\\sum_{k=1}^{n} k^{3} \\;=\\; \\left(\\frac{n(n+1)}{2}\\right)^{2}.\n",
    "\\]\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}[Proof 1 (Induction)]\n",
    "Let $S(n) = \\sum_{k=1}^{n} k^3$. For $n=1$, $S(1)=1=(1\\cdot 2/2)^2$, so the base case holds.\n",
    "\n",
    "Assume $S(n)=\\big(\\tfrac{n(n+1)}{2}\\big)^2$ for some $n\\ge 1$.\n",
    "Then\n",
    "\\[\n",
    "S(n+1)\n",
    "= S(n) + (n+1)^3\n",
    "= \\left(\\frac{n(n+1)}{2}\\right)^2 + (n+1)^3.\n",
    "\\]\n",
    "Factor out $(n+1)^2$:\n",
    "\\[\n",
    "S(n+1)\n",
    "= (n+1)^2\\left( \\frac{n^2}{4} + (n+1) \\right)\n",
    "= (n+1)^2\\left( \\frac{n^2 + 4n + 4}{4} \\right)\n",
    "= (n+1)^2\\left( \\frac{(n+2)^2}{4} \\right).\n",
    "\\]\n",
    "Thus\n",
    "\\[\n",
    "S(n+1)=\\left(\\frac{(n+1)(n+2)}{2}\\right)^2,\n",
    "\\]\n",
    "which matches the claimed formula with $n$ replaced by $n+1$. By induction, the identity holds for all $n\\ge 1$.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{proof}[Proof 2 (Algebraic telescoping)]\n",
    "Recall the binomial identity\n",
    "\\[\n",
    "(k+1)^4 - k^4 = 4k^3 + 6k^2 + 4k + 1.\n",
    "\\]\n",
    "Summing both sides from $k=0$ to $n$ telescopes:\n",
    "\\[\n",
    "(n+1)^4 - 0^4\n",
    "= \\sum_{k=0}^{n}\\big(4k^3 + 6k^2 + 4k + 1\\big)\n",
    "= 4\\sum_{k=1}^{n}k^3 + 6\\sum_{k=1}^{n}k^2 + 4\\sum_{k=1}^{n}k + (n+1).\n",
    "\\]\n",
    "Using the standard sums\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\n",
    "\\quad\\text{and}\\quad\n",
    "\\sum_{k=1}^{n}k^2 = \\frac{n(n+1)(2n+1)}{6},\n",
    "\\]\n",
    "solve for $\\sum_{k=1}^{n}k^3$ to get\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k^3 = \\left(\\frac{n(n+1)}{2}\\right)^2.\n",
    "\\]\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{remark}\n",
    "Geometrically, the identity says: ``adding up $1^3,2^3,\\dots,n^3$ builds a perfect squareâ€™â€™â€”namely the square of the $n$th triangular number. This is why one sometimes calls it the \\emph{sum-of-cubes is a square} phenomenon.\n",
    "\\end{remark}\n",
    "\n",
    "\\end{document}\n",
    "\"\"\".strip()\n",
    "\n",
    "science_text = r\"\"\"\n",
    "Photosynthesis is a photochemical energy transduction process in which light-harvesting pigmentâ€“protein complexes within the thylakoid membranes of oxygenic phototrophs absorb photons and initiate charge separation at the reaction center, driving the linear electron transport chain from water to NADPâº via photosystem II, the cytochrome bâ‚†f complex, and photosystem I, concomitantly generating a trans-thylakoid proton motive force utilized by chloroplastic ATP synthase. The light-dependent reactions produce ATP and NADPH, which fuel the Calvinâ€“Bensonâ€“Bassham cycle in the stroma, wherein ribulose-1,5-bisphosphate is carboxylated by ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) to form 3-phosphoglycerate, subsequently reduced and regenerated through a series of enzymatic steps, enabling net assimilation of COâ‚‚ into triose phosphates and ultimately carbohydrates. This process is tightly regulated by photoprotective mechanisms, redox feedback, and metabolite flux, representing a central biochemical pathway coupling solar energy capture to the biosphereâ€™s primary productivity.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142e9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tokenizer was trained on data from earlier shards, so it has seen this data\n",
    "train_docs = next(parquets_iter_batched(split=\"train\"))\n",
    "train_text = \"\\n\".join(train_docs)\n",
    "val_docs = next(parquets_iter_batched(split=\"val\"))\n",
    "val_text = \"\\n\".join(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2129131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = [\n",
    "    (\"news\", news_text),\n",
    "    (\"korean\", korean_text),\n",
    "    (\"code\", code_text),\n",
    "    (\"math\", math_text),\n",
    "    (\"science\", science_text),\n",
    "    (\"fwe-train\", train_text),\n",
    "]\n",
    "if val_text:\n",
    "    all_text.append((\"fwe-val\", val_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1803ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    # return HuggingFaceTokenizer.from_directory(tokenizer_dir)\n",
    "    return RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67dc7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out current default compared to GPT-2 and GPT-4 tokenizers\n",
    "tokenizer_results = {}\n",
    "vocab_sizes = {}\n",
    "\n",
    "for tokenizer_name in [\"gpt2\", \"gpt4\", \"ours\"]:\n",
    "\n",
    "    if tokenizer_name == \"gpt2\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\") # gpt-2 base model tokenizer\n",
    "    elif tokenizer_name == \"gpt4\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\") # gpt-4 base model tokenizer\n",
    "    else:\n",
    "        tokenizer = get_tokenizer()\n",
    "\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "    tokenizer_results[tokenizer_name] = {}\n",
    "\n",
    "    for name, text in all_text:\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text\n",
    "\n",
    "        encoded_bytes = text.encode('utf-8')\n",
    "        ratio = len(encoded_bytes) / len(encoded)\n",
    "        tokenizer_results[tokenizer_name][name] = {\n",
    "            'bytes': len(encoded_bytes),\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d25ac02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocab sizes:\n",
      "GPT-2: 50257\n",
      "GPT-4: 100277\n",
      "Ours: 65536\n"
     ]
    }
   ],
   "source": [
    "# Print vocab sizes\n",
    "print(f\"\\nVocab sizes:\")\n",
    "print(f\"GPT-2: {vocab_sizes['gpt2']}\")\n",
    "print(f\"GPT-4: {vocab_sizes['gpt4']}\")\n",
    "print(f\"Ours: {vocab_sizes['ours']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60f4c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison with GPT-2:\n",
      "===============================================================================================\n",
      "Text Type  Bytes    GPT-2           Ours            Relative     Better    \n",
      "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "news       1819     \u001b[91m404    \u001b[0m \u001b[91m4.50   \u001b[0m \u001b[92m375    \u001b[0m \u001b[92m4.85   \u001b[0m \u001b[92m   +7.2%\u001b[0m     Ours      \n",
      "korean     893      \u001b[91m745    \u001b[0m \u001b[91m1.20   \u001b[0m \u001b[92m712    \u001b[0m \u001b[92m1.25   \u001b[0m \u001b[92m   +4.4%\u001b[0m     Ours      \n",
      "code       1259     \u001b[91m576    \u001b[0m \u001b[91m2.19   \u001b[0m \u001b[92m492    \u001b[0m \u001b[92m2.56   \u001b[0m \u001b[92m  +14.6%\u001b[0m     Ours      \n",
      "math       1834     \u001b[92m936    \u001b[0m \u001b[92m1.96   \u001b[0m \u001b[91m966    \u001b[0m \u001b[91m1.90   \u001b[0m \u001b[91m   -3.2%\u001b[0m     GPT-2     \n",
      "science    1112     \u001b[91m260    \u001b[0m \u001b[91m4.28   \u001b[0m \u001b[92m228    \u001b[0m \u001b[92m4.88   \u001b[0m \u001b[92m  +12.3%\u001b[0m     Ours      \n",
      "fwe-train  4208518  \u001b[91m900364 \u001b[0m \u001b[91m4.67   \u001b[0m \u001b[92m856883 \u001b[0m \u001b[92m4.91   \u001b[0m \u001b[92m   +4.8%\u001b[0m     Ours      \n",
      "fwe-val    4991242  \u001b[91m1075364\u001b[0m \u001b[91m4.64   \u001b[0m \u001b[92m1027241\u001b[0m \u001b[92m4.86   \u001b[0m \u001b[92m   +4.5%\u001b[0m     Ours      \n",
      "\n",
      "Comparison with GPT-4:\n",
      "===============================================================================================\n",
      "Text Type  Bytes    GPT-4           Ours            Relative     Better    \n",
      "                    Tokens  Ratio   Tokens  Ratio   Diff %      \n",
      "-----------------------------------------------------------------------------------------------\n",
      "news       1819     \u001b[91m387    \u001b[0m \u001b[91m4.70   \u001b[0m \u001b[92m375    \u001b[0m \u001b[92m4.85   \u001b[0m \u001b[92m   +3.1%\u001b[0m     Ours      \n",
      "korean     893      \u001b[92m364    \u001b[0m \u001b[92m2.45   \u001b[0m \u001b[91m712    \u001b[0m \u001b[91m1.25   \u001b[0m \u001b[91m  -95.6%\u001b[0m     GPT-4     \n",
      "code       1259     \u001b[92m309    \u001b[0m \u001b[92m4.07   \u001b[0m \u001b[91m492    \u001b[0m \u001b[91m2.56   \u001b[0m \u001b[91m  -59.2%\u001b[0m     GPT-4     \n",
      "math       1834     \u001b[92m832    \u001b[0m \u001b[92m2.20   \u001b[0m \u001b[91m966    \u001b[0m \u001b[91m1.90   \u001b[0m \u001b[91m  -16.1%\u001b[0m     GPT-4     \n",
      "science    1112     \u001b[91m249    \u001b[0m \u001b[91m4.47   \u001b[0m \u001b[92m228    \u001b[0m \u001b[92m4.88   \u001b[0m \u001b[92m   +8.4%\u001b[0m     Ours      \n",
      "fwe-train  4208518  \u001b[91m874799 \u001b[0m \u001b[91m4.81   \u001b[0m \u001b[92m856883 \u001b[0m \u001b[92m4.91   \u001b[0m \u001b[92m   +2.0%\u001b[0m     Ours      \n",
      "fwe-val    4991242  \u001b[91m1048837\u001b[0m \u001b[91m4.76   \u001b[0m \u001b[92m1027241\u001b[0m \u001b[92m4.86   \u001b[0m \u001b[92m   +2.1%\u001b[0m     Ours      \n"
     ]
    }
   ],
   "source": [
    "# ANSI color codes\n",
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "def print_comparison(baseline_name, baseline_results, ours_results, all_text):\n",
    "    \"\"\"Print comparison table between baseline tokenizer and ours.\"\"\"\n",
    "    print(f\"\\nComparison with {baseline_name}:\")\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"{'Text Type':<10} {'Bytes':<8} {baseline_name:<15} {'Ours':<15} {'Relative':<12} {'Better':<10}\")\n",
    "    print(f\"{'':10} {'':8} {'Tokens':<7} {'Ratio':<7} {'Tokens':<7} {'Ratio':<7} {'Diff %':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, text in all_text:\n",
    "        baseline_data = baseline_results[name]\n",
    "        ours_data = ours_results[name]\n",
    "\n",
    "        # Calculate relative difference (positive means ours is better, negative means worse)\n",
    "        # Using tokens: fewer tokens is better, so we calculate (baseline_tokens - ours_tokens) / baseline_tokens\n",
    "        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n",
    "\n",
    "        # Determine which has better compression (higher ratio = better)\n",
    "        if baseline_data['ratio'] > ours_data['ratio']:\n",
    "            baseline_color, ours_color = GREEN, RED\n",
    "            better = baseline_name\n",
    "            diff_color = RED\n",
    "        elif ours_data['ratio'] > baseline_data['ratio']:\n",
    "            baseline_color, ours_color = RED, GREEN\n",
    "            better = \"Ours\"\n",
    "            diff_color = GREEN\n",
    "        else:\n",
    "            baseline_color, ours_color = \"\", \"\"\n",
    "            better = \"Tie\"\n",
    "            diff_color = \"\"\n",
    "\n",
    "        print(f\"{name:<10} {baseline_data['bytes']:<8} \"\n",
    "              f\"{baseline_color}{baseline_data['tokens']:<7}{RESET} \"\n",
    "              f\"{baseline_color}{baseline_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['tokens']:<7}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{diff_color}{relative_diff:+7.1f}%{RESET}     \"\n",
    "              f\"{better:<10}\")\n",
    "\n",
    "# Print comparisons\n",
    "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)\n",
    "print_comparison(\"GPT-4\", tokenizer_results['gpt4'], tokenizer_results['ours'], all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59e90f",
   "metadata": {},
   "source": [
    "## æœ€é©åŒ–é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc80e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e532dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.mT\n",
    "        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df2075e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer should not be used for the embedding layer, the final fully connected layer,\n",
    "    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        ns_steps: The number of Newton-Schulz iteration steps to use.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        params: list[Tensor] = [*params]\n",
    "        param_groups = []\n",
    "        for size in {p.numel() for p in params}:\n",
    "            group = dict(params=[p for p in params if p.numel() == size])\n",
    "            param_groups.append(group)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            for p in params:\n",
    "                g = p.grad\n",
    "                assert g is not None\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf: Tensor = state[\"momentum_buffer\"]\n",
    "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "                p.add_(g, alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3c30d",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6bcce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea258fa0",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f83803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ddp():\n",
    "    # TODO is there a proper way\n",
    "    return int(os.environ.get('RANK', -1)) != -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37f2ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_info():\n",
    "    if is_ddp():\n",
    "        assert all(var in os.environ for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE'])\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        return True, ddp_rank, ddp_local_rank, ddp_world_size\n",
    "    else:\n",
    "        return False, 0, 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d00ae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    # Purely functional rmsnorm with no learnable params\n",
    "    return F.rms_norm(x, (x.size(-1),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "413e54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, cos, sin):\n",
    "    assert x.ndim == 4  # multihead attention\n",
    "    d = x.shape[3] // 2\n",
    "    x1, x2 = x[..., :d], x[..., d:] # split up last time into two halves\n",
    "    y1 = x1 * cos + x2 * sin # rotate pairs of dims\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    out = torch.cat([y1, y2], 3) # re-assemble\n",
    "    out = out.to(x.dtype) # ensure input/output dtypes match\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355ebe2",
   "metadata": {},
   "source": [
    "### CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d3ba25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.n_head = config.n_head\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # Project the input to get queries, keys, and values\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        # Apply Rotary Embeddings to queries and keys to get relative positional encoding\n",
    "        cos, sin = cos_sin\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin) # QK rotary embedding\n",
    "        q, k = norm(q), norm(k) # QK norm\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) # make head be batch dim, i.e. (B, T, H, D) -> (B, H, T, D)\n",
    "\n",
    "        # Apply KV cache: insert current k,v into cache, get the full view so far\n",
    "        if kv_cache is not None:\n",
    "            k, v = kv_cache.insert_kv(self.layer_idx, k, v)\n",
    "        Tq = q.size(2) # number of queries in this forward pass\n",
    "        Tk = k.size(2) # number of keys/values in total (in the cache + current forward pass)\n",
    "\n",
    "        # Attention: queries attend to keys/values autoregressively. A few cases to handle:\n",
    "        enable_gqa = self.n_head != self.n_kv_head # Group Query Attention (GQA): duplicate key/value heads to match query heads if desired\n",
    "        if kv_cache is None or Tq == Tk:\n",
    "            # During training (no KV cache), attend as usual with causal attention\n",
    "            # And even if there is KV cache, we can still use this simple version when Tq == Tk\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "        elif Tq == 1:\n",
    "            # During inference but with a single query in this forward pass:\n",
    "            # The query has to attend to all the keys/values in the cache\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n",
    "        else:\n",
    "            # During inference AND we have a chunk of queries in this forward pass:\n",
    "            # First, each query attends to all the cached keys/values (i.e. full prefix)\n",
    "            attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=q.device) # True = keep, False = mask\n",
    "            prefix_len = Tk - Tq\n",
    "            if prefix_len > 0: # can't be negative but could be zero\n",
    "                attn_mask[:, :prefix_len] = True\n",
    "            # Then, causal attention within this chunk\n",
    "            attn_mask[:, prefix_len:] = torch.tril(torch.ones((Tq, Tq), dtype=torch.bool, device=q.device))\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)\n",
    "\n",
    "        # Re-assemble the heads side by side and project back to residual stream\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257364b",
   "metadata": {},
   "source": [
    "### MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9cbcfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square()\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd7f0a",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "270f0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        x = x + self.attn(norm(x), cos_sin, kv_cache)\n",
    "        x = x + self.mlp(norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac54f3e",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cdec548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            \"h\": nn.ModuleList([Block(config, layer_idx) for layer_idx in range(config.n_layer)]),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # To support meta device initialization, we init the rotary embeddings here, but it's fake\n",
    "        # As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,\n",
    "        # so let's just over-compute them, but assert fail if we ever reach that amount.\n",
    "        # In the future we can dynamically grow the cache, for now it's fine.\n",
    "        self.rotary_seq_len = config.sequence_len * 10 # 10X over-compute should be enough, TODO make nicer?\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.register_buffer(\"cos\", cos, persistent=False) # persistent=False means it's not saved to the checkpoint\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(self._init_weights)\n",
    "        # zero out classifier weights\n",
    "        torch.nn.init.zeros_(self.lm_head.weight)\n",
    "        # zero out c_proj weights in all blocks\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n",
    "            torch.nn.init.zeros_(block.attn.c_proj.weight)\n",
    "        # init the rotary embeddings\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "        # Cast the embeddings from fp32 to bf16: optim can tolerate it and it saves memory: both in the model and the activations\n",
    "        if self.transformer.wte.weight.device.type == \"cuda\":\n",
    "            self.transformer.wte.to(dtype=torch.bfloat16)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # https://arxiv.org/pdf/2310.17813\n",
    "            fan_out = module.weight.size(0)\n",
    "            fan_in = module.weight.size(1)\n",
    "            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
    "\n",
    "    # TODO: bump base theta more, e.g. 100K is more common more recently\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        # autodetect the device from model embeddings\n",
    "        if device is None:\n",
    "            device = self.transformer.wte.weight.device\n",
    "        # stride the channels\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        # stride the time steps\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        # calculate the rotation frequencies at each (time, channel) pair\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16() # keep them in bfloat16\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :] # add batch and head dims for later broadcasting\n",
    "        return cos, sin\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.transformer.wte.weight.device\n",
    "\n",
    "    def estimate_flops(self):\n",
    "        \"\"\" Return the estimated FLOPs per token for the model. Ref: https://arxiv.org/abs/2204.02311 \"\"\"\n",
    "        nparams = sum(p.numel() for p in self.parameters())\n",
    "        nparams_embedding = self.transformer.wte.weight.numel()\n",
    "        l, h, q, t = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.sequence_len\n",
    "        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t\n",
    "        return num_flops_per_token\n",
    "\n",
    "    def setup_optimizers(self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):\n",
    "        model_dim = self.config.n_embd\n",
    "        ddp, rank, local_rank, world_size = get_dist_info()\n",
    "        # Separate out all parameters into 3 groups (matrix, embedding, lm_head)\n",
    "        matrix_params = list(self.transformer.h.parameters())\n",
    "        embedding_params = list(self.transformer.wte.parameters())\n",
    "        lm_head_params = list(self.lm_head.parameters())\n",
    "        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params)\n",
    "        # Create the AdamW optimizer for the embedding and lm_head\n",
    "        # Scale the LR for the AdamW parameters by âˆ1/âˆšdmodel (having tuned the LRs for 768 dim model)\n",
    "        dmodel_lr_scale = (model_dim / 768) ** -0.5\n",
    "        if rank == 0:\n",
    "            print(f\"Scaling the LR for the AdamW parameters âˆ1/âˆš({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n",
    "        adam_groups = [\n",
    "            dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),\n",
    "            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n",
    "        ]\n",
    "        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)\n",
    "\n",
    "        # åˆ†æ•£å‡¦ç†ã¯è¡Œã‚ãªã„\n",
    "        # AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=True)\n",
    "        AdamWFactory = partial(torch.optim.AdamW, fused=True)\n",
    "\n",
    "        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
    "        # Create the Muon optimizer for the linear layers\n",
    "        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)\n",
    "\n",
    "        # åˆ†æ•£å‡¦ç†ã¯è¡Œã‚ãªã„\n",
    "        # MuonFactory = DistMuon if ddp else Muon\n",
    "        MuonFactory = Muon\n",
    "\n",
    "        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)\n",
    "        # Combine them the two optimizers into one list\n",
    "        optimizers = [adamw_optimizer, muon_optimizer]\n",
    "        for opt in optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                group[\"initial_lr\"] = group[\"lr\"]\n",
    "        return optimizers\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        # Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim))\n",
    "        assert T <= self.cos.size(1), f\"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}\"\n",
    "        assert idx.device == self.cos.device, f\"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}\"\n",
    "        assert self.cos.dtype == torch.bfloat16, \"Rotary embeddings must be in bfloat16\"\n",
    "        # if kv cache exists, we need to offset the rotary embeddings to the current position in the cache\n",
    "        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "        cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T] # truncate cache to current sequence length\n",
    "\n",
    "        # Forward the trunk of the Transformer\n",
    "        x = self.transformer.wte(idx)\n",
    "        x = norm(x)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, cos_sin, kv_cache)\n",
    "        x = norm(x)\n",
    "\n",
    "        # Forward the lm_head (compute logits)\n",
    "        softcap = 15\n",
    "        if targets is not None:\n",
    "            # training mode: compute and return the loss\n",
    "            # TODO: experiment with Liger Kernels / chunked cross-entropy etc.\n",
    "            logits = self.lm_head(x)\n",
    "            logits = softcap * torch.tanh(logits / softcap) # logits softcap\n",
    "            logits = logits.float() # use tf32/fp32 for logits\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1, reduction=loss_reduction)\n",
    "            return loss\n",
    "        else:\n",
    "            # inference mode: compute and return the logits\n",
    "            logits = self.lm_head(x)\n",
    "            logits = softcap * torch.tanh(logits / softcap) # logits softcap\n",
    "            return logits\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"\n",
    "        Naive autoregressive streaming inference.\n",
    "        To make it super simple, let's assume:\n",
    "        - batch size is 1\n",
    "        - ids and the yielded tokens are simple Python lists and ints\n",
    "        \"\"\"\n",
    "        assert isinstance(tokens, list)\n",
    "        device = self.get_device()\n",
    "        rng = None\n",
    "        if temperature > 0:\n",
    "            rng = torch.Generator(device=device)\n",
    "            rng.manual_seed(seed)\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device) # add batch dim\n",
    "        for _ in range(max_tokens):\n",
    "            logits = self.forward(ids) # (B, T, vocab_size)\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            if temperature > 0:\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "            else:\n",
    "                next_ids = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            ids = torch.cat((ids, next_ids), dim=1)\n",
    "            token = next_ids.item()\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d86ed3",
   "metadata": {},
   "source": [
    "### GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14c30cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    sequence_len: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 6 # number of query heads\n",
    "    n_kv_head: int = 6 # number of key/value heads (MQA)\n",
    "    n_embd: int = 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae35199",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d61d6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e5d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_distributed_data_loader(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128, device=\"cuda\"):\n",
    "    \"\"\"Stream pretraining text from parquet files, tokenize, yield training batches.\"\"\"\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    needed_tokens = B * T + 1 # +1 is because we also need the target at the last token\n",
    "    # get the tokenizer and the bos token\n",
    "    tokenizer = get_tokenizer()\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "    # scratch buffer holds the tokens for one iteration\n",
    "    token_buffer = deque() # we stream tokens on the right and pop from the left\n",
    "\n",
    "    # infinite iterator over document batches\n",
    "    def document_batches():\n",
    "        while True:\n",
    "            # batch will iterate in group size of the parquet files, usually e.g. 1024 rows\n",
    "            for batch in parquets_iter_batched(split=split, start=ddp_rank, step=ddp_world_size):\n",
    "                # for the tokenizer we might want to go in usually smaller batches, e.g. 128 rows\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size]\n",
    "    batches = document_batches()\n",
    "\n",
    "    batch_index = 0\n",
    "    while True:\n",
    "        # Accumulate enough tokens for one iteration before yielding.\n",
    "        while len(token_buffer) < needed_tokens:\n",
    "            doc_batch = next(batches)\n",
    "            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "            for tokens in token_lists:\n",
    "                token_buffer.extend(tokens)\n",
    "            batch_index += 1\n",
    "        # Move tokens from the deque into the scratch buffer\n",
    "        tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
    "        # CUDA supports memory pinning for faster transfers between CPU and GPU:\n",
    "        scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
    "        # Create the inputs/targets as 1D tensors\n",
    "        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
    "        targets_cpu = scratch[1:]\n",
    "        # Reshape to 2D and move to GPU async\n",
    "        inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
    "        targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81361726",
   "metadata": {},
   "source": [
    "## ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef3725fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of processes/GPUs to use\n",
    "# NPROC_PER_NODE=8\n",
    "NPROC_PER_NODE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e7e0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User settings\n",
    "run = \"dummy\" # wandb run name default (\"dummy\" is special - we won't log to wandb)\n",
    "\n",
    "# Runtime\n",
    "device_type = \"\" # cuda|cpu|mps (empty => autodetect good device type default, in order: CUDA > MPS > CPU)\n",
    "\n",
    "# Model architecture\n",
    "depth = 20 # the depth of the Transformer model to train, rest of the kwargs are derived\n",
    "max_seq_len = 2048 # max context length\n",
    "# Training horizon. Only one of these 3 will be used, in this order of precedence.\n",
    "num_iterations = -1 # explicit number of steps of the optimization (-1 = disable)\n",
    "target_flops = -1.0 # calculate num_iterations to reach target_flops. Useful for scaling laws experiments (-1 = disable)\n",
    "target_param_data_ratio = 20 # calculate num_iterations to maintain fixed data:param ratio (Chinchilla=20) (-1 = disable)\n",
    "\n",
    "# Optimization\n",
    "# device_batch_size = 32 # per-device batch size (set to not OOM)\n",
    "device_batch_size = 4 # per-device batch size (set to not OOM)\n",
    "\n",
    "total_batch_size = 524288 # total desired batch size, in #tokens\n",
    "embedding_lr = 0.2 # learning rate for the embedding parameters (Adam)\n",
    "unembedding_lr = 0.004 # learning rate for the unembedding parameters (Adam)\n",
    "weight_decay = 0.0 # weight decay for the embedding/unembedding parameters (Adam)\n",
    "matrix_lr = 0.02 # learning rate for the matrix parameters (Muon)\n",
    "grad_clip = 1.0 # gradient clipping value (0.0 = disabled)\n",
    "warmup_ratio = 0.0 # ratio of iterations for LR warmup\n",
    "warmdown_ratio = 0.2 # ratio of iterations for LR warmdown\n",
    "final_lr_frac = 0.0 # final LR is this fraction of the initial LR\n",
    "# Evaluation\n",
    "eval_every = 250 # every how many steps to evaluate the model for val bpb\n",
    "eval_tokens = 20*524288 # number of tokens to evaluate val loss on\n",
    "core_metric_every = 2000 # every how many steps to evaluate the core metric (-1 = disable)\n",
    "core_metric_max_per_task = 500 # examples per task in estimating the core metric\n",
    "sample_every = 2000 # every how many steps to sample from the model\n",
    "# Output\n",
    "model_tag = \"\" # optionally override the model tag for the output checkpoint directory name\n",
    "# now allow CLI to override the settings via the configurator lol\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open(os.path.join('nanochat', 'configurator.py')).read()) # overrides from command line or config file\n",
    "user_config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5c03bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autodetect_device_type():\n",
    "    # prefer to use CUDA if available, otherwise use MPS, otherwise fallback on CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = \"mps\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    print0(f\"Autodetected device type: {device_type}\")\n",
    "    return device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12952377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_init(device_type=\"cuda\"): # cuda|cpu|mps\n",
    "    \"\"\"Basic initialization that we keep doing over and over, so make common.\"\"\"\n",
    "\n",
    "    assert device_type in [\"cuda\", \"mps\", \"cpu\"], \"Invalid device type atm\"\n",
    "    if device_type == \"cuda\":\n",
    "        assert torch.cuda.is_available(), \"Your PyTorch installation is not configured for CUDA but device_type is 'cuda'\"\n",
    "    if device_type == \"mps\":\n",
    "        assert torch.backends.mps.is_available(), \"Your PyTorch installation is not configured for MPS but device_type is 'mps'\"\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.manual_seed(42)\n",
    "    # skipping full reproducibility for now, possibly investigate slowdown later\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # Precision\n",
    "    if device_type == \"cuda\":\n",
    "        torch.set_float32_matmul_precision(\"high\") # uses tf32 instead of fp32 for matmuls\n",
    "\n",
    "    # Distributed setup: Distributed Data Parallel (DDP), optional, and requires CUDA\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    if ddp and device_type == \"cuda\":\n",
    "        device = torch.device(\"cuda\", ddp_local_rank)\n",
    "        torch.cuda.set_device(device)  # make \"cuda\" default to this device\n",
    "        dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "        dist.barrier()\n",
    "    else:\n",
    "        device = torch.device(device_type) # mps|cpu\n",
    "\n",
    "    if ddp_rank == 0:\n",
    "        logger.info(f\"Distributed world size: {ddp_world_size}\")\n",
    "\n",
    "    return ddp, ddp_rank, ddp_local_rank, ddp_world_size, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9884e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print0(s=\"\",**kwargs):\n",
    "    ddp_rank = int(os.environ.get('RANK', 0))\n",
    "    if ddp_rank == 0:\n",
    "        print(s, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9759f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autodetected device type: cuda\n"
     ]
    }
   ],
   "source": [
    "# Compute init\n",
    "device_type = autodetect_device_type() if device_type == \"\" else device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "077887d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb logging init\n",
    "\n",
    "class DummyWandb:\n",
    "    \"\"\"Useful if we wish to not use wandb but have all the same signatures\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def log(self, *args, **kwargs):\n",
    "        pass\n",
    "    def finish(self):\n",
    "        pass\n",
    "\n",
    "use_dummy_wandb = run == \"dummy\" or not master_process\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=run, config=user_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48d844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bytes(device=\"cpu\"):\n",
    "    import torch\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "    assert os.path.exists(token_bytes_path), f\"Token bytes not found at {token_bytes_path}? It gets written by tok_train.py\"\n",
    "    with open(token_bytes_path, \"rb\") as f:\n",
    "        token_bytes = torch.load(f, map_location=device)\n",
    "    return token_bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16c8d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 65,536\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer will be useful for evaluation, also we need the vocab size\n",
    "tokenizer = get_tokenizer()\n",
    "token_bytes = get_token_bytes(device=device)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print0(f\"Vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7661e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers: 20\n",
      "model_dim: 1280\n",
      "num_heads: 10\n",
      "num_kv_heads: 10\n"
     ]
    }
   ],
   "source": [
    "# Model kwargs are derived from the desired depth of the model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # aspect ratio 64 (usually this is varied from 64 -> 128 as model size increases)\n",
    "num_heads = max(1, (model_dim + 127) // 128) # head dim 128 (the division here is ceil div)\n",
    "num_kv_heads = num_heads # default is 1:1 GQA (Group Query Attention) ratio (i.e. GQA is disabled)\n",
    "print0(f\"num_layers: {num_layers}\")\n",
    "print0(f\"model_dim: {model_dim}\")\n",
    "print0(f\"num_heads: {num_heads}\")\n",
    "print0(f\"num_kv_heads: {num_kv_heads}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8a4a35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens / micro-batch / rank: 4 x 2048 = 8,192\n",
      "Tokens / micro-batch: 8,192\n",
      "Total batch size 524,288 => gradient accumulation steps: 64\n"
     ]
    }
   ],
   "source": [
    "# Optimizer / data / training length related hyperparameters\n",
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len # tokens per iteration for a single rank\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks\n",
    "assert total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n",
    "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d7639e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 560,988,160\n",
      "Estimated FLOPs per token: 3.491758e+09\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Model\n",
    "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)\n",
    "model.init_weights()\n",
    "orig_model = model # original, uncompiled model, for saving raw model state_dict\n",
    "model = torch.compile(model, dynamic=False) # TODO: dynamic True/False think through\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print0(f\"Number of parameters: {num_params:,}\")\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "713af8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated number of iterations from target data:param ratio: 21,400\n",
      "Total number of training tokens: 11,219,763,200\n",
      "Tokens : Params ratio: 20.00\n",
      "Total training FLOPs estimate: 3.917670e+19\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of iterations. Either it is given, or from target flops, or from target data:param ratio (in that order)\n",
    "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0\n",
    "if num_iterations > 0:\n",
    "    print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\n",
    "elif target_flops > 0:\n",
    "    # calculate the number of iterations from the target flops\n",
    "    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))\n",
    "    print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")\n",
    "elif target_param_data_ratio > 0:\n",
    "    # calculate the number of iterations from the target param data ratio\n",
    "    target_tokens = target_param_data_ratio * num_params\n",
    "    num_iterations = target_tokens // total_batch_size\n",
    "    print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")\n",
    "else:\n",
    "    raise ValueError(\"No training horizon specified\")\n",
    "total_tokens = total_batch_size * num_iterations\n",
    "print0(f\"Total number of training tokens: {total_tokens:,}\")\n",
    "print0(f\"Tokens : Params ratio: {total_batch_size * num_iterations / num_params:.2f}\") # Chinchilla is ~20\n",
    "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74514c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling the LR for the AdamW parameters âˆ1/âˆš(1280/768) = 0.774597\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Optimizer (Muon for Linear layers, AdamW for embedding and lm_head)\n",
    "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "713af6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataLoaders for train/val\n",
    "base_dir = get_base_dir()\n",
    "tokens_dir = os.path.join(base_dir, \"tokenized_data\")\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "build_val_loader = lambda: tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"val\", device=device)\n",
    "x, y = next(train_loader) # kick off load of the very first batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94f2ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Set up hyperparameter schedulers\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_frac\n",
    "\n",
    "# Momentum scheduler for Muon optimizer\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1ca4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bpb(model, batches, steps, token_bytes):\n",
    "    \"\"\"\n",
    "    Instead of the naive 'mean loss', this function returns the bits per byte (bpb),\n",
    "    which is a tokenization vocab size-indepedent metric, meaning you are still comparing\n",
    "    apples:apples if you change the vocab size. The way this works is that instead of just\n",
    "    calculating the average loss as usual, you calculate the sum loss, and indepependently\n",
    "    also the sum bytes (of all the target tokens), and divide. This normalizes the loss by\n",
    "    the number of bytes that the target tokens represent.\n",
    "\n",
    "    The added complexity is so that:\n",
    "    1) All \"normal\" tokens are normalized by the length of the token in bytes\n",
    "    2) No special tokens (e.g. <|bos|>) are included in the metric - they are masked out.\n",
    "    3) No actively masked tokens (using ignore_index of e.g. -1) are included in the metric.\n",
    "\n",
    "    In addition to evaluate_loss, we need the token_bytes tensor:\n",
    "    It is a 1D tensor of shape (vocab_size,), indicating the number of bytes for\n",
    "    each token id, or 0 if the token is to not be counted (e.g. special tokens).\n",
    "    \"\"\"\n",
    "    # record the losses\n",
    "    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "    total_bytes = torch.tensor(0, dtype=torch.int64, device=model.get_device())\n",
    "    batch_iter = iter(batches)\n",
    "    for _ in range(steps):\n",
    "        x, y = next(batch_iter)\n",
    "        loss2d = model(x, y, loss_reduction='none') # (B, T)\n",
    "        loss2d = loss2d.view(-1) # flatten\n",
    "        y = y.view(-1) # flatten\n",
    "        if (y.int() < 0).any(): # mps does not currently have kernel for < 0 for int64, only int32\n",
    "            # slightly more complex code path if some target tokens are ignore_index (e.g. -1)\n",
    "            # any target token < 0 is to be ignored: do NOT index token_bytes with negatives\n",
    "            valid = y >= 0\n",
    "            y_safe = torch.where(valid, y, torch.zeros_like(y))\n",
    "            # map valid targets to their byte length; ignored targets contribute 0 bytes\n",
    "            num_bytes2d = torch.where(\n",
    "                valid,\n",
    "                token_bytes[y_safe],\n",
    "                torch.zeros_like(y, dtype=token_bytes.dtype)\n",
    "            )\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "        else:\n",
    "            # fast path: no ignored targets, safe to index directly\n",
    "            num_bytes2d = token_bytes[y]\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "    # sum reduce across all ranks\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    if world_size > 1:\n",
    "        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n",
    "    # move both to cpu, calculate bpb and return\n",
    "    total_nats = total_nats.item()\n",
    "    total_bytes = total_bytes.item()\n",
    "    if total_bytes == 0:\n",
    "        return float('inf')\n",
    "    bpb = total_nats / (math.log(2) * total_bytes)\n",
    "    return bpb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "164d0dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 00000 | Validation bpb: 3.3069\n",
      "step 00000/21400 (0.00%) | loss: 11.090355 | grad norm: 0.4337 | lrm: 1.00 | dt: 26108.02ms | tok/sec: 20,081 | mfu: 7.09 | total time: 0.00m\n",
      "step 00001/21400 (0.00%) | loss: 10.805095 | grad norm: 11.0557 | lrm: 1.00 | dt: 10066.63ms | tok/sec: 52,081 | mfu: 18.39 | total time: 0.00m\n",
      "step 00002/21400 (0.01%) | loss: 10.235309 | grad norm: 5.7539 | lrm: 1.00 | dt: 10078.40ms | tok/sec: 52,020 | mfu: 18.37 | total time: 0.00m\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "min_val_bpb = float(\"inf\")\n",
    "smooth_train_loss = 0 # EMA of training loss\n",
    "ema_beta = 0.9 # EMA decay factor\n",
    "total_training_time = 0 # total wall-clock time of training\n",
    "# note that we run +1 steps only so that we can eval and save at the end\n",
    "# for step in range(num_iterations + 1):\n",
    "for step in range(3):\n",
    "    last_step = step == num_iterations\n",
    "    flops_so_far = num_flops_per_token * total_batch_size * step\n",
    "\n",
    "    # once in a while: evaluate the val bpb (all ranks participate)\n",
    "    if last_step or step % eval_every == 0:\n",
    "        model.eval()\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n",
    "        if val_bpb < min_val_bpb:\n",
    "            min_val_bpb = val_bpb\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: estimate the CORE metric (all ranks participate)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    results = {}\n",
    "    if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\n",
    "        model.eval()\n",
    "        with autocast_ctx:\n",
    "            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
    "        print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"core_metric\": results[\"core_metric\"],\n",
    "            \"centered_results\": results[\"centered_results\"],\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: sample from the model (only on master process)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    if master_process and (last_step or (step > 0 and step % sample_every == 0)):\n",
    "        model.eval()\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(orig_model, tokenizer) # use orig_model to avoid recompilation\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            print0(tokenizer.decode(sample[0]))\n",
    "        model.train()\n",
    "\n",
    "    # save checkpoint at the end of the run (only on master process)\n",
    "    if master_process and last_step:\n",
    "        output_dirname = model_tag if model_tag else f\"d{depth}\" # e.g. d12\n",
    "        checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(),\n",
    "            [opt.state_dict() for opt in optimizers], # TODO: make sure saving across ranks is done correctly\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # loss at last step\n",
    "                \"model_config\": model_config_kwargs,\n",
    "                \"user_config\": user_config, # inputs to the training script\n",
    "                \"device_batch_size\": device_batch_size,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # single training step\n",
    "    # evaluate the gradient\n",
    "    synchronize()\n",
    "    t0 = time.time()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach() # for logging\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward\n",
    "    # gradient clipping\n",
    "    grad_clip_enabled = grad_clip > 0.0\n",
    "    if grad_clip_enabled:\n",
    "        grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "        grad_norm = grad_norm_tensor.item() # GPU tensor -> CPU float (note: cpu-gpu sync point)\n",
    "    # step the optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # logging\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() # EMA the training loss\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA\n",
    "    pct_done = 100 * step / num_iterations\n",
    "    tok_per_sec = int(total_batch_size / dt)\n",
    "    flops_per_sec = num_flops_per_token * total_batch_size / dt\n",
    "    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # bfloat16 H100 SXM and without 2:4 sparsity\n",
    "    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %\n",
    "    if step > 10:\n",
    "        total_training_time += dt # only count the time after the first 10 steps\n",
    "    print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\n",
    "    print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n",
    "    if step % 100 == 0:\n",
    "        log_data = {\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "        }\n",
    "        if grad_clip_enabled:\n",
    "            log_data[\"train/grad_norm\"] = grad_norm\n",
    "        wandb_run.log(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
