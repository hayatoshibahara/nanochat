{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c39d5d",
   "metadata": {},
   "source": [
    "# nanochat pre-training\n",
    "\n",
    "GPT-2ãƒ¬ãƒ™ãƒ«ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å˜ä¸€ã®GPUã§äº‹å‰å­¦ç¿’ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2a9f3",
   "metadata": {},
   "source": [
    "## å‚è€ƒ\n",
    "\n",
    "- [nanochat][0]\n",
    "- [The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale][4]\n",
    "- [Muon: An optimizer for hidden layers in neural networks][1]\n",
    "- [Muon is Scaling for LLM Training][6]\n",
    "- [A Spectral Condition for Feature Learning][2]\n",
    "- [PaLM: Scaling Language Modeling with Pathways][3]\n",
    "- [DataComp-LM: In search of the next generation of training sets for language models][5]\n",
    "- [Training Compute-Optimal Large Language Models][7]\n",
    "\n",
    "[0]: https://github.com/karpathy/nanochat\n",
    "[1]: https://kellerjordan.github.io/posts/muon/\n",
    "[2]: https://arxiv.org/pdf/2310.17813\n",
    "[3]: https://arxiv.org/abs/2204.02311\n",
    "[4]: https://arxiv.org/abs/2406.17557\n",
    "[5]: https://arxiv.org/abs/2406.11794\n",
    "[6]: https://arxiv.org/abs/2502.16982\n",
    "[7]: https://arxiv.org/abs/2203.15556"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5884fe2",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPUãŒåˆ©ç”¨ã§ãã¾ã›ã‚“\"\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "\n",
    "if \"A100\" in device_name:\n",
    "    gpu_name = \"A100\" # 40GB -> 67æ™‚é–“~\n",
    "elif \"T4\" in device_name:\n",
    "    gpu_name = \"T4\" # 16GB -> å¿è€ãŒå¿…è¦ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«90åˆ†ãã‚‰ã„ã‹ã‹ã‚‹ï¼‰\n",
    "elif \"5090\" in device_name:\n",
    "    gpu_name = \"RTX5090\" # 32GB -> 74æ™‚é–“~\n",
    "else:\n",
    "    raise NotImplementedError # æœªæ¤œè¨¼\n",
    "\n",
    "# å…¬å¼ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆspeedrun.shï¼‰ã®å ´åˆã€8å°ã®H100 GPUï¼ˆVRAM80GBï¼‰ã§4æ™‚é–“ã‹ã‹ã‚‹ï¼ˆ~$100ï¼‰\n",
    "# GPT-3ç´šã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆrun1000.shï¼‰ã®å ´åˆã€8å°ã®H100 GPUã§32æ™‚é–“ã‹ã‹ã‚‹ï¼ˆ~$1000ï¼‰\n",
    "gpu_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãŠè©¦ã—ãƒ¢ãƒ¼ãƒ‰\n",
    "\n",
    "# æœ‰åŠ¹åŒ–ã™ã‚‹ã¨ä»¥ä¸‹ã®å¤‰æ›´ãŒåŠ ã‚ã‚‹:\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º 24GB -> 800MB\n",
    "# BPBè©•ä¾¡ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•° 250 -> 10\n",
    "# COREãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•° 2000 -> 10 \n",
    "# ã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•° 2000 -> 10\n",
    "\n",
    "OTAMESHI_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "%pip install maturin\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"nanochat\"):\n",
    "    !git clone https://github.com/karpathy/nanochat\n",
    "\n",
    "try:\n",
    "    # Google Colabã®å ´åˆ\n",
    "    from google.colab import userdata\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Rustã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && . \"$HOME/.cargo/env\"\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # maturin build --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸwhlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # pip install nanochat/rustbpe/target/wheels/*.whl\n",
    "\n",
    "    if not os.path.exists(\"nanochat/rustbpe/target\"):\n",
    "        raise FileNotFoundError(\"rustbpeã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "except ImportError:\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
    "    !maturin develop --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "import rustbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "import logging as logging\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = 'ğŸŸ¦'\n",
    "        case logging.INFO:\n",
    "            level = 'ğŸŸ©'\n",
    "        case logging.WARNING:\n",
    "            level = 'ğŸŸ¨'\n",
    "        case logging.ERROR:\n",
    "            level = 'ğŸŸ¥'\n",
    "        case logging.CRITICAL:\n",
    "            level = 'ğŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.debug(\"ãƒ­ã‚°ã‚’åˆæœŸåŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "\n",
    "def get_base_dir():\n",
    "    \"\"\"\n",
    "    ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
    "    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€~/.cache/nanochat\n",
    "    NANOCHAT_BASE_DIRç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½\n",
    "\n",
    "    Returns:\n",
    "        str: ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"NANOCHAT_BASE_DIR\"):\n",
    "        nanochat_dir = os.environ.get(\"NANOCHAT_BASE_DIR\")\n",
    "    else:\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        cache_dir = os.path.join(home_dir, \".cache\")\n",
    "        nanochat_dir = os.path.join(cache_dir, \"nanochat\")\n",
    "    os.makedirs(nanochat_dir, exist_ok=True)\n",
    "    return nanochat_dir\n",
    "\n",
    "logger.debug(f\"ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {get_base_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00863e",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3e368",
   "metadata": {},
   "source": [
    "äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã¨ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "[karpathy/fineweb-edu-100b-shuffle][2]ã‚’ä½¿ç”¨:\n",
    "\n",
    "- [FineWeb-EDUãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ][1]ã‚’ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆæ–­ç‰‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã«å†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "- å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¯25ä¸‡æ–‡å­—ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã€ç´„100MB\n",
    "- åˆè¨ˆã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯1822\n",
    "- depth=20ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯240ã§ã€ç´„24GB\n",
    "- å¿…è¦ãªã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯[Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡][3]ã§æ±‚ã‚ã‚‹ï¼ˆè¨“ç·´ã«å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•° = 20 * ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰\n",
    "    1. 20å±¤ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ561Mï¼ˆ5å„„6100ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨ã™ã‚‹ã¨ã€20 * 561M = 11.2Bï¼ˆ11.2å„„ï¼‰ãƒˆãƒ¼ã‚¯ãƒ³ãŒå¿…è¦\n",
    "    1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ€§èƒ½ãŒ4.8æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã™ã‚‹ã¨ã€11.2 * 4.8 = 54Bï¼ˆ540å„„ï¼‰æ–‡å­—ãŒå¿…è¦\n",
    "    1. ã‚·ãƒ£ãƒ¼ãƒ‰ã‚ãŸã‚Š250Mï¼ˆ2å„„5000ä¸‡ï¼‰æ–‡å­—ã§ã‚ã‚‹ãŸã‚ã€54B / 250M = 216ã‚·ãƒ£ãƒ¼ãƒ‰ãŒæœ€ä½å¿…è¦\n",
    "    1. ã‚­ãƒªã‚ˆã240ã‚·ãƒ£ãƒ¼ãƒ‰ãŒå¿…è¦\n",
    "\n",
    "[1]: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "[2]: https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle\n",
    "[3]: https://arxiv.org/abs/2203.15556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e247ab87",
   "metadata": {},
   "source": [
    "### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4501892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®URL\n",
    "# æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚å¯èƒ½\n",
    "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"\n",
    "logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆURL {BASE_URL=}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¤§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "# fineweb-edu-100b-shuffleã®å ´åˆã€1823å€‹ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯0ã‹ã‚‰1822ã¾ã§\n",
    "MAX_SHARD = 1822\n",
    "logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¤§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {MAX_SHARD=}\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®æ•°\n",
    "num_files = 8 if OTAMESHI_MODE else 240\n",
    "logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰æ•° {num_files=}\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "num_workers = 4\n",
    "logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•° {num_workers=}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "base_dir = get_base_dir()\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {DATA_DIR=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0fa12c",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0af98d",
   "metadata": {},
   "source": [
    "è¤‡æ•°ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ä¸¦åˆ—ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®é–¢æ•°ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2749ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_filename(index):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        index (int): ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    Returns:\n",
    "        str: ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "    \"\"\"\n",
    "    return f\"shard_{index:05d}.parquet\"\n",
    "\n",
    "index_to_filename(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_file(index):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        index (int): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    Returns:\n",
    "        bool: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹\n",
    "    \"\"\"\n",
    "    logger.debug(f\"ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ {index=}\")\n",
    "\n",
    "    # 1) åˆæœŸåŒ–\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    logger.debug(f\"ä¿å­˜å…ˆ {filepath=}\")\n",
    "\n",
    "    # æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "    if os.path.exists(filepath):\n",
    "        logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— {index=}\")\n",
    "        return True\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’ä½œæˆ\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL {url=}\")\n",
    "\n",
    "    # 2) ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å›ã®ãƒªãƒˆãƒ©ã‚¤ï¼‰\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ¢ãƒ¼ãƒ‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "            logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ {attempt}/{max_attempts} {index=}\")\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            temp_path = filepath + f\".tmp\"\n",
    "            logger.debug(f\"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ {temp_path=}\")\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            os.rename(temp_path, filepath)\n",
    "            logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† {index=}\")\n",
    "            return True\n",
    "\n",
    "        except (requests.RequestException, IOError) as e:\n",
    "            logger.warning(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ {e} {index=} {attempt=}\")\n",
    "\n",
    "            # ä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤\n",
    "            for path in [filepath + f\".tmp\", filepath]:\n",
    "                if os.path.exists(path):\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # ãƒãƒƒã‚¯ã‚ªãƒ•\n",
    "            # 2^0, 2^1, 2^2, ...ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt\n",
    "                logger.debug(f\"ãƒãƒƒã‚¯ã‚ªãƒ•: {wait_time} ç§’å¾…æ©Ÿä¸­\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                logger.debug(f\"ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "download_single_file(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a75980",
   "metadata": {},
   "source": [
    "### ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "\n",
    "num = MAX_SHARD + 1 if num_files == -1 else min(num_files, MAX_SHARD + 1)\n",
    "ids_to_download = list(range(num))\n",
    "logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {ids_to_download=}\")\n",
    "\n",
    "# 2) ä¸¦åˆ—å‡¦ç†ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\n",
    "\n",
    "with Pool(processes=num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\n",
    "# 3) çµæœã‚’è¡¨ç¤º\n",
    "\n",
    "successful = sum(1 for success in results if success)\n",
    "success_rate = successful / len(ids_to_download)\n",
    "logger.debug(f\"ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† {success_rate=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcf16b",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d486f",
   "metadata": {},
   "source": [
    "rustbpeã‚’ãƒ©ãƒƒãƒ—ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹RustBPETokenizerã‚’å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rustbpe\n",
    "import tiktoken\n",
    "import os\n",
    "import copy\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412f1687",
   "metadata": {},
   "source": [
    "#### è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³\n",
    "\n",
    "SPECIAL_TOKENS = [\n",
    "    # äº‹å‰å­¦ç¿’ã§ä½¿ç”¨\n",
    "    \"<|bos|>\", # æ–‡ã®é–‹å§‹\n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ä½¿ç”¨\n",
    "    \"<|user_start|>\", # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "    \"<|user_end|>\",\n",
    "    \"<|assistant_start|>\", # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "    \"<|assistant_end|>\",\n",
    "    \"<|python_start|>\", # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒPython REPLãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™\n",
    "    \"<|python_end|>\",\n",
    "    \"<|output_start|>\", # Python REPLãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å‡ºåŠ›ã‚’è¿”ã™\n",
    "    \"<|output_end|>\",\n",
    "]\n",
    "\n",
    "logger.debug(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ•° {len(SPECIAL_TOKENS)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ­£è¦è¡¨ç¾ã‚’è¨­å®š\n",
    "\n",
    "# GPT-4ã¨åŒã˜\n",
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "logger.debug(f\"äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ­£è¦è¡¨ç¾: {SPLIT_PATTERN=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6eefa4",
   "metadata": {},
   "source": [
    "### RustBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RustBPETokenizer:\n",
    "    \"\"\"\n",
    "    rustbpeã®ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹\n",
    "    è¨“ç·´æ™‚ã¯rustbpeã‚’ä½¿ç”¨ã—ã€æ¨è«–æ™‚ã¯tiktokenã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enc, bos_token):\n",
    "        logger.debug(f\"RustBPETokenizeråˆæœŸåŒ–é–‹å§‹ {enc=} {bos_token=}\")\n",
    "\n",
    "        # tiktokenã®Encodingã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        self.enc = enc \n",
    "\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        self.bos_token_id = self.encode_special(bos_token)\n",
    "        logger.debug(f\"RustBPETokenizeråˆæœŸåŒ–å®Œäº† {self.bos_token_id=}\")\n",
    "\n",
    "    @classmethod # ã‚¯ãƒ©ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦å®šç¾©\n",
    "    def train_from_iterator(cls, text_iterator, vocab_size):\n",
    "        \"\"\"\n",
    "        rustbpeã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´ã—ã€tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            text_iterator (iterable): ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿\n",
    "            vocab_size (int): èªå½™æ•°\n",
    "        Returns:\n",
    "            RustBPETokenizer: è¨“ç·´æ¸ˆã¿ã®RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(f\"RustBPETokenizerã®è¨“ç·´é–‹å§‹ {vocab_size=}\")\n",
    "\n",
    "        # 1) rustbpeã‚’è¨“ç·´\n",
    "\n",
    "        # rustbpeã®Tokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "        tokenizer = rustbpe.Tokenizer()\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯å¾Œã§__init__ã§æŒ¿å…¥ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯è¨“ç·´ã—ãªã„\n",
    "        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n",
    "\n",
    "        assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n",
    "\n",
    "        # 2) tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰\n",
    "\n",
    "        # äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        pattern = tokenizer.get_pattern()\n",
    "\n",
    "        # tiktokenã«å¯¾å¿œã—ãŸãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã‚’ä½œæˆ\n",
    "        # {ãƒã‚¤ãƒˆåˆ—: ãƒãƒ¼ã‚¸ã®å„ªå…ˆé †ä½ãƒ©ãƒ³ã‚¯}\n",
    "        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n",
    "        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n",
    "\n",
    "        # tiktokenã«å¯¾å¿œã—ãŸç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¾æ›¸ã‚’ä½œæˆ\n",
    "        # {ãƒˆãƒ¼ã‚¯ãƒ³å: ãƒˆãƒ¼ã‚¯ãƒ³ID}\n",
    "        tokens_offset = len(mergeable_ranks)\n",
    "        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n",
    "\n",
    "        # tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰\n",
    "        enc = tiktoken.Encoding(\n",
    "            name=\"rustbpe\",\n",
    "            pat_str=pattern,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=special_tokens,\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"RustBPETokenizerã®è¨“ç·´å®Œäº†\")\n",
    "\n",
    "        # RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™\n",
    "        # clsã§ã“ã®ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãŒå‘¼ã°ã‚Œã‚‹\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, tokenizer_dir):\n",
    "        \"\"\"\n",
    "        è¨“ç·´æ¸ˆã¿ã®RustBPETokenizerã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "\n",
    "        Args:\n",
    "            tokenizer_dir (str): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        Returns:\n",
    "            RustBPETokenizer: èª­ã¿è¾¼ã‚“ã RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(f\"RustBPETokenizerã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿é–‹å§‹ {tokenizer_dir=}\")\n",
    "\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {pickle_path=}\")\n",
    "\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            enc = pickle.load(f)\n",
    "\n",
    "        logger.debug(f\"RustBPETokenizerã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº† {enc=}\")\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tiktoken_name):\n",
    "        \"\"\"\n",
    "        å­¦ç¿’æ¸ˆã¿ã®tiktokenãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "        Args:\n",
    "            tiktoken_name (str): tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å\n",
    "        Returns:\n",
    "            RustBPETokenizer: èª­ã¿è¾¼ã‚“ã RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"å­¦ç¿’æ¸ˆã¿ã®tiktokenãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {tiktoken_name=}\")\n",
    "\n",
    "        # https://github.com/openai/tiktoken/blob/eedc8563/tiktoken_ext/openai_public.py\n",
    "        enc = tiktoken.get_encoding(tiktoken_name)\n",
    "\n",
    "        # nanochatã§ã¯<|bos|>ã‚’ä½¿ç”¨ã™ã‚‹ãŒã€tiktokenã®gpt2ãªã©ã§ã¯<|endoftext|>ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŸã‚\n",
    "\n",
    "        logger.debug(f\"å­¦ç¿’æ¸ˆã¿ã®tiktokenãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿å®Œäº† {enc=}\")\n",
    "        return cls(enc, \"<|endoftext|>\")\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.enc.n_vocab\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        return self.enc.special_tokens_set\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        return self.enc.decode([id])\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def encode_special(self, text):\n",
    "        return self.enc.encode_single_token(text)\n",
    "\n",
    "    def get_bos_token_id(self):\n",
    "        return self.bos_token_id\n",
    "\n",
    "    def encode(self, text, prepend=None, append=None, num_threads=8):\n",
    "        \"\"\"\n",
    "        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³IDã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            text (str or list[str]): ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "            prepend (str or int, optional): å…ˆé ­ã«è¿½åŠ ã™ã‚‹ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰\n",
    "            append (str or int, optional): æœ«å°¾ã«è¿½åŠ ã™ã‚‹ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰\n",
    "            num_threads (int, optional): ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ™‚ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8ï¼‰\n",
    "        Returns:\n",
    "            list[int] or list[list[int]]: ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é–‹å§‹ {len(text)=} {prepend=} {append=} {num_threads=}\")\n",
    "\n",
    "        # 1) ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š\n",
    "\n",
    "        # å‡ºåŠ›ã®å…ˆé ­ã«ä»˜ã‘ã‚‹ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³\n",
    "        if prepend is not None:\n",
    "            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n",
    "            logger.debug(f\"{prepend_id=}\") # 65527\n",
    "\n",
    "        # å‡ºåŠ›ã®æœ«å°¾ã«ä»˜ã‘ã‚‹ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "        # ãªã—\n",
    "        if append is not None:\n",
    "            append_id = append if isinstance(append, int) else self.encode_special(append)\n",
    "            logger.debug(f\"{append_id=}\") \n",
    "\n",
    "        # 2) ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "\n",
    "        # å…¥åŠ›ãŒæ–‡å­—åˆ—ã®å ´åˆ\n",
    "        if isinstance(text, str):\n",
    "\n",
    "            # tiktokenã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            ids = self.enc.encode_ordinary(text)\n",
    "\n",
    "            if prepend is not None:\n",
    "                # å…ˆé ­ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                ids.insert(0, prepend_id)\n",
    "\n",
    "            if append is not None:\n",
    "                # æœ«å°¾ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                ids.append(append_id)\n",
    "\n",
    "        # å…¥åŠ›ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "        elif isinstance(text, list):\n",
    "\n",
    "            # tiktokenã§ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            ids = self.enc.encode_ordinary_batch(text, num_threads=num_threads)\n",
    "\n",
    "            if prepend is not None:\n",
    "                # å…ˆé ­ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                for ids_row in ids:\n",
    "                    ids_row.insert(0, prepend_id)\n",
    "\n",
    "            if append is not None:\n",
    "                # æœ«å°¾ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                for ids_row in ids:\n",
    "                    ids_row.append(append_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
    "\n",
    "        logger.debug(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº† {len(ids)=}\")\n",
    "        return ids\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            ids (list[int]): ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        Returns:\n",
    "            str: ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹ {len(ids)=}\")\n",
    "\n",
    "        # tiktokenã§ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "        res = self.enc.decode(ids)\n",
    "\n",
    "        logger.debug(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† {len(res)=}\")\n",
    "        return res\n",
    "\n",
    "    def save(self, tokenizer_dir):\n",
    "        \"\"\"\n",
    "        è¨“ç·´å¾Œã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            tokenizer_dir (str): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜é–‹å§‹ {tokenizer_dir=}\")\n",
    "\n",
    "        # 1) ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {pickle_path=}\")\n",
    "\n",
    "        # 2) pickleã§ä¿å­˜\n",
    "\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(self.enc, f)\n",
    "\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜å®Œäº† {pickle_path=}\")\n",
    "\n",
    "    def render_conversation(self, conversation, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹\n",
    "        äº‹å‰å­¦ç¿’ã§ã¯ä½¿ã‚ãªã„\n",
    "\n",
    "        Args:\n",
    "            conversation: dict ãƒãƒ£ãƒƒãƒˆä¼šè©±ãƒ‡ãƒ¼ã‚¿\n",
    "            max_tokens: int æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        Returns:\n",
    "            ids: list[int] ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            mask: list[int] åŒã˜é•·ã•ã®ãƒã‚¹ã‚¯ãƒªã‚¹ãƒˆã€mask=1ã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒå­¦ç¿’ã™ã¹ããƒˆãƒ¼ã‚¯ãƒ³\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ä¼šè©±ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°é–‹å§‹ {conversation=} {max_tokens=}\")\n",
    "\n",
    "        # ids, masks that we will return and a helper function to help build them up.\n",
    "        ids, mask = [], []\n",
    "\n",
    "        def add_tokens(token_ids, mask_val):\n",
    "            if isinstance(token_ids, int):\n",
    "                token_ids = [token_ids]\n",
    "            ids.extend(token_ids)\n",
    "            mask.extend([mask_val] * len(token_ids))\n",
    "\n",
    "        # sometimes the first message is a system message...\n",
    "        # => just merge it with the second (user) message\n",
    "        if conversation[\"messages\"][0][\"role\"] == \"system\":\n",
    "            # some conversation surgery is necessary here for now...\n",
    "            conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "            messages = conversation[\"messages\"]\n",
    "            assert messages[1][\"role\"] == \"user\", \"System message must be followed by a user message\"\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        else:\n",
    "            messages = conversation[\"messages\"]\n",
    "\n",
    "        assert len(messages) >= 1, f\"Conversation has less than 1 message: {messages}\"\n",
    "\n",
    "        # fetch all the special tokens we need\n",
    "        bos = self.get_bos_token_id()\n",
    "        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n",
    "        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n",
    "        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n",
    "        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n",
    "\n",
    "        # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "        add_tokens(bos, 0)\n",
    "\n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é †ã«å‡¦ç†\n",
    "        for i, message in enumerate(messages):\n",
    "\n",
    "            # some sanity checking here around assumptions, to prevent footguns\n",
    "            must_be_from = \"user\" if i % 2 == 0 else \"assistant\"\n",
    "            assert message[\"role\"] == must_be_from, f\"Message {i} is from {message['role']} but should be from {must_be_from}\"\n",
    "\n",
    "            # content can be either a simple string or a list of parts (e.g. containing tool calls)\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ\n",
    "            if message[\"role\"] == \"user\":\n",
    "                assert isinstance(content, str), \"User messages are simply expected to be strings\"\n",
    "                value_ids = self.encode(content)\n",
    "\n",
    "                # æå¤±è¨ˆç®—ã«å«ã‚ãªã„\n",
    "                add_tokens(user_start, 0)\n",
    "                add_tokens(value_ids, 0)\n",
    "                add_tokens(user_end, 0)\n",
    "\n",
    "            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "\n",
    "                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                add_tokens(assistant_start, 0)\n",
    "\n",
    "                if isinstance(content, str):\n",
    "                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                    value_ids = self.encode(content)\n",
    "\n",
    "                    # æå¤±è¨ˆç®—ã«å«ã‚ã‚‹\n",
    "                    add_tokens(value_ids, 1)\n",
    "\n",
    "                elif isinstance(content, list):\n",
    "                    for part in content:\n",
    "                        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                        value_ids = self.encode(part[\"text\"])\n",
    "\n",
    "                        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ\n",
    "                        if part[\"type\"] == \"text\":\n",
    "\n",
    "                            # æå¤±è¨ˆç®—ã«å«ã‚ã‚‹\n",
    "                            add_tokens(value_ids, 1)\n",
    "\n",
    "                        # Pythonã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å ´åˆ\n",
    "                        elif part[\"type\"] == \"python\":\n",
    "                            # ãƒ„ãƒ¼ãƒ«ã®é–‹å§‹ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                            add_tokens(python_start, 1)\n",
    "                            add_tokens(value_ids, 1)\n",
    "                            add_tokens(python_end, 1)\n",
    "\n",
    "                        # Pythonã®å‡ºåŠ›ã®å ´åˆ\n",
    "                        elif part[\"type\"] == \"python_output\":\n",
    "                            # ãƒ„ãƒ¼ãƒ«ã®å‡ºåŠ›ã®é–‹å§‹ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                            # æ¨è«–æ™‚ã¯Pythonã®å‡ºåŠ›ã‚’ç”¨ã„ã‚‹ãŸã‚æå¤±è¨ˆç®—ã«å«ã‚ãªã„\n",
    "                            add_tokens(output_start, 0)\n",
    "                            add_tokens(value_ids, 0)\n",
    "                            add_tokens(output_end, 0)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown part type: {part['type']}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown content type: {type(content)}\")\n",
    "                add_tokens(assistant_end, 1)\n",
    "\n",
    "        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "        ids = ids[:max_tokens]\n",
    "        mask = mask[:max_tokens]\n",
    "\n",
    "        logger.debug(f\"ä¼šè©±ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº† {ids=} {mask=}\")\n",
    "        return ids, mask\n",
    "\n",
    "    def visualize_tokenization(self, ids, mask, with_token_id=False):\n",
    "        \"\"\"\n",
    "        render_conversationã®çµæœã®å¯è¦–åŒ–\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        RED = '\\033[91m'\n",
    "        GREEN = '\\033[92m'\n",
    "        RESET = '\\033[0m'\n",
    "        GRAY = '\\033[90m'\n",
    "        tokens = []\n",
    "        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n",
    "            token_str = self.decode([token_id])\n",
    "            color = GREEN if mask_val == 1 else RED\n",
    "            tokens.append(f\"{color}{token_str}{RESET}\")\n",
    "            if with_token_id:\n",
    "                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n",
    "        return '|'.join(tokens)\n",
    "\n",
    "    def render_for_completion(self, conversation):\n",
    "        \"\"\"\n",
    "        Assistantã®è£œå®Œã‚’ä¿ƒã™ãŸã‚ã«ä¼šè©±ã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "        ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "        \"\"\"\n",
    "        # We have some surgery to do: we need to pop the last message (of the Assistant)\n",
    "        conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "        messages = conversation[\"messages\"]\n",
    "        assert messages[-1][\"role\"] == \"assistant\", \"Last message must be from the Assistant\"\n",
    "        messages.pop() # remove the last message (of the Assistant) inplace\n",
    "\n",
    "        # Now tokenize the conversation\n",
    "        ids, mask = self.render_conversation(conversation)\n",
    "\n",
    "        # Finally, to prime the Assistant for a completion, append the Assistant start token\n",
    "        assistant_start = self.encode_special(\"<|assistant_start|>\")\n",
    "        ids.append(assistant_start)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551ac0",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa89209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763428e6",
   "metadata": {},
   "source": [
    "#### è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10Bï¼ˆ100å„„ï¼‰æ–‡å­—ã ãŒã€å¤šã„ã®ã§2Bï¼ˆ20å„„ï¼‰æ–‡å­—ã«è¨­å®š\n",
    "max_chars = 2_000_000_000\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æ–‡å­—æ•°: {max_chars=:,}\")\n",
    "\n",
    "# 1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10,000æ–‡å­—\n",
    "doc_cap = 10_000\n",
    "logger.info(f\"1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°: {doc_cap=:,}\")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯65,536ï¼ˆ2ã®16ä¹—ï¼‰\n",
    "vocab_size = 65_536\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°: {vocab_size=:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41540fb5",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee6d917",
   "metadata": {},
   "source": [
    "parquetå½¢å¼ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€é–¢æ•°ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquet_files(data_dir=None):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™\n",
    "    Args:\n",
    "        data_dir (str, optional): ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®DATA_DIRã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "    Returns:\n",
    "        List[str]: parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "    data_dir = DATA_DIR if data_dir is None else data_dir\n",
    "\n",
    "    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    parquet_files = sorted([\n",
    "        f for f in os.listdir(data_dir)\n",
    "        if f.endswith('.parquet') and not f.endswith('.tmp')\n",
    "    ])\n",
    "\n",
    "    # ãƒ•ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›\n",
    "    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]\n",
    "\n",
    "    return parquet_paths\n",
    "\n",
    "list_parquet_files()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08792b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquets_iter_batched(split, start=0, step=1):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒƒãƒã§èª­ã¿è¾¼ã‚€ãŸã‚ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "\n",
    "    splitã¯ã€\"train\"ã¾ãŸã¯\"val\"ã‚’æŒ‡å®š\n",
    "    \"train\"ã®å ´åˆã€æœ€åˆã®N-1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    \"val\"ã®å ´åˆã€æœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    startã¨stepã¯ã€åˆ†æ•£ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆDistributed Data Processingï¼‰ã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹éš›ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        split (str): \"train\"ã¾ãŸã¯\"val\"\n",
    "        start (int, optional): é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "        step (int, optional): ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "    Yields:\n",
    "        List[str]: å„ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {split=} {start=} {step=}\")\n",
    "\n",
    "    # 1) ä½¿ç”¨ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    # ã™ã¹ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    parquet_paths = list_parquet_files()\n",
    "    logger.debug(f\"å…¨ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ« {parquet_paths=}\")\n",
    "\n",
    "    # splitã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ\n",
    "    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n",
    "    logger.debug(f\"ä½¿ç”¨ã™ã‚‹parquetãƒ•ã‚¡ã‚¤ãƒ«: {parquet_paths=}\")\n",
    "\n",
    "    # 2) å„ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã§å‡ºåŠ›ã™ã‚‹\n",
    "\n",
    "    for filepath in parquet_paths:\n",
    "        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        pf = pq.ParquetFile(filepath)\n",
    "        logger.debug(f\"{filepath=} {pf.num_row_groups=}\")\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¡Œã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆrow groupsï¼‰ã§ãƒ«ãƒ¼ãƒ—\n",
    "        # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã¯ã€æ•°åƒã‹ã‚‰æ•°ä¸‡è¡Œã®ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯\n",
    "        for rg_idx in range(start, pf.num_row_groups, step):\n",
    "\n",
    "            # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’èª­ã¿è¾¼ã‚€\n",
    "            # pyarrow.lib.Table, 1024è¡Œ\n",
    "            rg = pf.read_row_group(rg_idx)\n",
    "            logger.debug(f\"{type(rg)=} {len(rg)=:,}\")\n",
    "\n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—ã—ã¦yield\n",
    "            # list, 1024è¡Œ, 4278æ–‡å­—\n",
    "            texts = rg.column('text').to_pylist()\n",
    "            logger.debug(f\"{type(texts)=} {len(texts)=:,} {len(texts[0])=:,}\")\n",
    "\n",
    "            # 1ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™\n",
    "            yield texts\n",
    "\n",
    "text = parquets_iter_batched(\"train\")\n",
    "first_row_group = next(text)\n",
    "first_row_group[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator():\n",
    "    \"\"\"\n",
    "    ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’1ãšã¤ä¾›çµ¦ã™ã‚‹ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿\n",
    "\n",
    "    Returns:\n",
    "        Iterator[str]: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿\n",
    "\n",
    "    1) Flatten the batches into a single iterator\n",
    "    2) Crop every document to args.doc_cap characters\n",
    "    3) Break when we've seen args.max_chars characters\n",
    "    \"\"\"\n",
    "    # æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
    "    nchars = 0\n",
    "\n",
    "    # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆãƒãƒƒãƒï¼‰ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "\n",
    "        # ãƒãƒƒãƒã«å«ã¾ã‚Œã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for doc in batch:\n",
    "\n",
    "            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯å¹³å‡9,000æ–‡å­—ç¨‹åº¦\n",
    "            doc_text = doc\n",
    "            logger.debug(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾— {len(doc_text)=:,}\")\n",
    "\n",
    "            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ–‡å­—æ•°ãŒ10,000æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã€åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "            if len(doc_text) > doc_cap:\n",
    "                doc_text = doc_text[:doc_cap]\n",
    "                logger.debug(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ {len(doc_text)=:,}\")\n",
    "\n",
    "            # æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°\n",
    "            nchars += len(doc_text)\n",
    "            logger.debug(f\"ç´¯ç©æ–‡å­—æ•°ã‚’æ›´æ–° {nchars=}\")\n",
    "\n",
    "            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§å‡ºåŠ›\n",
    "            yield doc_text\n",
    "\n",
    "            # æœ€å¤§æ–‡å­—æ•°ã«é”ã—ãŸå ´åˆã€çµ‚äº†\n",
    "            if nchars > max_chars:\n",
    "                logger.debug(f\"æœ€å¤§æ–‡å­—æ•°ã«é”ã—ãŸãŸã‚çµ‚äº† {nchars=}\")\n",
    "                return\n",
    "\n",
    "text_iter = text_iterator()\n",
    "first_doc = next(text_iter)\n",
    "len(first_doc), first_doc[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0388e93",
   "metadata": {},
   "source": [
    "### è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "base_dir = get_base_dir()\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {tokenizer_dir}\")\n",
    "\n",
    "# è¨“ç·´æ¸ˆã¿ã§ãªã„å ´åˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    logger.debug(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã‚’é–‹å§‹\")\n",
    "\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "    text_iter = text_iterator()\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "    tokenizer = RustBPETokenizer.train_from_iterator(text_iter, vocab_size)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    # RTX5090ã®å ´åˆã€26ç§’ç¨‹åº¦\n",
    "    train_time = t1 - t0\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´å®Œäº† {train_time=}ç§’\")\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜\n",
    "    tokenizer.save(tokenizer_dir)\n",
    "\n",
    "# è¨“ç·´æ¸ˆã¿ã®å ´åˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "else:\n",
    "    logger.debug(\"æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\")\n",
    "    tokenizer = RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ¤œè¨¼\n",
    "\n",
    "test_text = \"\"\"Hello world! This is a test.\n",
    "Numbers: 123, 4567, 89\n",
    "Contractions: I'm, you're, it's\n",
    "Special chars: @#$%^&*()\n",
    "Unicode: ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ ğŸŒ\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "logger.debug(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœ: {encoded}\")\n",
    "\n",
    "decoded = tokenizer.decode(encoded)\n",
    "logger.debug(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœ: {decoded}\")\n",
    "assert decoded == test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5c94b",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒã‚¤ãƒˆæ•°ã®å¯¾å¿œãƒªã‚¹ãƒˆã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8e549",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã®æå¤±è¨ˆç®—ã‚’ã€ãƒã‚¤ãƒˆæ•°ã”ã¨ã®å¹³å‡æå¤±ã«ã™ã‚‹ï¼ˆBPBè©•ä¾¡ï¼‰ãŸã‚ã®å¯¾å¿œãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "\n",
    "ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ãŸå ´åˆã§ã‚‚ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å…¬å¹³ã«æ¯”è¼ƒã§ãã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°: {vocab_size=}\")\n",
    "\n",
    "special_set = set(tokenizer.get_special_tokens())\n",
    "logger.debug(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(special_set)=}\")\n",
    "\n",
    "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒã‚¤ãƒˆæ•°ã®å¯¾å¿œãƒªã‚¹ãƒˆã®ä¿å­˜ãƒ‘ã‚¹: {token_bytes_path=}\")\n",
    "\n",
    "# å¯¾å¿œãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦ã„ãªã„å ´åˆ\n",
    "if not os.path.exists(token_bytes_path):\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "    token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n",
    "    token_bytes = []\n",
    "\n",
    "    # å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã§ãƒ«ãƒ¼ãƒ—\n",
    "    for token_id in range(vocab_size):\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¯¾å¿œã™ã‚‹æ–‡å­—åˆ—ã‚’å–å¾—\n",
    "        token_str = token_strings[token_id]\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ\n",
    "        if token_str in special_set:\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯æå¤±è¨ˆç®—å¯¾è±¡å¤–ãªãŸã‚ã€0ãƒã‚¤ãƒˆã¨ã—ã¦å¯¾å¿œãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            token_bytes.append(0)\n",
    "        else:\n",
    "            # UTF-8ã®ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›ã—ã€ãã®ãƒã‚¤ãƒˆæ•°ã‚’å¯¾å¿œãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            id_bytes = len(token_str.encode(\"utf-8\"))\n",
    "            token_bytes.append(id_bytes)\n",
    "\n",
    "    # å¯¾å¿œãƒªã‚¹ãƒˆã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "    token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n",
    "\n",
    "    # å¯¾å¿œãƒªã‚¹ãƒˆã‚’ä¿å­˜\n",
    "    with open(token_bytes_path, \"wb\") as f:\n",
    "        torch.save(token_bytes, f)\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# å¯¾å¿œãƒªã‚¹ãƒˆã‚’ä½œæˆæ¸ˆã¿ã®å ´åˆ\n",
    "else:\n",
    "    # å¯¾å¿œãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
    "    token_bytes = torch.load(token_bytes_path)\n",
    "\n",
    "# çµ±è¨ˆã‚’è¡¨ç¤º\n",
    "token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®æœ€å°ãƒã‚¤ãƒˆæ•° {int(token_bytes_nonzero.min().item())}\")\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®æœ€å¤§ãƒã‚¤ãƒˆæ•° {int(token_bytes_nonzero.max().item())}\")\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒã‚¤ãƒˆæ•°ã®å¹³å‡ {token_bytes_nonzero.mean().item()}\")\n",
    "logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒã‚¤ãƒˆæ•°ã®æ¨™æº–åå·® {token_bytes_nonzero.std().item()}\")\n",
    "\n",
    "# æ¤œè¨¼\n",
    "token_id = 5000\n",
    "token_str = tokenizer.decode([token_id])\n",
    "num_bytes = token_bytes[token_id].item()\n",
    "logger.debug(f\"Token ID: {token_id}, Token: '{token_str}', Bytes: {num_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1f17",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342cca8",
   "metadata": {},
   "source": [
    "#### è©•ä¾¡ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "news_text = r\"\"\"\n",
    "(Washington, D.C., July 9, 2025)- Yesterday, Mexicoâ€™s National Service of Agro-Alimentary Health, Safety, and Quality (SENASICA) reported a new case of New World Screwworm (NWS) in Ixhuatlan de Madero, Veracruz in Mexico, which is approximately 160 miles northward of the current sterile fly dispersal grid, on the eastern side of the country and 370 miles south of the U.S./Mexico border. This new northward detection comes approximately two months after northern detections were reported in Oaxaca and Veracruz, less than 700 miles away from the U.S. border, which triggered the closure of our ports to Mexican cattle, bison, and horses on May 11, 2025.\n",
    "\n",
    "While USDA announced a risk-based phased port re-opening strategy for cattle, bison, and equine from Mexico beginning as early as July 7, 2025, this newly reported NWS case raises significant concern about the previously reported information shared by Mexican officials and severely compromises the outlined port reopening schedule of five ports from July 7-September 15. Therefore, in order to protect American livestock and our nationâ€™s food supply, Secretary Rollins has ordered the closure of livestock trade through southern ports of entry effective immediately.\n",
    "\n",
    "â€œThe United States has promised to be vigilant â€” and after detecting this new NWS case, we are pausing the planned port reopeningâ€™s to further quarantine and target this deadly pest in Mexico. We must see additional progress combatting NWS in Veracruz and other nearby Mexican states in order to reopen livestock ports along the Southern border,â€ said U.S. Secretary of Agriculture Brooke L. Rollins. â€œThanks to the aggressive monitoring by USDA staff in the U.S. and in Mexico, we have been able to take quick and decisive action to respond to the spread of this deadly pest.â€\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éè‹±èªãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "korean_text = r\"\"\"\n",
    "ì •ì§í•œ ì‚¬ì‹¤ ìœ„ì—, ê³µì •í•œ ì‹œì„ ì„ ë”í•˜ë‹¤\n",
    "Herald Korea Times\n",
    "\n",
    "í—¤ëŸ´ë“œì½”ë¦¬ì•„íƒ€ì„ì¦ˆëŠ” ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™” ë“± í•œêµ­ ì‚¬íšŒ ì „ë°˜ì˜ ì£¼ìš” ì´ìŠˆë¥¼ ì‹¬ë„ ìˆê²Œ ë‹¤ë£¨ëŠ” ì¢…í•© ì˜¨ë¼ì¸ ì‹ ë¬¸ì‚¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë‹¨ìˆœíˆ ë‰´ìŠ¤ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ì‹¤(Fact)ì— ê¸°ë°˜í•œ ì–‘ì¸¡ì˜ ì‹œê°ì„ ê· í˜• ìˆê²Œ ì¡°ëª…í•˜ë©°, ë…ì ì—¬ëŸ¬ë¶„ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ” â€˜ì •ë³´ì˜ ê· í˜•â€™ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "í•œêµ­ ì–¸ë¡ ì˜ ì˜¤ëœ ë¬¸ì œë¡œ ì§€ì ë˜ì–´ ì˜¨ ì •ì¹˜ì  í¸í–¥, ì´ë…ì  ì™œê³¡ì—ì„œ ë²—ì–´ë‚˜\n",
    "ì˜¤ì§ ì •ì§í•¨ê³¼ ê³µì •í•¨ì„ ì›ì¹™ìœ¼ë¡œ ì‚¼ëŠ” ì–¸ë¡ ì„ ì§€í–¥í•©ë‹ˆë‹¤.\n",
    "ì–´ëŠ í•œìª½ì˜ ì£¼ì¥ë§Œì„ í™•ëŒ€í•˜ê±°ë‚˜ ê°ì¶”ì§€ ì•Šê³ ,\n",
    "**ëª¨ë“  ìŸì ì— ëŒ€í•´ â€˜ë¬´ì—‡ì´ ìŸì ì¸ì§€â€™, â€˜ëˆ„ê°€ ë¬´ì—‡ì„ ì£¼ì¥í•˜ëŠ”ì§€â€™, â€˜ì‚¬ì‹¤ì€ ë¬´ì—‡ì¸ì§€â€™**ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n",
    "\n",
    "code_text = r\"\"\"\n",
    "class BasicTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # input text preprocessing\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count up the number of times every consecutive pair appears\n",
    "            stats = get_stats(ids)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°å­¦ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "math_text = r\"\"\"\n",
    "\\documentclass[12pt]{article}\n",
    "\\usepackage{amsmath,amsthm,amssymb}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\n",
    "\\newtheorem{theorem}{Theorem}\n",
    "\\newtheorem*{remark}{Remark}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\begin{center}\n",
    "{\\Large A Cute Identity: The Sum of Cubes is a Square}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{theorem}\n",
    "For every integer $n \\ge 1$,\n",
    "\\[\n",
    "\\sum_{k=1}^{n} k^{3} \\;=\\; \\left(\\frac{n(n+1)}{2}\\right)^{2}.\n",
    "\\]\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}[Proof 1 (Induction)]\n",
    "Let $S(n) = \\sum_{k=1}^{n} k^3$. For $n=1$, $S(1)=1=(1\\cdot 2/2)^2$, so the base case holds.\n",
    "\n",
    "Assume $S(n)=\\big(\\tfrac{n(n+1)}{2}\\big)^2$ for some $n\\ge 1$.\n",
    "Then\n",
    "\\[\n",
    "S(n+1)\n",
    "= S(n) + (n+1)^3\n",
    "= \\left(\\frac{n(n+1)}{2}\\right)^2 + (n+1)^3.\n",
    "\\]\n",
    "Factor out $(n+1)^2$:\n",
    "\\[\n",
    "S(n+1)\n",
    "= (n+1)^2\\left( \\frac{n^2}{4} + (n+1) \\right)\n",
    "= (n+1)^2\\left( \\frac{n^2 + 4n + 4}{4} \\right)\n",
    "= (n+1)^2\\left( \\frac{(n+2)^2}{4} \\right).\n",
    "\\]\n",
    "Thus\n",
    "\\[\n",
    "S(n+1)=\\left(\\frac{(n+1)(n+2)}{2}\\right)^2,\n",
    "\\]\n",
    "which matches the claimed formula with $n$ replaced by $n+1$. By induction, the identity holds for all $n\\ge 1$.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{proof}[Proof 2 (Algebraic telescoping)]\n",
    "Recall the binomial identity\n",
    "\\[\n",
    "(k+1)^4 - k^4 = 4k^3 + 6k^2 + 4k + 1.\n",
    "\\]\n",
    "Summing both sides from $k=0$ to $n$ telescopes:\n",
    "\\[\n",
    "(n+1)^4 - 0^4\n",
    "= \\sum_{k=0}^{n}\\big(4k^3 + 6k^2 + 4k + 1\\big)\n",
    "= 4\\sum_{k=1}^{n}k^3 + 6\\sum_{k=1}^{n}k^2 + 4\\sum_{k=1}^{n}k + (n+1).\n",
    "\\]\n",
    "Using the standard sums\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\n",
    "\\quad\\text{and}\\quad\n",
    "\\sum_{k=1}^{n}k^2 = \\frac{n(n+1)(2n+1)}{6},\n",
    "\\]\n",
    "solve for $\\sum_{k=1}^{n}k^3$ to get\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k^3 = \\left(\\frac{n(n+1)}{2}\\right)^2.\n",
    "\\]\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{remark}\n",
    "Geometrically, the identity says: ``adding up $1^3,2^3,\\dots,n^3$ builds a perfect squareâ€™â€™â€”namely the square of the $n$th triangular number. This is why one sometimes calls it the \\emph{sum-of-cubes is a square} phenomenon.\n",
    "\\end{remark}\n",
    "\n",
    "\\end{document}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç§‘å­¦ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "science_text = r\"\"\"\n",
    "Photosynthesis is a photochemical energy transduction process in which light-harvesting pigmentâ€“protein complexes within the thylakoid membranes of oxygenic phototrophs absorb photons and initiate charge separation at the reaction center, driving the linear electron transport chain from water to NADPâº via photosystem II, the cytochrome bâ‚†f complex, and photosystem I, concomitantly generating a trans-thylakoid proton motive force utilized by chloroplastic ATP synthase. The light-dependent reactions produce ATP and NADPH, which fuel the Calvinâ€“Bensonâ€“Bassham cycle in the stroma, wherein ribulose-1,5-bisphosphate is carboxylated by ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) to form 3-phosphoglycerate, subsequently reduced and regenerated through a series of enzymatic steps, enabling net assimilation of COâ‚‚ into triose phosphates and ultimately carbohydrates. This process is tightly regulated by photoprotective mechanisms, redox feedback, and metabolite flux, representing a central biochemical pathway coupling solar energy capture to the biosphereâ€™s primary productivity.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "\n",
    "train_docs = next(parquets_iter_batched(split=\"train\"))\n",
    "train_text = \"\\n\".join(train_docs)\n",
    "val_docs = next(parquets_iter_batched(split=\"val\"))\n",
    "val_text = \"\\n\".join(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«é›†ç´„\n",
    "\n",
    "all_text = [\n",
    "    (\"news\", news_text),\n",
    "    (\"korean\", korean_text),\n",
    "    (\"code\", code_text),\n",
    "    (\"math\", math_text),\n",
    "    (\"science\", science_text),\n",
    "    (\"fwe-train\", train_text),\n",
    "    (\"fwe-val\", val_text),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf706a49",
   "metadata": {},
   "source": [
    "#### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1803ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer():\n",
    "    \"\"\"\n",
    "    RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™\n",
    "\n",
    "    Returns:\n",
    "        RustBPETokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    # return HuggingFaceTokenizer.from_directory(tokenizer_dir)\n",
    "    return RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8504ea0",
   "metadata": {},
   "source": [
    "#### GPT-2ã¨GPT-4ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ca71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "tokenizer_results = {}\n",
    "vocab_sizes = {}\n",
    "\n",
    "# ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§ãƒ«ãƒ¼ãƒ—\n",
    "for tokenizer_name in [\"gpt2\", \"gpt4\", \"ours\"]:\n",
    "\n",
    "    # 1) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—\n",
    "\n",
    "    if tokenizer_name == \"gpt2\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\")\n",
    "    elif tokenizer_name == \"gpt4\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\")\n",
    "    else:\n",
    "        tokenizer = get_tokenizer()\n",
    "\n",
    "    # èªå½™æ•°ã‚’å–å¾—\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "\n",
    "    # çµæœã‚’åˆæœŸåŒ–\n",
    "    tokenizer_results[tokenizer_name] = {}\n",
    "\n",
    "    # 2) æ¤œè¨¼\n",
    "\n",
    "    for name, text in all_text:\n",
    "\n",
    "        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¨ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è¡Œã„ã€å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text\n",
    "\n",
    "        # UTF-8ã®ãƒã‚¤ãƒˆåˆ—ã«å¤‰æ›\n",
    "        encoded_bytes = text.encode('utf-8')\n",
    "\n",
    "        # ãƒã‚¤ãƒˆæ•°ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "        ratio = len(encoded_bytes) / len(encoded)\n",
    "\n",
    "        # çµæœã‚’ä¿å­˜\n",
    "        tokenizer_results[tokenizer_name][name] = {\n",
    "            'bytes': len(encoded_bytes),\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–\n",
    "\n",
    "print(f\"\\nèªå½™æ•°:\")\n",
    "print(f\"GPT-2: {vocab_sizes['gpt2']}\")\n",
    "print(f\"GPT-4: {vocab_sizes['gpt4']}\")\n",
    "print(f\"Ours: {vocab_sizes['ours']}\")\n",
    "\n",
    "# ANSI color codes\n",
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "def print_comparison(baseline_name, baseline_results, ours_results, all_text):\n",
    "    print(f\"\\n{baseline_name}ã¨ã®æ¯”è¼ƒ:\")\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"{'Text Type':<10} {'Bytes':<8} {baseline_name:<15} {'Ours':<15} {'Relative':<12} {'Better':<10}\")\n",
    "    print(f\"{'':10} {'':8} {'Tokens':<7} {'Ratio':<7} {'Tokens':<7} {'Ratio':<7} {'Diff %':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, text in all_text:\n",
    "        baseline_data = baseline_results[name]\n",
    "        ours_data = ours_results[name]\n",
    "\n",
    "        # Calculate relative difference (positive means ours is better, negative means worse)\n",
    "        # Using tokens: fewer tokens is better, so we calculate (baseline_tokens - ours_tokens) / baseline_tokens\n",
    "        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n",
    "\n",
    "        # Determine which has better compression (higher ratio = better)\n",
    "        if baseline_data['ratio'] > ours_data['ratio']:\n",
    "            baseline_color, ours_color = GREEN, RED\n",
    "            better = baseline_name\n",
    "            diff_color = RED\n",
    "        elif ours_data['ratio'] > baseline_data['ratio']:\n",
    "            baseline_color, ours_color = RED, GREEN\n",
    "            better = \"Ours\"\n",
    "            diff_color = GREEN\n",
    "        else:\n",
    "            baseline_color, ours_color = \"\", \"\"\n",
    "            better = \"Tie\"\n",
    "            diff_color = \"\"\n",
    "\n",
    "        print(f\"{name:<10} {baseline_data['bytes']:<8} \"\n",
    "              f\"{baseline_color}{baseline_data['tokens']:<7}{RESET} \"\n",
    "              f\"{baseline_color}{baseline_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['tokens']:<7}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{diff_color}{relative_diff:+7.1f}%{RESET}     \"\n",
    "              f\"{better:<10}\")\n",
    "\n",
    "# Print comparisons\n",
    "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)\n",
    "print_comparison(\"GPT-4\", tokenizer_results['gpt4'], tokenizer_results['ours'], all_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59e90f",
   "metadata": {},
   "source": [
    "## æœ€é©åŒ–é–¢æ•°ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2299c2",
   "metadata": {},
   "source": [
    "Transformerã®æœ€é©åŒ–é–¢æ•°ã¯[Muonï¼ˆãƒŸãƒ¥ãƒ¼ã‚ªãƒ³, Momentum Orthgonalized by Newton-Schulzï¼‰][1]ã‚’ä½¿ç”¨\n",
    "\n",
    "ãã®ä»–ã®æœ€é©åŒ–é–¢æ•°ã¯AdamWã‚’ä½¿ç”¨\n",
    "\n",
    "[1]: https://arxiv.org/abs/2502.16982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97e6c3",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fddc6",
   "metadata": {},
   "source": [
    "Muonæœ€é©åŒ–é–¢æ•°ã§ä½¿ç”¨ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile # PyTorch2.0ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã‚’ä½¿ç”¨\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©æ³•ã§å‹¾é…ã®ç›´è¡ŒåŒ–ã‚’è¿‘ä¼¼ã™ã‚‹\n",
    "\n",
    "    è¡Œåˆ—Gã®ç›´äº¤è¡Œåˆ—ã¯ã€Gã®ç‰¹ç•°å€¤åˆ†è§£G = USV^Tã«å¯¾ã—ã¦UV^Tã§è¨ˆç®—ã§ãã‚‹\n",
    "    ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚è¿‘ä¼¼æ‰‹æ³•ã‚’ç”¨ã„ã‚‹\n",
    "    åŸç‚¹ã«ãŠã‘ã‚‹å‚¾ãã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«é¸æŠã•ã‚ŒãŸä¿‚æ•°ã‚’æŒã¤5æ¬¡ã®åå¾©æ³•ã‚’æ¡ç”¨\n",
    "\n",
    "    Args: \n",
    "        G (Tensor): ç›´è¡ŒåŒ–ã™ã‚‹è¡Œåˆ—ã€å½¢çŠ¶ã¯(..., m, n)\n",
    "        steps (int): åå¾©å›æ•°\n",
    "    Returns:\n",
    "        Tensor: ç›´äº¤åŒ–ã•ã‚ŒãŸè¡Œåˆ—ã€å½¢çŠ¶ã¯(..., m, n)\n",
    "    \"\"\"\n",
    "\n",
    "    assert G.ndim >= 2\n",
    "\n",
    "    # 5æ¬¡åå¾©æ³•ã®ãŸã‚ã®ä¿‚æ•°\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "\n",
    "    # å‹¾é…ã‚’bfloat16ã«ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "    X = G.bfloat16()\n",
    "\n",
    "    # å®‰å®šåŒ–ã®ãŸã‚è¡Œåˆ—ã‚’æ¨ªé•·ã«ã™ã‚‹\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ï¼ˆæœ€å¤§ã®ç‰¹ç•°å€¤ï¼‰ã‚’1ä»¥ä¸‹ã«æ­£è¦åŒ–\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "\n",
    "    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„åå¾©æ³•ã‚’é©ç”¨\n",
    "    for _ in range(steps):\n",
    "        # A = X X^T\n",
    "        A = X @ X.mT\n",
    "\n",
    "        # B = b(X X^T) + c(X X^T)^2\n",
    "        B = b * A + c * A @ A\n",
    "\n",
    "        # X_k+1 = a X_k + (B) X_k \n",
    "        X = a * X + B @ X\n",
    "\n",
    "    # å…ƒã®å½¢çŠ¶ã«æˆ»ã™\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # ç›´äº¤è¡Œåˆ—ã‚’è¿”ã™\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f17f7",
   "metadata": {},
   "source": [
    "### Muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2075e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muonæœ€é©åŒ–é–¢æ•° https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    å†…éƒ¨çš„ã«æ¨™æº–çš„ãªSGDãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’å®Ÿè¡Œã—ã€ãã®å¾Œã«ç›´äº¤åŒ–ã®å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    ç›´è¡ŒåŒ–ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å„2Dãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ãŒæœ€ã‚‚è¿‘ã„ç›´äº¤è¡Œåˆ—ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹\n",
    "    å„æ›´æ–°ã‚’åŠ¹ç‡çš„ã«ç›´äº¤åŒ–ã™ã‚‹ãŸã‚ã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    ã“ã‚Œã«ã‚ˆã‚Šã€GPUä¸Šã§bfloat16ã§å®‰å®šã—ã¦å®Ÿè¡Œã§ãã‚‹åˆ©ç‚¹ãŒã‚ã‚‹\n",
    "\n",
    "    æ³¨æ„:\n",
    "    - ã“ã®æœ€é©åŒ–é–¢æ•°ã¯ã€åŸ‹ã‚è¾¼ã¿å±¤ã€æœ€çµ‚å…¨çµåˆå±¤ã€0æ¬¡å…ƒãƒ»1æ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯ä½¿ç”¨ã—ãªã„ï¼ˆAdamWãªã©ã‚’ä½¿ç”¨ï¼‰\n",
    "    - 4Dã®ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã«ä½¿ç”¨ã™ã‚‹å ´åˆã€æœ€å¾Œã®3ã¤ã®æ¬¡å…ƒã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã™ã‚‹ã¨æ©Ÿèƒ½ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        lr (float): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹å­¦ç¿’ç‡\n",
    "        momentum (float): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ \n",
    "        nesterov (bool): å†…éƒ¨SGDã§Nesterovã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆæ¨å¥¨ï¼‰\n",
    "        ns_steps (int): ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        \"\"\"\n",
    "        Muonæœ€é©åŒ–é–¢æ•°ã®åˆæœŸåŒ–\n",
    "\n",
    "        Args:\n",
    "            params (iterable): æœ€é©åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            lr (float, optional): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹å­¦ç¿’ç‡\n",
    "            momentum (float, optional): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ \n",
    "            nesterov (bool, optional): å†…éƒ¨SGDã§Nesterovã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ns_steps (int, optional): ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "        \"\"\"\n",
    "        # æœ€é©åŒ–é–¢æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "\n",
    "        params: list[Tensor] = [*params]\n",
    "\n",
    "        param_groups = []\n",
    "\n",
    "        # åŒã˜è¦ç´ æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "        for size in {p.numel() for p in params}:\n",
    "            group = dict(params=[p for p in params if p.numel() == size])\n",
    "            param_groups.append(group)\n",
    "\n",
    "        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Muonæœ€é©åŒ–é–¢æ•°ã®1ã‚¹ãƒ†ãƒƒãƒ—ã®æ›´æ–°ã‚’å®Ÿè¡Œ\n",
    "        \"\"\"\n",
    "\n",
    "        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "\n",
    "            # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ«ãƒ¼ãƒ—\n",
    "            for p in params:\n",
    "                # å‹¾é…ã‚’å–å¾—\n",
    "                g = p.grad\n",
    "                assert g is not None\n",
    "\n",
    "                # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒãƒƒãƒ•ã‚¡ã‚’å–å¾—ã¾ãŸã¯åˆæœŸåŒ–\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf: Tensor = state[\"momentum_buffer\"]\n",
    "\n",
    "                # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒãƒƒãƒ•ã‚¡ã‚’æœ€æ–°ã®å‹¾é…ã§æ›´æ–°ï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰\n",
    "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "\n",
    "                # ãƒã‚¹ãƒ†ãƒ­ãƒ•ãƒ»ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å‹¾é…ã‚’èª¿æ•´\n",
    "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "\n",
    "                # å‹¾é…ã‚’ç›´äº¤åŒ–\n",
    "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "                # -group[\"lr\"]: å­¦ç¿’ç‡ã‚’è² ã«ã—ã¦æ¸›å°‘æ–¹å‘ã«æ›´æ–°\n",
    "                # max(1, p.size(-2) / p.size(-1))**0.5: è¡Œåˆ—ã®å½¢çŠ¶ã«åŸºã¥ã„ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "                p.add_(g, alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f30cb",
   "metadata": {},
   "source": [
    "### DistMuon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeacbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "\n",
    "class DistMuon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muonæœ€é©åŒ–é–¢æ•°ã®åˆ†æ•£å‡¦ç†å¯¾å¿œãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
    "\n",
    "    ZeRO-2ã¨åŒæ§˜ã®æ‰‹æ³•ã§ã€æœ€é©åŒ–é–¢æ•°ã®çŠ¶æ…‹ã¨å‹¾é…ã‚’ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆåˆ†å‰²ï¼‰ã™ã‚‹\n",
    "\n",
    "    æœ€é©åŒ–é–¢æ•°ã®çŠ¶æ…‹ã¯ã€Nåˆ†å‰²\n",
    "    å‹¾é…ã¯ã€å„GPUãŒæ‹…å½“ã®å‹¾é…ã ã‘ã‚’è¨ˆç®—ãƒ»ä¿æŒ\n",
    "\n",
    "    Muon: SGD-momentum + (optional) Nesterov, then orthogonalize the 2D update via Newtonâ€“Schulz,\n",
    "    finally apply aspect-ratio scaled step. Performs its own distributed synchronization:\n",
    "      - reduce_scatter(AVG) for gradient averaging\n",
    "      - all_gather to replicate updated weights\n",
    "\n",
    "    Notes:\n",
    "      * Designed for 2D parameters (e.g., linear/conv kernels reshaped to 2D). Do not use for 0D/1D\n",
    "        params like embeddings or scalars.\n",
    "      * Momentum buffers are maintained only on the 'owner' rank for each parameter (rank chosen\n",
    "        by block-cyclic assignment below). If you checkpoint optimizer state on a single rank,\n",
    "        consolidate states beforehand.\n",
    "\n",
    "    Args:\n",
    "        params: iterable of Tensors\n",
    "        lr: learning rate\n",
    "        momentum: momentum coefficient in [0,1)\n",
    "        nesterov: if True, Nesterov-style update (g <- lerp(g, buf, momentum)); else use buf\n",
    "        ns_steps: number of Newtonâ€“Schulz iterations for the orthogonalization\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr: float = 0.02, momentum: float = 0.95,\n",
    "                 nesterov: bool = True, ns_steps: int = 5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        params = list(params)\n",
    "        assert all(p.ndim == 2 for p in params), \"Muon expects 2D parameters only\"\n",
    "        rank = dist.get_rank()\n",
    "        # Group all parameters by their shape\n",
    "        shapes = sorted({p.shape for p in params}) # sort to ensure consistent / deterministic ordering\n",
    "        param_groups = []\n",
    "        for shape in shapes:\n",
    "            group_params = [p for p in params if p.shape == shape]\n",
    "            device, dtype = group_params[0].device, group_params[0].dtype\n",
    "            assert all(p.device == device for p in group_params)\n",
    "            assert all(p.dtype == dtype for p in group_params)\n",
    "            if rank == 0:\n",
    "                print(f\"Muon: Grouping {len(group_params)} params of shape {shape}, device {device}, dtype {dtype}\")\n",
    "            param_groups.append(dict(params=group_params, zero_buffer=torch.zeros_like(group_params[0])))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Ensure all grads exist\n",
    "        assert all(p.grad is not None for group in self.param_groups for p in group[\"params\"]), \"All params must have grads\"\n",
    "\n",
    "        # Kick off all the reduce scatter operations to average up the gradients across all ranks\n",
    "        all_reduce_futures = []\n",
    "        for group in self.param_groups:\n",
    "            params = group[\"params\"]\n",
    "            zero_buffer = group[\"zero_buffer\"]\n",
    "            # Go through params in groups of world_size.\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                # The compute owner of each param is rank i % world_size\n",
    "                owner_idx = base_i + rank\n",
    "                # each rank stacks up its chunk of world_size params into a list\n",
    "                rs_input = [p.grad for p in params[base_i:base_i + world_size]]\n",
    "                # pad rs_input with the zero buffer to complete the group\n",
    "                rs_input.extend([zero_buffer] * (world_size - len(rs_input)))\n",
    "                # the output buffer gets strided across the group based on the rank\n",
    "                rs_output = params[owner_idx].grad if owner_idx < len(params) else torch.empty_like(zero_buffer)\n",
    "                # reduce scatter the gradients within this group of world_size params\n",
    "                work = dist.reduce_scatter(rs_output, rs_input, op=dist.ReduceOp.AVG, async_op=True).get_future()\n",
    "                all_reduce_futures.append(work)\n",
    "\n",
    "        # Now each rank computes the update and gathers\n",
    "        future_idx = 0\n",
    "        all_gather_futures = []\n",
    "        for group in self.param_groups:\n",
    "            params = group[\"params\"]\n",
    "            zero_buffer = group[\"zero_buffer\"]\n",
    "            # Go through params in groups of world_size.\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                # The compute owner of each param is rank i % world_size\n",
    "                owner_idx = base_i + rank # calculate the index of the param that this rank owns\n",
    "                # Wait for the reduce scatter to complete\n",
    "                all_reduce_futures[future_idx].wait() # possibly later we could use wait_any polling instead\n",
    "                future_idx += 1\n",
    "                # Owner computes the Muon update, result is in its param\n",
    "                if owner_idx < len(params):\n",
    "                    p = params[owner_idx]\n",
    "                    g = p.grad  # now averaged across ranks\n",
    "                    state = self.state[p]\n",
    "                    if \"momentum_buffer\" not in state:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                    buf: Tensor = state[\"momentum_buffer\"]\n",
    "                    buf.lerp_(g, 1.0 - group[\"momentum\"])\n",
    "                    g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                    g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "                    scale = (max(1.0, p.size(-2) / p.size(-1)) ** 0.5)\n",
    "                    p.add_(g, alpha=-group[\"lr\"] * scale)\n",
    "                # Replicate updated parameters to all ranks\n",
    "                ag_input = params[owner_idx] if owner_idx < len(params) else zero_buffer\n",
    "                ag_output = params[base_i:base_i + world_size]\n",
    "                ag_output.extend([torch.empty_like(zero_buffer) for _ in range(world_size - len(ag_output))]) # pad\n",
    "                work = dist.all_gather(ag_output, ag_input, async_op=True).get_future()\n",
    "                all_gather_futures.append(work)\n",
    "\n",
    "        # Wait for all work to finish\n",
    "        torch.futures.collect_all(all_gather_futures).wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a1795",
   "metadata": {},
   "source": [
    "### DistAdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0db608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä»Šå›ã¯ä½¿ã‚ãªã„\n",
    "\n",
    "class DistAdamW(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    AdamWæœ€é©åŒ–é–¢æ•°ã®åˆ†æ•£å‡¦ç†ãƒãƒ¼ã‚¸ãƒ§ãƒ³\n",
    "\n",
    "    In the style of ZeRO-2, i.e. sharded optimizer states and gradient reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3c30d",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea258fa0",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc20df",
   "metadata": {},
   "source": [
    "åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDistributed Data Parallel, DDPï¼‰ã§ä½¿ç”¨ã™ã‚‹é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f83803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ddp():\n",
    "    \"\"\"\n",
    "    åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDistributed Data Parallel, DDPï¼‰ãŒæœ‰åŠ¹ã‹ã©ã†ã‹\n",
    "\n",
    "    Returns:\n",
    "        bool: DDPãŒæœ‰åŠ¹ãªå ´åˆTrueã€ãã†ã§ãªã„å ´åˆFalse\n",
    "    \"\"\"\n",
    "\n",
    "    # RANKç’°å¢ƒå¤‰æ•°ã¯torchrunãªã©ã«ã‚ˆã£ã¦è¨­å®šã•ã‚Œã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®è­˜åˆ¥å­\n",
    "    return int(os.environ.get('RANK', -1)) != -1\n",
    "\n",
    "is_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_info():\n",
    "    \"\"\"\n",
    "    åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDistributed Data Parallel, DDPï¼‰è¨“ç·´ã®ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—\n",
    "    \"\"\"\n",
    "    if is_ddp():\n",
    "        assert all(var in os.environ for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE'])\n",
    "\n",
    "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ—ãƒ­ã‚»ã‚¹è­˜åˆ¥å­\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆåŒä¸€ãƒãƒ¼ãƒ‰å†…ï¼‰ã®ãƒ—ãƒ­ã‚»ã‚¹è­˜åˆ¥å­\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "        # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºï¼ˆå…¨ãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        return True, ddp_rank, ddp_local_rank, ddp_world_size\n",
    "    else:\n",
    "        return False, 0, 0, 1\n",
    "\n",
    "get_dist_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0b964",
   "metadata": {},
   "source": [
    "ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"\n",
    "    å¹³æ–¹æ ¹å¹³å‡äºŒä¹—ãƒãƒ«ãƒ æ­£è¦åŒ–ï¼ˆRoot Mean Square Normalization, RMSNormï¼‰ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    Returns:\n",
    "        Tensor: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.debug(f\"RMSNormã‚’é©ç”¨é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "    # ã‚²ã‚¤ãƒ³ã‚„ãƒã‚¤ã‚¢ã‚¹ã‚’æŒãŸãªã„ãŸã‚é«˜é€Ÿ\n",
    "    # (4, 2048, 1280) -> (4, 2048, 1280)\n",
    "    res = F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "    logger.debug(f\"RMSNormé©ç”¨å®Œäº† {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df3700",
   "metadata": {},
   "source": [
    "Transformerã®ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã«RoPEã‚’é©ç”¨ã™ã‚‹é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, cos, sin):\n",
    "    \"\"\"\n",
    "    å›è»¢ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆRoPE, Rotary Positional Embeddingï¼‰ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        cos (Tensor): ã‚³ã‚µã‚¤ãƒ³æˆåˆ†ã€å½¢çŠ¶ã¯(1, seq_len, 1, head_dim/2)\n",
    "        sin (Tensor): ã‚µã‚¤ãƒ³æˆåˆ†ã€å½¢çŠ¶ã¯(1, seq_len, 1, head_dim/2)\n",
    "    Returns:\n",
    "        Tensor: RoPEãŒé©ç”¨ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯(batch_size, seq_len, num_heads, head_dim)\n",
    "    \"\"\"\n",
    "    logger.debug(f\"RoPEã‚’é©ç”¨ {x.shape=}, {x.dtype=} {cos.shape=}, {cos.dtype=} {sin.shape=} {sin.dtype=}\")\n",
    "\n",
    "    assert x.ndim == 4\n",
    "\n",
    "    # head_dimã‚’2ã§åˆ†å‰²\n",
    "    d = x.shape[3] // 2\n",
    "\n",
    "    # æœ€å¾Œã®æ¬¡å…ƒã‚’2ã¤ã«åˆ†å‰²\n",
    "    x1, x2 = x[..., :d], x[..., d:]\n",
    "\n",
    "    # 2æ¬¡å…ƒå›è»¢è¡Œåˆ—ã‚’é©ç”¨\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    out = torch.cat([y1, y2], 3) # æœ€å¾Œã®æ¬¡å…ƒã§çµåˆ\n",
    "\n",
    "    # sinã¨cosã¯float32ã§è¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ãŸã‚ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã«æˆ»ã™\n",
    "    out = out.to(x.dtype)\n",
    "\n",
    "    logger.debug(f\"RoPEé©ç”¨å®Œäº† {out.shape=}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e339f",
   "metadata": {},
   "source": [
    "### GPTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beec71d",
   "metadata": {},
   "source": [
    "GPTãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "\n",
    "å¾Œã§ä¸Šæ›¸ãã•ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ef441",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    sequence_len: int = 1024 # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    vocab_size: int = 50304 # èªå½™ã‚µã‚¤ã‚º\n",
    "    n_layer: int = 12 # Transformerã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°\n",
    "    n_head: int = 6 # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    n_kv_head: int = 6 # ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    n_embd: int = 768 # Transformerã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355ebe2",
   "metadata": {},
   "source": [
    "### CausalSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43118852",
   "metadata": {},
   "source": [
    "Transformerã®å‰åŠã«ã‚ã‚‹ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ba25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    GQAï¼ˆGroup Query Attentionï¼‰ã‚’å®Ÿè£…ã—ãŸå› æœã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx):\n",
    "        logger.debug(f\"CausalSelfAttentionã‚’åˆæœŸåŒ–é–‹å§‹ {config.n_head=} {config.n_kv_head=} {config.n_embd=} {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # 10\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "        # 10\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "\n",
    "        # 1280\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # 1280 / 10 = 128\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0\n",
    "\n",
    "        # 1280 -> 1280\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "\n",
    "        # 1280 -> 1280\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "\n",
    "        # 1280 -> 1280\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "\n",
    "        # 1280 -> 1280\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "\n",
    "        logger.debug(\"CausalSelfAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        logger.debug(f\"CausalSelfAttentionã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=} {cos_sin[0].shape=} {cos_sin[0].dtype=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        # (4, 2048, 1280)\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1) ã‚¯ã‚¨ãƒªãƒ»ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®—\n",
    "\n",
    "        # (4, 2048, 1280) -> (4, 2048, 1280) -> (4, 2048, 10, 128)\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "\n",
    "        # (4, 2048, 1280) -> (4, 2048, 1280) -> (4, 2048, 10, 128)\n",
    "        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        # (4, 2048, 1280) -> (4, 2048, 1280) -> (4, 2048, 10, 128)\n",
    "        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        # 2) ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã«RoPEã‚’é©ç”¨\n",
    "\n",
    "        # (1, 2048, 1, 64), (1, 2048, 1, 64)\n",
    "        cos, sin = cos_sin\n",
    "\n",
    "        # (4, 2048, 10, 128), (4, 2048, 10, 128)\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n",
    "\n",
    "        # 3) RMSNormã‚’é©ç”¨\n",
    "\n",
    "        # (4, 2048, 10, 128) -> (4, 2048, 10, 128)\n",
    "        # (4, 2048, 10, 128) -> (4, 2048, 10, 128)\n",
    "        q, k = norm(q), norm(k)\n",
    "\n",
    "        # 4) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é©ç”¨\n",
    "\n",
    "        # (4, 2048, 10, 128) -> (4, 10, 2048, 128)\n",
    "        # (4, 2048, 10, 128) -> (4, 10, 2048, 128)\n",
    "        # (4, 2048, 10, 128) -> (4, 10, 2048, 128)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆï¼ˆæ¨è«–æ™‚ï¼‰\n",
    "        if kv_cache is not None:\n",
    "            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã€ã“ã‚Œã¾ã§ã®å…¨ã¦ã®KVã‚’å–å¾—\n",
    "            k, v = kv_cache.insert_kv(self.layer_idx, k, v)\n",
    "            logger.debug(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # 5) ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ»ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã®æ•°\n",
    "        # 2048\n",
    "        Tq = q.size(2)\n",
    "        logger.debug(f\"{Tq=}\")\n",
    "\n",
    "        # ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼ã®æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…ã‚‚å«ã‚€ï¼‰\n",
    "        # 2048\n",
    "        Tk = k.size(2)\n",
    "        logger.debug(f\"{Tk=}\")\n",
    "\n",
    "        # GQAã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        # False\n",
    "        enable_gqa = self.n_head != self.n_kv_head\n",
    "        logger.debug(f\"GQAã‚’æœ‰åŠ¹åŒ–: {enable_gqa=}\")\n",
    "\n",
    "        # è¨“ç·´æ™‚ã¾ãŸã¯ã‚¯ã‚¨ãƒªæ•°ã¨ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼æ•°ãŒç­‰ã—ã„å ´åˆ\n",
    "        if kv_cache is None or Tq == Tk:\n",
    "            logger.debug(\"é€šå¸¸ã®å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "\n",
    "        # æ¨è«–æ™‚ã§ã‚¯ã‚¨ãƒªãŒ1ã¤ã ã‘ã®å ´åˆ\n",
    "        elif Tq == 1:\n",
    "            logger.debug(\"ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ãªã„å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n",
    "\n",
    "        # æ¨è«–æ™‚ã§è¤‡æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹å ´åˆï¼ˆprefillï¼‰\n",
    "        else:\n",
    "            logger.debug(\"ãƒã‚¹ã‚¯ã‚’æ‰‹å‹•ã§ä½œæˆã—ã¦å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "\n",
    "            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–\n",
    "            attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=q.device)\n",
    "            logger.debug(f\"{attn_mask.shape=}\")\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã®é•·ã•ã‚’å–å¾—\n",
    "            prefix_len = Tk - Tq\n",
    "            logger.debug(f\"{prefix_len=}\")\n",
    "\n",
    "            if prefix_len > 0:\n",
    "                # ãƒã‚¹ã‚¯ã®å·¦å´ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥éƒ¨åˆ†ï¼‰ã‚’ãƒã‚¹ã‚¯ã—ãªã„\n",
    "                attn_mask[:, :prefix_len] = True\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã®å³å´ã«ä¸‹ä¸‰è§’è¡Œåˆ—ã‚’è¨­å®š\n",
    "            attn_mask[:, prefix_len:] = torch.tril(\n",
    "                torch.ones((Tq, Tq), dtype=torch.bool, device=q.device)\n",
    "            )\n",
    "\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)\n",
    "\n",
    "        # 6) å‡ºåŠ›å‡¦ç†\n",
    "\n",
    "        # (4, 10, 2048, 128) -> (4, 2048, 10, 128) -> (4, 2048, 1280)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ {y.shape=}\")\n",
    "\n",
    "        # (4, 2048, 1280) -> (4, 2048, 1280)\n",
    "        y = self.c_proj(y)\n",
    "        logger.debug(f\"æœ€çµ‚ç·šå½¢å¤‰æ›ã‚’é©ç”¨ {y.shape=}\")\n",
    "\n",
    "        logger.debug(f\"CausalSelfAttentionã®é †ä¼æ¬å®Œäº† {y.shape=}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257364b",
   "metadata": {},
   "source": [
    "### MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b853d",
   "metadata": {},
   "source": [
    "Transformerã®å¾ŒåŠã«ã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFFNï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.debug(f\"MLPã‚’åˆæœŸåŒ– {config.n_embd=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 1280 -> 5120\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "\n",
    "        # 5120 -> 1280\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        logger.debug(\"MLPã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logger.debug(f\"MLPã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=}\")\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        logger.debug(f\"{x.shape=}\")\n",
    "\n",
    "        # Squared ReLUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        # GELUã‚„SiLUï¼ˆSwishï¼‰ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„\n",
    "        x = F.relu(x).square()\n",
    "\n",
    "        x = self.c_proj(x)\n",
    "        logger.debug(f\"MLPã®é †ä¼æ¬å®Œäº† {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd7f0a",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02877a11",
   "metadata": {},
   "source": [
    "ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’çµ„ã¿åˆã‚ã›ãŸTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerãƒ–ãƒ­ãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx):\n",
    "        logger.debug(f\"Transformer Blockã‚’åˆæœŸåŒ– {config.n_embd=} {layer_idx=}\")\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "        logger.debug(\"Transformer Blockã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        logger.debug(f\"Transformer Blockã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=} {cos_sin[0].shape=} {cos_sin[0].dtype=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        x = x + self.attn(norm(x), cos_sin, kv_cache)\n",
    "\n",
    "        x = x + self.mlp(norm(x))\n",
    "\n",
    "        logger.debug(f\"Transformer Blockã®é †ä¼æ¬å®Œäº† {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac54f3e",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc31839",
   "metadata": {},
   "source": [
    "20å±¤ã®Transformerã‚’ç©ã¿é‡ã­ã€å…¥åŠ›ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã€å‡ºåŠ›ã«ãƒ˜ãƒƒãƒ‰ã‚’ã¤ã‘ãŸGPTãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdec548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPTãƒ¢ãƒ‡ãƒ«å…¨ä½“\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.debug(f\"GPTãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ– {config.n_layer=} {config.n_head=} {config.n_embd=} {config.vocab_size=} {config.sequence_len=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Transformerã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            # å˜èªåŸ‹ã‚è¾¼ã¿å±¤\n",
    "            # 65536 -> 1280\n",
    "            \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "\n",
    "            # 20å±¤ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’ä½œæˆ\n",
    "            \"h\": nn.ModuleList(\n",
    "                [Block(config, layer_idx) for layer_idx in range(config.n_layer)]\n",
    "            ),\n",
    "        })\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤ï¼ˆLanguage Model Headï¼‰\n",
    "        # 1280 -> 65536\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # RoPEã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¨­å®š\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§é•·ã‚’è¶…ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ä½™è£•ã‚’ã‚‚ãŸã›ã‚‹\n",
    "        # 2048 * 10 = 20480\n",
    "        self.rotary_seq_len = config.sequence_len * 10\n",
    "        logger.debug(f\"{self.rotary_seq_len=}\")\n",
    "\n",
    "        # ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒæ•°ã‚’è¨ˆç®—\n",
    "        # 1280 / 10 = 128\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        logger.debug(f\"{head_dim=}\")\n",
    "\n",
    "        # RoPEã®cosã¨sinã‚’äº‹å‰è¨ˆç®—\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "\n",
    "        # cosã‚’ãƒãƒƒãƒ•ã‚¡ã«ç™»éŒ²ï¼ˆstate_dictã«ã¯ä¿å­˜ã—ãªã„ï¼‰\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "\n",
    "        # sinã‚’ãƒãƒƒãƒ•ã‚¡ã«ç™»éŒ²ï¼ˆstate_dictã«ã¯ä¿å­˜ã—ãªã„ï¼‰\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "        logger.debug(\"GPTãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–ã™ã‚‹\n",
    "        \"\"\"\n",
    "        logger.debug(\"GPTãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–é–‹å§‹\")\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¯¾ã—ã¦åˆæœŸåŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # å‡ºåŠ›å±¤ã®é‡ã¿ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–\n",
    "        torch.nn.init.zeros_(self.lm_head.weight)\n",
    "\n",
    "        # å…¨ã¦ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›å±¤ã®é‡ã¿ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n",
    "            torch.nn.init.zeros_(block.attn.c_proj.weight)\n",
    "\n",
    "        # RoPEã®cosã¨sinã‚’è¨ˆç®—\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’bfloat16ã«ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›\n",
    "        if self.transformer.wte.weight.device.type == \"cuda\":\n",
    "            self.transformer.wte.to(dtype=torch.bfloat16)\n",
    "\n",
    "        logger.debug(\"GPTãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        init_weightsã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹é‡ã¿åˆæœŸåŒ–é–¢æ•°\n",
    "\n",
    "        Args:\n",
    "            module (nn.Module): åˆæœŸåŒ–ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–é–‹å§‹ {module.__class__.__name__=}\")\n",
    "\n",
    "        # å…¨çµåˆå±¤ã®å ´åˆ\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # è«–æ–‡ã«åŸºã¥ãæ–¹æ³•ã§é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "            # https://arxiv.org/pdf/2310.17813\n",
    "            fan_out = module.weight.size(0)\n",
    "            fan_in = module.weight.size(1)\n",
    "            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®å ´åˆ\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # æ¨™æº–æ­£è¦åˆ†å¸ƒã§é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
    "\n",
    "        logger.debug(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é‡ã¿ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        \"\"\"\n",
    "        RoPEã®cosã¨sinã‚’äº‹å‰è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            seq_len (int): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "            head_dim (int): ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒæ•°\n",
    "            base (int, optional): é€†å‘¨æ³¢æ•°ã®åŸºåº•å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10000ã€‚\n",
    "            device (torch.device, optional): è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã§ã€è‡ªå‹•æ¤œå‡ºã•ã‚Œã‚‹ã€‚\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: äº‹å‰è¨ˆç®—ã•ã‚ŒãŸcosã¨sinã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(f\"RoPEã®äº‹å‰è¨ˆç®—ã‚’å®Ÿè¡Œ {seq_len=} {head_dim=} {base=}\")\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if device is None:\n",
    "            # è‡ªå‹•æ¤œå‡º\n",
    "            device = self.transformer.wte.weight.device\n",
    "\n",
    "        # è¦ç´ æ•°ã‚’åŠåˆ†ã«ã™ã‚‹ãŸã‚ã®å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "\n",
    "        # é€†å‘¨æ³¢æ•°ï¼ˆinvert frequencyï¼‰ã‚’è¨ˆç®—\n",
    "        # è§’é€Ÿåº¦ã¨å®Ÿè³ªçš„ã«åŒã˜\n",
    "        # \\theta = base^{-\\frac{2i}{head\\_dim}}\n",
    "        # (64,)\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        logger.debug(f\"{inv_freq.shape=}\")\n",
    "\n",
    "        # æ–‡å­—ä½ç½®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ\n",
    "        # (20480,)\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        logger.debug(f\"{t.shape=}\")\n",
    "    \n",
    "        # å›è»¢è§’åº¦ã‚’è¨ˆç®—\n",
    "        # å›è»¢è§’åº¦ = æ–‡å­—ä½ç½® * é€†å‘¨æ³¢æ•°\n",
    "        # (20480, 64)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        logger.debug(f\"{freqs.shape=}\")\n",
    "\n",
    "        # å›è»¢è§’åº¦ã‹ã‚‰cosã¨sinã‚’è¨ˆç®—\n",
    "        # (20480, 64), (20480, 64)\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "\n",
    "        # bfloat16ã«å¤‰æ›\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16()\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®ãŸã‚ã«æ¬¡å…ƒã‚’è¿½åŠ \n",
    "        # (1, 20480, 1, 64), (1, 20480, 1, 64)\n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
    "\n",
    "        logger.debug(f\"RoPEã®äº‹å‰è¨ˆç®—å®Œäº† {cos.shape=} {sin.shape=}\")\n",
    "        return cos, sin\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.transformer.wte.weight.device\n",
    "\n",
    "    def estimate_flops(self):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®FLOPsã‚’æ¨å®šã™ã‚‹\n",
    "        FLOPSã¯1ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªæµ®å‹•å°æ•°ç‚¹æ¼”ç®—å›æ•°\n",
    "        Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒ«å‰‡ã§ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å°ããŸã‚ã«å¿…è¦\n",
    "        https://arxiv.org/abs/2204.02311\n",
    "        \"\"\"\n",
    "        logger.debug(\"ãƒ¢ãƒ‡ãƒ«ã®FLOPsã‚’æ¨å®šé–‹å§‹\")\n",
    "    \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—\n",
    "        # 560,988,160 = 5.6å„„\n",
    "        nparams = sum(p.numel() for p in self.parameters())\n",
    "        logger.debug(f\"{nparams=:,}\")\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—\n",
    "        # 83,886,080 = 8,400ä¸‡\n",
    "        nparams_embedding = self.transformer.wte.weight.numel()\n",
    "        logger.debug(f\"{nparams_embedding=:,}\")\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°l=20\n",
    "        # ãƒ˜ãƒƒãƒ‰æ•°h=10\n",
    "        # ã‚¯ã‚¨ãƒªã®æ¬¡å…ƒq=128\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·t=2048\n",
    "        l, h, q, t = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.sequence_len\n",
    "        logger.debug(f\"{l=}, {h=}, {q=}, {t=}\")\n",
    "\n",
    "        # FLOPsã‚’è¨ˆç®—\n",
    "        # 3,491,758,080 = 3.5GFLOPs\n",
    "        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t\n",
    "\n",
    "        logger.debug(f\"ãƒ¢ãƒ‡ãƒ«ã®FLOPsã®æ¨å®šå®Œäº† {num_flops_per_token=:,}\")\n",
    "        return num_flops_per_token\n",
    "\n",
    "    def setup_optimizers(self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):\n",
    "        \"\"\"\n",
    "        æœ€é©åŒ–é–¢æ•°ã‚’è¨­å®šã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            unembedding_lr (float): å‡ºåŠ›å±¤ã®å­¦ç¿’ç‡\n",
    "            embedding_lr (float): åŸ‹ã‚è¾¼ã¿å±¤ã®å­¦ç¿’ç‡\n",
    "            matrix_lr (float): ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å­¦ç¿’ç‡\n",
    "            weight_decay (float): é‡ã¿æ¸›è¡°ï¼ˆL2æ­£å‰‡åŒ–ï¼‰ä¿‚æ•°\n",
    "        Returns:\n",
    "            List[torch.optim.Optimizer]: è¨­å®šã•ã‚ŒãŸæœ€é©åŒ–é–¢æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"æœ€é©åŒ–é–¢æ•°ã‚’è¨­å®šé–‹å§‹ {unembedding_lr=} {embedding_lr=} {matrix_lr=} {weight_decay=}\")\n",
    "\n",
    "        # 1) ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒæ•°ã‚’å–å¾—\n",
    "\n",
    "        model_dim = self.config.n_embd\n",
    "\n",
    "        ddp, rank, local_rank, world_size = get_dist_info()\n",
    "\n",
    "        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        matrix_params = list(self.transformer.h.parameters())\n",
    "        logger.debug(f\"{len(matrix_params)=}\")\n",
    "\n",
    "        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        embedding_params = list(self.transformer.wte.parameters())\n",
    "\n",
    "        # å‡ºåŠ›å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        lm_head_params = list(self.lm_head.parameters())\n",
    "\n",
    "        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params)\n",
    "\n",
    "        # 2) å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã¨å‡ºåŠ›ã®ç·šå½¢å±¤ã«å¯¾ã—ã¦AdamWã‚’åˆæœŸåŒ–\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒã«åŸºã¥ã„ã¦å­¦ç¿’ç‡ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        dmodel_lr_scale = (model_dim / 768) ** -0.5\n",
    "        if rank == 0:\n",
    "            logger.debug(f\"Scaling the LR for the AdamW parameters âˆ1/âˆš({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n",
    "\n",
    "        adam_groups = [\n",
    "            dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),\n",
    "            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n",
    "        ]\n",
    "\n",
    "        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)\n",
    "\n",
    "        AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=True)\n",
    "\n",
    "        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
    "\n",
    "        # 3) Transformerã®è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦Muonã‚’åˆæœŸåŒ–\n",
    "\n",
    "        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)\n",
    "\n",
    "        MuonFactory = DistMuon if ddp else Muon\n",
    "\n",
    "        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)\n",
    "\n",
    "\n",
    "        # 4) 2ã¤ã®æœ€é©åŒ–é–¢æ•°ã‚’1ã¤ã®ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹\n",
    "\n",
    "        optimizers = [adamw_optimizer, muon_optimizer]\n",
    "\n",
    "        for opt in optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                group[\"initial_lr\"] = group[\"lr\"]\n",
    "\n",
    "        logger.debug(\"æœ€é©åŒ–é–¢æ•°ã®è¨­å®šå®Œäº†\")\n",
    "        return optimizers\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n",
    "        \"\"\"\n",
    "        GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬\n",
    "\n",
    "        Args:\n",
    "            idx (Tensor): ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯(batch_size, seq_len)\n",
    "            targets (Tensor, optional): ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯(batch_size, seq_len)ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            kv_cache (KVCache, optional): KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã€‚\n",
    "            loss_reduction (str, optional): æå¤±ã®é›†ç´„æ–¹æ³•ã€‚'mean'ã¾ãŸã¯'sum'ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'mean'ã€‚\n",
    "        Returns:\n",
    "            Tensor: è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æå¤±ãƒ†ãƒ³ã‚½ãƒ«ã€æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ­ã‚¸ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«\n",
    "            å½¢çŠ¶ã¯(batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {idx.shape=} {idx.dtype=} {targets.shape if targets is not None else None=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        B, T = idx.size()\n",
    "\n",
    "        # 1) RoPEã®æº–å‚™\n",
    "\n",
    "        assert T <= self.cos.size(1), f\"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}\"\n",
    "\n",
    "        assert idx.device == self.cos.device, f\"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}\"\n",
    "\n",
    "        assert self.cos.dtype == torch.bfloat16, \"Rotary embeddings must be in bfloat16\"\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…ã®ä½ç½®ã«RoPEã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "\n",
    "        # äº‹å‰è¨ˆç®—ã•ã‚ŒãŸRoPEã‚’ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "        # (1, 2048, 1, 64), (1, 2048, 1, 64)\n",
    "        cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]\n",
    "        logger.debug(f\"RoPEã®åˆ‡ã‚Šå‡ºã—å®Œäº† {cos_sin[0].shape=} {cos_sin[1].shape=}\")\n",
    "\n",
    "        # 2) 20å±¤ã®Transformerã®é †ä¼æ’­\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›\n",
    "        x = self.transformer.wte(idx)\n",
    "\n",
    "        # æœ€åˆã®RMSNormã‚’é©ç”¨ï¼ˆPre-LNï¼‰\n",
    "        x = norm(x)\n",
    "\n",
    "        # å„Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«é©ç”¨\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, cos_sin, kv_cache)\n",
    "\n",
    "        # 3) å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "\n",
    "        x = norm(x)\n",
    "\n",
    "        # Logit Softcapping\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã®å€¤ãŒ+/-15ã‚’è¶…ãˆãªã„ã‚ˆã†ã«åˆ¶é™ã™ã‚‹\n",
    "        softcap = 15\n",
    "\n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "        if targets is not None:\n",
    "\n",
    "            # å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            # logitsã«ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ã‚’é©ç”¨\n",
    "            logits = softcap * torch.tanh(logits / softcap)\n",
    "\n",
    "            # logitsã‚’float32ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "            logits = logits.float()\n",
    "\n",
    "            # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1,\n",
    "                reduction=loss_reduction\n",
    "            )\n",
    "\n",
    "            logger.debug(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬å®Œäº†ï¼ˆè¨“ç·´ãƒ¢ãƒ¼ãƒ‰ï¼‰ {loss.shape=}\")\n",
    "            return loss\n",
    "\n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "        else:\n",
    "            # å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            # logitsã«ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ã‚’é©ç”¨\n",
    "            logits = softcap * torch.tanh(logits / softcap)\n",
    "\n",
    "            logger.debug(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬å®Œäº†ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰ {logits.shape=}\")\n",
    "            return logits\n",
    "\n",
    "    @torch.inference_mode() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"\n",
    "        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼é–¢æ•°\n",
    "\n",
    "        Args:\n",
    "            tokens (List[int]): åˆæœŸãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆ\n",
    "            max_tokens (int): ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "            temperature (float, optional): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0ã€‚\n",
    "            top_k (int, optional): Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Kå€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneã§ç„¡åŠ¹ã€‚\n",
    "            seed (int, optional): ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯42ã€‚\n",
    "        Yields:\n",
    "            int: ç”Ÿæˆã•ã‚ŒãŸå„ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ {tokens=} {max_tokens=} {temperature=} {top_k=} {seed=}\")\n",
    "\n",
    "        # 1) åˆæœŸåŒ–\n",
    "\n",
    "        assert isinstance(tokens, list)\n",
    "\n",
    "        device = self.get_device()\n",
    "        logger.debug(f\"{device=}\")\n",
    "\n",
    "        rng = None\n",
    "\n",
    "        # æ¸©åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä¹±æ•°ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–\n",
    "        if temperature > 0:\n",
    "            rng = torch.Generator(device=device)\n",
    "            rng.manual_seed(seed)\n",
    "\n",
    "        # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ \n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "        # 2) ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¾ã§ç”Ÿæˆ\n",
    "        for _ in range(max_tokens):\n",
    "\n",
    "            # 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "            # (B, T, vocab_size)\n",
    "            logits = self.forward(ids)\n",
    "\n",
    "            # ç›´è¿‘ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—\n",
    "            # (B, vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "            if top_k is not None:\n",
    "                # ä¸Šä½Kå€‹ä»¥å¤–ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’ç„¡é™å°ã«è¨­å®š\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # æ¸©åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if temperature > 0:\n",
    "                # ç¢ºç‡åˆ†å¸ƒã‚’ãªã‚ã‚‰ã‹ã«ã—ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "\n",
    "            # æ¸©åº¦ãŒ0ã®å ´åˆ\n",
    "            else:\n",
    "                # æœ€ã‚‚é«˜ã„ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠï¼ˆGreedy Decodingï¼‰\n",
    "                next_ids = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ«å°¾ã«è¿½åŠ \n",
    "            ids = torch.cat((ids, next_ids), dim=1)\n",
    "\n",
    "            # ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’CPUã«ç§»å‹•ã—ã¦Pythonã®intã«å¤‰æ›\n",
    "            token = next_ids.item()\n",
    "\n",
    "            logger.debug(f\"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³: {token}\")\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae35199",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_distributed_data_loader(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€è¨“ç·´ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\n",
    "\n",
    "    Args:\n",
    "        B (int): ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        T (int): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "        split (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰² (\"train\" ã¾ãŸã¯ \"val\")\n",
    "        tokenizer_threads (int): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°\n",
    "        tokenizer_batch_size (int): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        device (str): ãƒ‡ãƒã‚¤ã‚¹ (\"cuda\" ã¾ãŸã¯ \"cpu\")\n",
    "\n",
    "    Yields:\n",
    "        Tuple[Tensor, Tensor]: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¿ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "    logger.debug(f\"åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ– {B=} {T=} {split=} {tokenizer_threads=} {tokenizer_batch_size=} {device=}\")\n",
    "\n",
    "    # 1) åˆæœŸåŒ–\n",
    "\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "\n",
    "    # å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    # æœ€å¾Œã®æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚ã‚‹ãŸã‚1ã‚’åŠ ç®—\n",
    "    # 8193\n",
    "    needed_tokens = B * T + 1\n",
    "    logger.debug(f\"{needed_tokens=}\")\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "\n",
    "    # ä¸€åº¦ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ä¿æŒã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã®ã‚¹ã‚¯ãƒ©ãƒƒãƒãƒãƒƒãƒ•ã‚¡\n",
    "    # dequeã‚’ä½¿ç”¨ã™ã‚‹ã®ã¯ã€popleftãŒO(1)ã§é«˜é€Ÿã§ã‚ã‚‹ãŸã‚\n",
    "    token_buffer = deque()\n",
    "\n",
    "    # 2) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒã‚’ç„¡é™ã«ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ\n",
    "\n",
    "    def document_batches():\n",
    "        \"\"\"\n",
    "        ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç„¡é™ã«ä¾›çµ¦ã™ã‚‹ãŸã‚ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼\n",
    "\n",
    "        Returns:\n",
    "            Iterator[List[str]]: ãƒ†ã‚­ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒã‚’ç„¡é™ã«ç”Ÿæˆã™ã‚‹ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒ¼\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒã‚’å–å¾—ï¼ˆ1024è¡Œï¼‰\n",
    "            # DDPãŒæœ‰åŠ¹ãªå ´åˆã€å„ãƒ—ãƒ­ã‚»ã‚¹ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆRank0ã¯0,8,16..., Rank1ã¯1,9,17,...ï¼‰\n",
    "            for batch in parquets_iter_batched(split=split, start=ddp_rank, step=ddp_world_size):\n",
    "                # ãƒŸãƒ‹ãƒãƒƒãƒï¼ˆ128è¡Œï¼‰ã«åˆ†å‰²ã—ã¦è¿”ã™\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size]\n",
    "\n",
    "    batches = document_batches()\n",
    "\n",
    "    # 3) æ–‡å­—åˆ—ã‚’èª­ã¿è¾¼ã¿ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€æ•´å½¢ã—ã¦ãƒãƒƒãƒã‚’ç”Ÿæˆ\n",
    "\n",
    "    batch_index = 0\n",
    "    while True:\n",
    "        # ãƒãƒƒãƒ•ã‚¡ã«ååˆ†ãªãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ8193ï¼‰ã‚’è“„ç©ã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™\n",
    "        while len(token_buffer) < needed_tokens:\n",
    "            # ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒã‚’å–å¾—ï¼ˆ128è¡Œï¼‰\n",
    "            doc_batch = next(batches)\n",
    "\n",
    "            # BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ˆé ­ã«è¿½åŠ ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒãƒƒãƒ•ã‚¡ã®æœ«å°¾ã«è¿½åŠ \n",
    "            for tokens in token_lists:\n",
    "                token_buffer.extend(tokens)\n",
    "\n",
    "            batch_index += 1\n",
    "\n",
    "        # å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆ8193ï¼‰ã‚’ãƒãƒƒãƒ•ã‚¡ã®å…ˆé ­ã‹ã‚‰å–ã‚Šå‡ºã™\n",
    "        tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
    "\n",
    "        # pin_memoryã‚’æœ‰åŠ¹åŒ–ã—GPUãŒãƒ¡ãƒ¢ãƒªã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã—ãŸ1æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "        scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
    "\n",
    "        # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
    "\n",
    "        # æ­£è§£ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "        # æ­£è§£ã¯å…¥åŠ›ã‚’1ãƒˆãƒ¼ã‚¯ãƒ³å³ã«ã‚·ãƒ•ãƒˆã—ãŸã‚‚ã®\n",
    "        targets_cpu = scratch[1:]\n",
    "\n",
    "        # 2Dã«å¤‰æ›ã—ï¼ˆB, Tï¼‰ã€éåŒæœŸï¼ˆnon_blocking=Trueï¼‰ã§GPUã«è»¢é€\n",
    "        inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
    "        targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
    "        yield inputs, targets\n",
    "\n",
    "# æ¤œè¨¼\n",
    "data_loader = tokenizing_distributed_data_loader(\n",
    "    B=4,\n",
    "    T=2048,\n",
    "    split=\"train\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "first_batch = next(data_loader)\n",
    "logger.debug(f\"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æœ€åˆã®ãƒãƒƒãƒã‚’å–å¾— {first_batch[0].shape=} {first_batch[1].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc65856d",
   "metadata": {},
   "source": [
    "## æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import signal\n",
    "import warnings\n",
    "from contextlib import contextmanager\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27dfe3",
   "metadata": {},
   "source": [
    "### KVã‚­ãƒ£ãƒƒã‚·ãƒ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd2538",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—ã§ã€ä¸€åº¦ç”Ÿæˆã—ãŸã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’å†åˆ©ç”¨ã—è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, num_heads, seq_len, head_dim, num_layers):\n",
    "        logger.debug(f\"KVCacheã‚’åˆæœŸåŒ–é–‹å§‹ {batch_size=} {num_heads=} {seq_len=} {head_dim=} {num_layers=}\")\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®å…¨å±¤ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹\n",
    "        self.kv_shape = (num_layers, 2, batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "        self.kv_cache = None\n",
    "\n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…ã®æ›¸ãè¾¼ã¿ä½ç½®\n",
    "        self.pos = 0\n",
    "\n",
    "        logger.debug(\"KVCacheã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.pos = 0\n",
    "\n",
    "    def get_pos(self):\n",
    "        return self.pos\n",
    "\n",
    "    def prefill(self, other):\n",
    "        \"\"\"\n",
    "        KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ¥ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§äº‹å‰è¨­å®šï¼ˆprefillï¼‰ã™ã‚‹\n",
    "        ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒãƒƒãƒæ¬¡å…ƒã«æ²¿ã£ã¦æ‹¡å¼µã™ã‚‹ã“ã¨ã‚‚å¯èƒ½\n",
    "        è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä¸¦åˆ—ã«ç”Ÿæˆã™ã‚‹å ´åˆã«ä½¿ç”¨\n",
    "\n",
    "        Args:\n",
    "            other (KVCache): äº‹å‰è¨­å®šã«ä½¿ç”¨ã™ã‚‹åˆ¥ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        \"\"\"\n",
    "        logger.debug(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰è¨­å®šé–‹å§‹ {other.kv_cache is not None=}\")\n",
    "\n",
    "        # 1) å½¢çŠ¶ã‚’æ¤œè¨¼\n",
    "\n",
    "        assert self.kv_chache is None, \"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã§ãªã„å ´åˆã€äº‹å‰è¨­å®šã§ãã¾ã›ã‚“\"\n",
    "        assert other.kv_cache is not None, \"äº‹å‰è¨­å®šã«ä½¿ç”¨ã™ã‚‹KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç©ºã§ã™\"\n",
    "\n",
    "        # å„æ¬¡å…ƒã‚’æ¤œè¨¼\n",
    "        for ix, (dim1, dim2) in enumerate(zip(self.kv_shape, other.kv_shape)):\n",
    "            # Transformerã®å±¤æ•°ã€K/Vã€ãƒ˜ãƒƒãƒ‰æ•°ã€ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã®æ¤œè¨¼\n",
    "            if ix in [0, 1, 3, 5]:\n",
    "                assert dim1 == dim2, f\"Batch dim mismatch: {dim1} != {dim2}\"\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ¤œè¨¼\n",
    "            elif ix == 2:\n",
    "                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯æ‹¡å¼µå¯èƒ½\n",
    "                assert dim1 == dim2 or dim2 == 1, f\"Batch dim mismatch: {dim1} != {dim2}\"\n",
    "\n",
    "            # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®æ¤œè¨¼\n",
    "            elif ix == 4:\n",
    "                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¯é•·ã„å¿…è¦ãŒã‚ã‚‹\n",
    "                assert dim1 >= dim2, f\"Seq len mismatch: {dim1} < {dim2}\"\n",
    "\n",
    "        # 2) ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        dtype, device = other.kv_cache.dtype, other.kv_cache.device\n",
    "        self.kv_cache = torch.empty(self.kv_shape, dtype=dtype, device=device)\n",
    "\n",
    "        # 3) ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "        # ãƒãƒƒãƒæ¬¡å…ƒã«æ²¿ã£ã¦è¤‡è£½\n",
    "        self.kv_cache[:, :, :, :, :other.pos, :] = other.kv_cache\n",
    "\n",
    "        # 4) æ›¸ãè¾¼ã¿ä½ç½®ã‚’æ›´æ–°\n",
    "\n",
    "        self.pos = other.pos\n",
    "\n",
    "        logger.debug(\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®äº‹å‰è¨­å®šå®Œäº†\")\n",
    "\n",
    "    def insert_kv(self, layer_idx, k, v):\n",
    "        \"\"\"\n",
    "        KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ–°ã—ã„ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’æŒ¿å…¥ã—ã€ã“ã‚Œã¾ã§ã®å…¨ã¦ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’è¿”ã™\n",
    "\n",
    "        Args:\n",
    "            layer_idx (int): ç¾åœ¨ã®Transformerå±¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            k (Tensor): æ–°ã—ã„ã‚­ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯ (B, H, T_add, D)\n",
    "            v (Tensor): æ–°ã—ã„ãƒãƒªãƒ¥ãƒ¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯ (B, H, T_add, D)\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: ã“ã‚Œã¾ã§ã®å…¨ã¦ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒ“ãƒ¥ãƒ¼ã€å½¢çŠ¶ã¯ (B, H, T_total, D)\n",
    "        \"\"\"\n",
    "        logger.debug(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æŒ¿å…¥ã‚’å®Ÿè¡Œ {layer_idx=} {k.shape=} {v.shape=} {self.pos=}\")\n",
    "\n",
    "        # 1) åˆæœŸåŒ–\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if self.kv_cache is None:\n",
    "            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "            # ãƒ‡ãƒ¼ã‚¿å‹ã¨ãƒ‡ãƒã‚¤ã‚¹ãŒå¿…è¦ãªãŸã‚é…å»¶åˆæœŸåŒ–ï¼ˆlazy initializationï¼‰\n",
    "            self.kv_cache = torch.empty(self.kv_shape, dtype=k.dtype, device=k.device)\n",
    "\n",
    "        # Insert new keys/values to the cache and return the full cache so far\n",
    "\n",
    "        # ç¾åœ¨ã®ä½ç½®ã¨è¿½åŠ ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—\n",
    "        B, H, T_add, D = k.size()\n",
    "\n",
    "        # æ›¸ãè¾¼ã¿é–‹å§‹ä½ç½®t0ã¨çµ‚äº†ä½ç½®t1ã‚’è¨ˆç®— \n",
    "        t0, t1 = self.pos, self.pos + T_add\n",
    "        logger.debug(f\"{t0=} {t1=}\")\n",
    "\n",
    "        # æ›¸ãè¾¼ã¿ã®çµ‚äº†ä½ç½®ãŒåˆæœŸåŒ–æ™‚ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¶…ãˆã‚‹å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ‹¡å¼µ\n",
    "        if t1 > self.kv_cache.size(4):\n",
    "            # 1024ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æ‹¡å¼µ\n",
    "            t_needed = t1 + 1024\n",
    "\n",
    "            # 1024ã®å€æ•°ã«åˆ‡ã‚Šä¸Šã’\n",
    "            t_needed = (t_needed + 1023) & ~1023\n",
    "\n",
    "            additional_shape = list(self.kv_cache.shape)\n",
    "\n",
    "            additional_shape[4] = t_needed - self.kv_cache.size(4)\n",
    "            additional_cache = torch.empty(additional_shape, dtype=k.dtype, device=k.device)\n",
    "            self.kv_cache = torch.cat([self.kv_cache, additional_cache], dim=4).contiguous()\n",
    "            self.kv_shape = self.kv_cache.shape\n",
    "\n",
    "        # 2) ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æŒ¿å…¥\n",
    "\n",
    "        # ã‚­ãƒ¼ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æŒ¿å…¥\n",
    "        self.kv_cache[layer_idx, 0, :, :, t0:t1] = k\n",
    "\n",
    "        # ãƒãƒªãƒ¥ãƒ¼ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æŒ¿å…¥\n",
    "        self.kv_cache[layer_idx, 1, :, :, t0:t1] = v\n",
    "\n",
    "        # 3) ã“ã‚Œã¾ã§ã®å…¨ã¦ã®ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’è¿”ã™\n",
    "\n",
    "        # ã“ã‚Œã¾ã§ã®å…¨ã¦ã®ã‚­ãƒ¼ã‚’å–å¾—\n",
    "        key_view = self.kv_cache[layer_idx, 0, :, :, :t1]\n",
    "\n",
    "        # ã“ã‚Œã¾ã§ã®å…¨ã¦ã®ãƒãƒªãƒ¥ãƒ¼ã‚’å–å¾—\n",
    "        value_view = self.kv_cache[layer_idx, 1, :, :, :t1]\n",
    "\n",
    "        # æœ€å¾Œã®Transformerå±¤ã®å ´åˆã€æ›¸ãè¾¼ã¿ä½ç½®ã‚’æ›´æ–°\n",
    "        if layer_idx == self.kv_cache.size(0) - 1:\n",
    "            self.pos = t1\n",
    "\n",
    "        logger.debug(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¸ã®æŒ¿å…¥å®Œäº† {key_view.shape=} {value_view.shape=} {self.pos=}\")\n",
    "        return key_view, value_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6528dbf",
   "metadata": {},
   "source": [
    "ã‚¨ãƒ³ã‚¸ãƒ³ã®generateãƒ¡ã‚½ãƒƒãƒ‰ã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "def sample_next_token(logits, rng, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    æ¬¡ã®1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "    ãƒãƒƒãƒå‡¦ç†ã€æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¯¾å¿œ\n",
    "\n",
    "    Args:\n",
    "        logits (Tensor): ãƒ­ã‚¸ãƒƒãƒˆã®ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯ (B, vocab_size)\n",
    "        rng (Generator): ä¹±æ•°ç”Ÿæˆå™¨\n",
    "        temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        top_k (int, optional): Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Kå€¤\n",
    "    Returns:\n",
    "        Tensor: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€å½¢çŠ¶ã¯ (B, 1)\n",
    "    \"\"\"\n",
    "    logger.debug(f\"æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–‹å§‹ {logits.shape=} {temperature=} {top_k=}\")\n",
    "\n",
    "    assert temperature >= 0.0, \"æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯0ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\"\n",
    "\n",
    "    # æ¸©åº¦ãŒ0ã®å ´åˆ\n",
    "    if temperature == 0.0:\n",
    "        # æœ€ã‚‚é«˜ã„ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠï¼ˆGreedy Decodingï¼‰\n",
    "        res = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    # Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "    if top_k is not None:\n",
    "        k = min(top_k, logits.size(-1))\n",
    "\n",
    "        # ä¸Šä½Kå€‹ã®ãƒ­ã‚¸ãƒƒãƒˆã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "        vals, idx = torch.topk(logits, k, dim=-1)\n",
    "\n",
    "        # æ¸©åº¦ã‚’é©ç”¨ã—ãƒ­ã‚¸ãƒƒãƒˆã‚’èª¿æ•´\n",
    "        vals = vals / temperature\n",
    "\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã‚’ç¢ºç‡ã«å¤‰æ›\n",
    "        probs = F.softmax(vals, dim=-1)\n",
    "\n",
    "        # ä¸Šä½Kå€‹ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ\n",
    "        choice = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "\n",
    "        # å…ƒã®èªå½™ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "        res = idx.gather(1, choice)\n",
    "\n",
    "    # æ¨™æº–çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å ´åˆ\n",
    "    else:\n",
    "        # æ¸©åº¦ã‚’é©ç”¨ã—ãƒ­ã‚¸ãƒƒãƒˆã‚’èª¿æ•´\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã‚’ç¢ºç‡ã«å¤‰æ›\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ\n",
    "        res = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "\n",
    "    logger.debug(f\"æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å®Œäº† {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99ea27",
   "metadata": {},
   "source": [
    "ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ä¸­ãªã©ç”Ÿæˆä¸­ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆRowï¼‰ã®ãƒ•ãƒ©ã‚°ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowState:\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆä¸­ã®å„è¡Œï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰ã®çŠ¶æ…‹ã‚’è¿½è·¡ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "    def __init__(self, current_tokens=None):\n",
    "\n",
    "        # ã“ã‚Œã¾ã§ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã®å®Œå…¨ãªãƒªã‚¹ãƒˆ\n",
    "        self.current_tokens = current_tokens or []\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç„¡è¦–ã—å¼·åˆ¶çš„ã«æŒ¿å…¥ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚­ãƒ¥ãƒ¼\n",
    "        # <|output_start|>ãªã©ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®æŒ¿å…¥ã«ä½¿ç”¨\n",
    "        self.forced_tokens = deque()\n",
    "\n",
    "        # <|python_start|>ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ãŸæ™‚ã«Trueã«ãªã‚‹ãƒ•ãƒ©ã‚°\n",
    "        self.in_python_block = False\n",
    "\n",
    "        # self.in_python_blockãŒTrueã®é–“ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        # Pythonã®ã‚³ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "        self.python_expr_tokens = []\n",
    "\n",
    "        # ç”ŸæˆãŒå®Œäº†ã—ãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        # <|assistant_end|>ã‚„<|bos|>ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆã«Trueã«ãªã‚‹\n",
    "        self.completed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a598caa",
   "metadata": {},
   "source": [
    "ç”Ÿæˆä¸­ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®‰å…¨ã«è¡Œã†ãŸã‚ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timeout(duration, formula):\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã—ãŸæ™‚é–“ã‚’è¶…ãˆãŸå ´åˆã«ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\n",
    "    Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹éš›ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        duration (int): ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ã§ã®æ™‚é–“ï¼ˆç§’ï¼‰\n",
    "        formula (str): å®Ÿè¡Œã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã®æ–‡å­—åˆ—\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise Exception(f\"'{formula}': timed out after {duration} seconds\")\n",
    "\n",
    "    # OSã®ã‚¢ãƒ©ãƒ¼ãƒ ã‚·ã‚°ãƒŠãƒ«ã¨ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’é–¢é€£ä»˜ã‘ã‚‹\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    # ã‚¿ã‚¤ãƒãƒ¼ã‚’é–‹å§‹ï¼ˆdurationç§’å¾Œã«ã‚·ã‚°ãƒŠãƒ«ã‚’é€ä¿¡ï¼‰\n",
    "    signal.alarm(duration)\n",
    "\n",
    "    yield\n",
    "\n",
    "    # ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢\n",
    "    signal.alarm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa757ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_with_timeout(formula, max_time=3):\n",
    "    \"\"\"\n",
    "    Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        formula (str): å®Ÿè¡Œã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã®æ–‡å­—åˆ—\n",
    "        max_time (int): ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰\n",
    "    Returns:\n",
    "        Any: å®Ÿè¡Œçµæœã€å¤±æ•—ã—ãŸå ´åˆã¯None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 3ç§’ä»¥å†…ã«å®Ÿè¡Œã‚’å®Œäº†ã§ããªã„å ´åˆã¯ä¾‹å¤–ã‚’ç™ºç”Ÿã•ã›ã‚‹\n",
    "        with timeout(max_time, formula):\n",
    "\n",
    "            # SyntaxWarningã‚’ç„¡è¦–\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", SyntaxWarning)\n",
    "\n",
    "                # ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\n",
    "                return eval(formula)\n",
    "\n",
    "    except Exception as e:\n",
    "        signal.alarm(0)\n",
    "        return None\n",
    "\n",
    "# æ¤œè¨¼\n",
    "eval_with_timeout(\"44 * 99\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad86658",
   "metadata": {},
   "source": [
    "ç”Ÿæˆä¸­ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã§Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d63df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_calculator(expr):\n",
    "    \"\"\"\n",
    "    Pythonã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œè¨¼ã—ã¦å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹\n",
    "    æ¨è«–æ™‚ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã§ä½¿ç”¨ï¼ˆTool Useï¼‰\n",
    "\n",
    "    Args:\n",
    "        expr (str): ã‚³ãƒ¼ãƒ‰\n",
    "    Returns:\n",
    "        Any: è©•ä¾¡çµæœã€å±é™ºãªå¼ã®å ´åˆã¯None\n",
    "    \"\"\"\n",
    "    logger.debug(f\"ã‚³ãƒ¼ãƒ‰ã‚’è©•ä¾¡é–‹å§‹ {expr=}\")\n",
    "\n",
    "    # 1) ã‚³ãƒ¼ãƒ‰ãŒæ•°å¼ã®å ´åˆ\n",
    "\n",
    "    # æ•°å€¤ã‹ã‚‰ã‚«ãƒ³ãƒã‚’å‰Šé™¤\n",
    "    expr = expr.replace(\",\", \"\")\n",
    "\n",
    "    # ç´”ç²‹ãªæ•°å­¦å¼ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "    if all([x in \"0123456789*+-/.() \" for x in expr]):\n",
    "\n",
    "        # æŒ‡æ•°æ¼”ç®—å­ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ‹’å¦\n",
    "        if \"**\" in expr:\n",
    "            return None\n",
    "\n",
    "        # å®‰å…¨ã«è©•ä¾¡\n",
    "        return eval_with_timeout(expr)\n",
    "\n",
    "    # 2) ã‚³ãƒ¼ãƒ‰ãŒæ–‡å­—åˆ—æ“ä½œã®å ´åˆ\n",
    "\n",
    "    # [ã‚„{ãªã©ã®å±é™ºãªæ–‡å­—ã‚’å«ã‚€å ´åˆã¯æ‹’å¦\n",
    "    allowed_chars = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'\\\"()._ \"\n",
    "    if not all([x in allowed_chars for x in expr]):\n",
    "        return None\n",
    "\n",
    "    # å±é™ºãªæ–‡å­—åˆ—ã‚’å«ã‚€å ´åˆã¯æ‹’å¦ \n",
    "    dangerous_patterns = ['__', 'import', 'exec', 'eval', 'compile', 'open', 'file',\n",
    "                         'input', 'raw_input', 'globals', 'locals', 'vars', 'dir',\n",
    "                         'getattr', 'setattr', 'delattr', 'hasattr']\n",
    "    expr_lower = expr.lower()\n",
    "    if any(pattern in expr_lower for pattern in dangerous_patterns):\n",
    "        return None\n",
    "\n",
    "    # countãƒ¡ã‚½ãƒƒãƒ‰ä»¥å¤–ã¯æ‹’å¦\n",
    "    if '.count(' not in expr:\n",
    "        return None\n",
    "\n",
    "    # å®‰å…¨ã«è©•ä¾¡\n",
    "    return eval_with_timeout(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50b2b1",
   "metadata": {},
   "source": [
    "ç”Ÿæˆä¸­ã®ãƒ•ãƒ©ã‚°ã‚’ç®¡ç†ã—ãªã‚‰æ¨è«–ã‚’è¡Œã†ã‚¨ãƒ³ã‚¸ãƒ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        logger.debug(f\"Engineã‚’åˆæœŸåŒ–é–‹å§‹ {model=} {tokenizer=}\")\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã§ä½¿ç”¨\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        logger.debug(\"Engineã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.inference_mode() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "    def generate(self, tokens, num_samples=1, max_tokens=None, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"\n",
    "        ãƒˆãƒ¼ã‚¯ãƒ³ã‚’1ã¤ãšã¤ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ç”Ÿæˆã™ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿\n",
    "\n",
    "        Args:\n",
    "            tokens (List[int]): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            num_samples (int): ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆè¡Œæ•°ï¼‰\n",
    "            max_tokens (int, optional): ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "            temperature (float): æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            top_k (int, optional): Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®Kå€¤\n",
    "            seed (int): ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
    "        Yields:\n",
    "            Tuple[List[int], List[int]]: å„è¡Œã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã¨å¯¾å¿œã™ã‚‹ãƒã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã‚’é–‹å§‹ {tokens=} {num_samples=} {max_tokens=} {temperature=} {top_k=} {seed=}\")\n",
    "\n",
    "        # 1) åˆæœŸåŒ–\n",
    "\n",
    "        # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å‹ã‚’æ¤œè¨¼\n",
    "        assert isinstance(tokens, list) and isinstance(tokens[0], int), \"expecting list of ints\"\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\n",
    "        device = self.model.get_device()\n",
    "\n",
    "        # ä¹±æ•°ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–\n",
    "        rng = torch.Generator(device=device)\n",
    "        rng.manual_seed(seed)\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’å–å¾—\n",
    "        get_special = lambda s: self.tokenizer.encode_special(s)\n",
    "        python_start = get_special(\"<|python_start|>\")\n",
    "        python_end = get_special(\"<|python_end|>\")\n",
    "        output_start = get_special(\"<|output_start|>\")\n",
    "        output_end = get_special(\"<|output_end|>\")\n",
    "        assistant_end = get_special(\"<|assistant_end|>\") # if sampled, ends row\n",
    "        bos = self.tokenizer.get_bos_token_id() # if sampled, ends row\n",
    "\n",
    "        # 2) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "        m = self.model.config\n",
    "        kv_model_kwargs = {\"num_heads\": m.n_kv_head, \"head_dim\": m.n_embd // m.n_head, \"num_layers\": m.n_layer}\n",
    "        kv_cache_prefill = KVCache(\n",
    "            batch_size=1, # ãƒãƒƒãƒã‚µã‚¤ã‚º1\n",
    "            seq_len=len(tokens), # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•\n",
    "            **kv_model_kwargs, # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š\n",
    "        )\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "        logger.debug(f\"{ids.shape=}\")\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’é †ä¼æ’­ã—ã¦KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰è¨­å®šï¼ˆprefillï¼‰\n",
    "        logits = self.model.forward(ids, kv_cache=kv_cache_prefill)\n",
    "        logger.debug(f\"{logits.shape=}\")\n",
    "\n",
    "        # æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¬¡ã®å˜èªã®äºˆæ¸¬ï¼‰ã‚’æŠ½å‡º\n",
    "        logits = logits[:, -1, :]\n",
    "        logger.debug(f\"{logits.shape=}\")\n",
    "\n",
    "        # æœ€åˆã®1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°    \n",
    "        # (B, 1)\n",
    "        next_ids = sample_next_token(logits, rng, temperature, top_k)\n",
    "        logger.debug(f\"{next_ids.shape=}\")\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’Pythonã®ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "        sampled_tokens = next_ids[:, 0].tolist()\n",
    "        logger.debug(f\"{sampled_tokens=}\")\n",
    "\n",
    "        # 2) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¤‡è£½\n",
    "\n",
    "        # ç”Ÿæˆç”¨ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆæœŸåŒ–\n",
    "        kv_length_hint = (len(tokens) + max_tokens) if max_tokens is not None else self.model.config.sequence_len\n",
    "        kv_cache_decode = KVCache(\n",
    "            batch_size=num_samples,\n",
    "            seq_len=kv_length_hint,\n",
    "            **kv_model_kwargs,\n",
    "        )\n",
    "        logger.debug(f\"{num_samples=} {kv_length_hint=}\")\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§äº‹å‰è¨­å®šã•ã‚ŒãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¤‡è£½\n",
    "        kv_cache_decode.prefill(kv_cache_prefill)\n",
    "\n",
    "        # Prefillç”¨ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä¸è¦ã«ãªã£ãŸã®ã§å‰Šé™¤\n",
    "        del kv_cache_prefill\n",
    "\n",
    "        # 3) å„ã‚µãƒ³ãƒ—ãƒ«ã®çŠ¶æ…‹ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        row_states = [RowState(tokens.copy()) for _ in range(num_samples)]\n",
    "        logger.debug(f\"{len(row_states)=}\")\n",
    "\n",
    "        # 4) ç”Ÿæˆãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "        # max_tokensã«é”ã™ã‚‹ã‹ã€å…¨ã‚µãƒ³ãƒ—ãƒ«ãŒå®Œäº†ã™ã‚‹ã¾ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "        num_generated = 0\n",
    "        first_iteration = True\n",
    "        while True:\n",
    "\n",
    "            # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«é”ã—ãŸå ´åˆã€åœæ­¢\n",
    "            if max_tokens is not None and num_generated >= max_tokens:\n",
    "                break\n",
    "\n",
    "            # å…¨ã‚µãƒ³ãƒ—ãƒ«ãŒå®Œäº†ã—ãŸå ´åˆã€åœæ­¢\n",
    "            if all(state.completed for state in row_states):\n",
    "                break\n",
    "\n",
    "            # æœ€åˆã®ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆ\n",
    "            if first_iteration:\n",
    "\n",
    "                # ãƒ—ãƒªãƒ•ã‚£ãƒ«ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨\n",
    "                # ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "                sampled_tokens = [sampled_tokens[0]] * num_samples\n",
    "                logger.debug(f\"æœ€åˆã®ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ— {sampled_tokens=}\")\n",
    "\n",
    "                first_iteration = False\n",
    "\n",
    "            # 2å›ç›®ä»¥é™ã®ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆ\n",
    "            else:\n",
    "\n",
    "                # é †ä¼æ’­ã‚’è¡Œã„ã€å„ã‚µãƒ³ãƒ—ãƒ«ã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "                # (B, T, vocab_size)\n",
    "                logits = self.model.forward(ids, kv_cache=kv_cache_decode)\n",
    "                logger.debug(f\"2å›ç›®ç§»è¡Œã®ç”Ÿæˆã‚¹ãƒ†ãƒƒãƒ— {logits.shape=}\")\n",
    "\n",
    "                # ç›´è¿‘ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "                # (B, vocab_size)\n",
    "                logits = logits[:, -1, :]\n",
    "                logger.debug(f\"{logits.shape=}\")\n",
    "\n",
    "                # å„ã‚µãƒ³ãƒ—ãƒ«ã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "                # (B, 1)\n",
    "                next_ids = sample_next_token(logits, rng, temperature, top_k)\n",
    "                logger.debug(f\"{next_ids.shape=}\")\n",
    "\n",
    "                # ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "                sampled_tokens = next_ids[:, 0].tolist()\n",
    "                logger.debug(f\"{sampled_tokens=}\")\n",
    "\n",
    "            # ãã‚Œãã‚Œã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†\n",
    "            token_column = [] # contains the next token id along each row\n",
    "            token_masks = [] # contains the mask (was it sampled (1) or forced (0)?) along each row\n",
    "            for i, state in enumerate(row_states):\n",
    "\n",
    "                # å¼·åˆ¶çš„ã«æŒ¿å…¥ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹ã‹ã‚’ç¢ºèª\n",
    "                is_forced = len(state.forced_tokens) > 0\n",
    "\n",
    "                # ãƒã‚¹ã‚¯ã‚’ä½œæˆ \n",
    "                # 0ãªã‚‰å¼·åˆ¶ãƒˆãƒ¼ã‚¯ãƒ³ã€1ãªã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "                token_masks.append(0 if is_forced else 1)\n",
    "\n",
    "                # å¼·åˆ¶ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ªå…ˆã—ã¦æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ±ºå®š\n",
    "                next_token = state.forced_tokens.popleft() if is_forced else sampled_tokens[i]\n",
    "\n",
    "                # ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                token_column.append(next_token)\n",
    "\n",
    "                # çŠ¶æ…‹ã®ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                state.current_tokens.append(next_token)\n",
    "\n",
    "                # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ<|assistant_end|>ã¾ãŸã¯<|bos|>ã®å ´åˆ\n",
    "                if next_token == assistant_end or next_token == bos:\n",
    "                    # ç”Ÿæˆå®Œäº†ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹\n",
    "                    state.completed = True\n",
    "\n",
    "                # <|python_start|>ãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆ\n",
    "                if next_token == python_start:\n",
    "                    # Pythonã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã«å…¥ã‚‹ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹\n",
    "                    state.in_python_block = True\n",
    "                    # Pythonã‚³ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’åˆæœŸåŒ–\n",
    "                    state.python_expr_tokens = []\n",
    "\n",
    "                # <|python_end|>ãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆ\n",
    "                elif next_token == python_end and state.in_python_block:\n",
    "                    # Pythonã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰å‡ºã‚‹ãƒ•ãƒ©ã‚°ã‚’ä¸‹ã‚ã™\n",
    "                    state.in_python_block = False\n",
    "\n",
    "                    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å®Ÿè¡Œ\n",
    "                    if state.python_expr_tokens:\n",
    "                        # ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’Pythonã‚³ãƒ¼ãƒ‰ã®æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "                        expr = self.tokenizer.decode(state.python_expr_tokens)\n",
    "\n",
    "                        # ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™\n",
    "                        result = use_calculator(expr)\n",
    "\n",
    "                        # çµæœãŒã‚ã‚‹å ´åˆ\n",
    "                        if result is not None:\n",
    "                            # çµæœã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                            result_tokens = self.tokenizer.encode(str(result))\n",
    "\n",
    "                            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã§å›²ã‚“ã§å¼·åˆ¶æŒ¿å…¥ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ \n",
    "                            # <|output_start|> result <|output_end|>\n",
    "                            state.forced_tokens.append(output_start)\n",
    "                            state.forced_tokens.extend(result_tokens)\n",
    "                            state.forced_tokens.append(output_end)\n",
    "\n",
    "                    # Pythonã‚³ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢\n",
    "                    state.python_expr_tokens = []\n",
    "\n",
    "                # Pythonã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å ´åˆ\n",
    "                elif state.in_python_block:\n",
    "                    # ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’Pythonã‚³ãƒ¼ãƒ‰ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                    state.python_expr_tokens.append(next_token)\n",
    "\n",
    "            # å„è¡Œã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨ãƒã‚¹ã‚¯ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›\n",
    "            yield token_column, token_masks\n",
    "\n",
    "            # ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’åŠ ç®—\n",
    "            num_generated += 1\n",
    "\n",
    "            # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã«ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "            ids = torch.tensor(token_column, dtype=torch.long, device=device).unsqueeze(1)\n",
    "\n",
    "    def generate_batch(self, tokens, num_samples=1, **kwargs):\n",
    "        \"\"\"\n",
    "        éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰ˆ\n",
    "\n",
    "        Args:\n",
    "            tokens (List[int]): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            num_samples (int): ç”Ÿæˆã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆè¡Œæ•°ï¼‰\n",
    "            **kwargs: generateãƒ¡ã‚½ãƒƒãƒ‰ã«æ¸¡ã™è¿½åŠ ã®å¼•æ•°\n",
    "        Returns:\n",
    "            Tuple[List[List[int]], List[List[int]]]: å„ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã¨å¯¾å¿œã™ã‚‹ãƒã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ\n",
    "        \"\"\"\n",
    "        logger.debug(f\"ãƒãƒƒãƒç”Ÿæˆã‚’é–‹å§‹ {tokens=} {num_samples=} {kwargs=}\")\n",
    "\n",
    "        # 1) åˆæœŸåŒ–\n",
    "\n",
    "        # ã‚¹ãƒˆãƒƒãƒ—ãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’å–å¾—\n",
    "        assistant_end = self.tokenizer.encode_special(\"<|assistant_end|>\")\n",
    "        bos = self.tokenizer.get_bos_token_id()\n",
    "\n",
    "        # å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å„ã‚µãƒ³ãƒ—ãƒ«ã«ã‚³ãƒ”ãƒ¼\n",
    "        results = [tokens.copy() for _ in range(num_samples)]\n",
    "\n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã®ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–\n",
    "        # 0ã¯å¼·åˆ¶çš„ã«æŒ¿å…¥ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã€1ã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³\n",
    "        masks = [[0] * len(tokens) for _ in range(num_samples)]\n",
    "\n",
    "        # å„ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆå®Œäº†ãƒ•ãƒ©ã‚°ã‚’åˆæœŸåŒ–\n",
    "        completed = [False] * num_samples\n",
    "\n",
    "        # 2) ç”Ÿæˆãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "        for token_column, token_masks in self.generate(tokens, num_samples, **kwargs):\n",
    "\n",
    "            # å„ã‚µãƒ³ãƒ—ãƒ«ã®ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒã‚¹ã‚¯ã‚’åé›†\n",
    "            for i, (token, mask) in enumerate(zip(token_column, token_masks)):\n",
    "\n",
    "                # æœªå®Œäº†ã®å ´åˆ\n",
    "                if not completed[i]:\n",
    "\n",
    "                    # çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ\n",
    "                    if token == assistant_end or token == bos:\n",
    "\n",
    "                        # å®Œäº†ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹\n",
    "                        completed[i] = True\n",
    "\n",
    "                    # é€šå¸¸ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ\n",
    "                    else:\n",
    "                        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                        results[i].append(token)\n",
    "\n",
    "                        # ãƒã‚¹ã‚¯ã‚’è¿½åŠ \n",
    "                        masks[i].append(mask)\n",
    "\n",
    "            # ã™ã¹ã¦ã®å®Œäº†ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹å ´åˆã€ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†\n",
    "            if all(completed):\n",
    "                break\n",
    "\n",
    "        logger.debug(f\"ãƒãƒƒãƒç”Ÿæˆå®Œäº† {results=} {masks=}\")\n",
    "        return results, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81361726",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5752e38",
   "metadata": {},
   "source": [
    "### ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9f1e5",
   "metadata": {},
   "source": [
    "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ±ºå®šä¾‹:\n",
    "\n",
    "1. äºˆç®—ã‚’æ±ºã‚ã‚‹\n",
    "    - 8å°ã®A100ã‚’40æ™‚é–“\n",
    "1. äºˆç®—å†…ã§è¨“ç·´ã§ããã†ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’çµŒé¨“çš„ã«æ±ºã‚ã‚‹\n",
    "    - 32å±¤ï¼ˆdepth=32ï¼‰\n",
    "1. VRAMã®é™ç•Œã‚’æ¢ã‚‹\n",
    "    - device_batch_sizeã‚’å¤‰ãˆã¦ã€ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆOOM, Out of Memoryï¼‰ã«ãªã‚‰ãªã„æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¢ã™\n",
    "    - nvidia-smiã§MFUï¼ˆModel Flops Utilization, GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼‰ãŒ50%ä»¥ä¸‹ã ã¨è‰¯å¥½\n",
    "1. ç·è¨“ç·´æ™‚é–“ã®æ¤œè¨¼\n",
    "    - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—\n",
    "    - Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã§è¨“ç·´ã«å¿…è¦ãªç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ±‚ã‚ã‚‹ï¼ˆ38Bï¼‰\n",
    "        - è¨“ç·´ã«å¿…è¦ãªç·ãƒˆãƒ¼ã‚¯ãƒ³æ•° = 20 * ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°\n",
    "    - ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è‡ªå‹•è¨ˆç®—ï¼ˆ71,680ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n",
    "    - 1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®æ™‚é–“ã‚’å®Ÿæ¸¬ï¼ˆ1.572ç§’ï¼‰\n",
    "    - è¨“ç·´ã«å¿…è¦ãªæ™‚é–“ã‚’è¨ˆç®—ï¼ˆ71,680 * 1.572 = 31.3æ™‚é–“ï¼‰\n",
    "    - æ®‹ã‚Šã®10æ™‚é–“ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«å‰²ã‚Šå½“ã¦ã‚‹\n",
    "1. å¿…è¦ãƒ‡ãƒ¼ã‚¿é‡ã®è¨ˆç®—\n",
    "    - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æ€§èƒ½ã‚’ç¢ºèªï¼ˆ-4.8æ–‡å­—/ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰\n",
    "    - å¿…è¦æ–‡å­—æ•°ã‚’è¨ˆç®—ï¼ˆè¨“ç·´ã«å¿…è¦ãªç·ãƒˆãƒ¼ã‚¯ãƒ³æ•° * 4.8 = 185Bæ–‡å­—\n",
    "    - 1ã‚·ãƒ£ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æ–‡å­—æ•°ã‚’è¨­å®š 250Mæ–‡å­—\n",
    "    - å¿…è¦ã‚·ãƒ£ãƒ¼ãƒ‰æ•° 185B / 250M = 740ã‚·ãƒ£ãƒ¼ãƒ‰\n",
    "    - å®‰å…¨ã®ãŸã‚ 800ã‚·ãƒ£ãƒ¼ãƒ‰ã«åˆ‡ã‚Šä¸Šã’ï¼ˆä¸è¶³ã™ã‚‹ã¨èª­ã¿ç›´ã™ãŸã‚éå­¦ç¿’ã®ãƒªã‚¹ã‚¯ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f46151b",
   "metadata": {},
   "source": [
    "#### åŸºæœ¬è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹GPUæ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯8\n",
    "NPROC_PER_NODE=1\n",
    "\n",
    "# Weights & Biasesã®å®Ÿè¡Œå\n",
    "# \"dummy\"ã®å ´åˆã¯ãƒ­ã‚®ãƒ³ã‚°ã‚’ç„¡åŠ¹ã«ã™ã‚‹\n",
    "run = \"dummy\"\n",
    "\n",
    "# å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹\n",
    "# \"\"ã®å ´åˆã¯CUDA > MPS > CPUã®é †ã§è‡ªå‹•æ¤œå‡ºã™ã‚‹\n",
    "device_type = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36857328",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ‡ãƒ«è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20ï¼ˆd20ï¼‰\n",
    "depth = 20\n",
    "\n",
    "# æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2048\n",
    "max_seq_len = 2048\n",
    "\n",
    "# è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯-1ï¼ˆç„¡åŠ¹ï¼‰\n",
    "num_iterations = -1\n",
    "\n",
    "# ç›®æ¨™ã¨ã™ã‚‹ç·è¨ˆç®—é‡ï¼ˆFLOPsï¼‰\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯-1.0ï¼ˆç„¡åŠ¹ï¼‰\n",
    "target_flops = -1.0\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«å¯¾ã™ã‚‹è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¯”ç‡\n",
    "# num_iterationsã‚ˆã‚Šã‚‚å„ªå…ˆã•ã‚Œã€è¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒè‡ªå‹•è¨ˆç®—ã•ã‚Œã‚‹\n",
    "# Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ï¼ˆè¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•° = 20 * ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰ã«åŸºã¥ã\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20ã§ã€-1ã®å ´åˆã¯ç„¡åŠ¹\n",
    "target_param_data_ratio = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fddf41",
   "metadata": {},
   "source": [
    "#### æœ€é©åŒ–è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1GPUã‚ãŸã‚Šã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "# ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ¸›ã‚‰ã™\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯32\n",
    "# 4 -> VRAM 31GBä½¿ç”¨\n",
    "# 1 -> VRAM 10GBç¨‹åº¦\n",
    "device_batch_size = 1 if gpu_name == \"T4\" else 4\n",
    "\n",
    "# ç›®æ¨™ã¨ã™ã‚‹ç·ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°å˜ä½ï¼‰\n",
    "# å‹¾é…è“„ç©ï¼ˆGradient Accumulationï¼‰ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è‡ªå‹•è¨ˆç®—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯524,288ãƒˆãƒ¼ã‚¯ãƒ³ = 32 * 2048 * 8 = device_batch_size * max_seq_len * world_size\n",
    "total_batch_size = 524_288\n",
    "\n",
    "# å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤ã®AdamWã®å­¦ç¿’ç‡\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.2\n",
    "embedding_lr = 0.2\n",
    "\n",
    "# æœ€çµ‚å±¤ï¼ˆlm_headï¼‰ã®AdamWã®å­¦ç¿’ç‡\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.004\n",
    "unembedding_lr = 0.004\n",
    "\n",
    "# AdamWã®é‡ã¿æ¸›è¡°ä¿‚æ•°ï¼ˆL2æ­£å‰‡åŒ–ï¼‰\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Transformerã®è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Muonã®å­¦ç¿’ç‡\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.02\n",
    "matrix_lr = 0.02\n",
    "\n",
    "# å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°\n",
    "# 0.0ã®å ´åˆã¯ç„¡åŠ¹\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1.0\n",
    "grad_clip = 1.0\n",
    "\n",
    "# å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®å‰²åˆ\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.0\n",
    "warmup_ratio = 0.0\n",
    "\n",
    "# å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ãƒ€ã‚¦ãƒ³ã®å‰²åˆ\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.2\n",
    "warmdown_ratio = 0.2\n",
    "\n",
    "# æœ€çµ‚çš„ãªå­¦ç¿’ç‡ã‚’åˆæœŸå­¦ç¿’ç‡ã®ä½•å€ã«ã™ã‚‹ã‹\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0.0\n",
    "final_lr_frac = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5ca16e",
   "metadata": {},
   "source": [
    "#### è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46588806",
   "metadata": {},
   "source": [
    "è¨“ç·´ä¸­ã®è©•ä¾¡ã¯2ã¤:\n",
    "\n",
    "- BPBè©•ä¾¡ï¼ˆBits per bytes, ãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªããƒã‚¤ãƒˆæ•°ã®å¹³å‡ã§æå¤±ã‚’è¨ˆç®—ã™ã‚‹æ‰‹æ³•ï¼‰\n",
    "- COREãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆ22å€‹ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af568b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPBè©•ä¾¡ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯250\n",
    "eval_every = 10 if OTAMESHI_MODE else 250\n",
    "\n",
    "# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡ã«ä½¿ã†ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20 * 524,288ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ10Mãƒˆãƒ¼ã‚¯ãƒ³ï¼‰\n",
    "eval_tokens = 20 * 524288\n",
    "\n",
    "# ã‚³ã‚¢ãƒ¡ãƒˆãƒªãƒƒã‚¯è©•ä¾¡ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "# -1ã®å ´åˆã¯ç„¡åŠ¹\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000\n",
    "core_metric_every = 10 if OTAMESHI_MODE else 2000\n",
    "\n",
    "# ã‚³ã‚¢ãƒ¡ãƒˆãƒªãƒƒã‚¯è©•ä¾¡ã‚’ã™ã‚‹éš›ã®å„ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯500\n",
    "core_metric_max_per_task = 500\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2000\n",
    "sample_every = 10 if OTAMESHI_MODE else 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd98d58",
   "metadata": {},
   "source": [
    "#### å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå\n",
    "# \"\"ã®å ´åˆã¯depthã«åŸºã¥ã„ã¦è‡ªå‹•ç”Ÿæˆ\n",
    "model_tag = \"\"\n",
    "\n",
    "# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã€è¾æ›¸ã«ã¾ã¨ã‚ã‚‹\n",
    "# WandBã®ãƒ­ã‚°ã«è¨˜éŒ²ã•ã‚Œã‚‹\n",
    "config_keys = [\n",
    "    k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "user_config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "user_config.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556a3136",
   "metadata": {},
   "source": [
    "#### è‡ªå‹•è¨ˆç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a40b9d",
   "metadata": {},
   "source": [
    "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚‰ã‹ã‚‰å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•è¨ˆç®—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autodetect_device_type():\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = \"mps\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "\n",
    "    logger.debug(f\"è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒã‚¤ã‚¹: {device_type}\")\n",
    "    return device_type\n",
    "\n",
    "device_type = autodetect_device_type() if device_type == \"\" else device_type\n",
    "logger.debug(f\"å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12952377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_init(device_type=\"cuda\"): # cuda|cpu|mps\n",
    "    \"\"\"\n",
    "    PyTorchã®å®Ÿè¡Œç’°å¢ƒã‚’åˆæœŸåŒ–ãƒ»è¨­å®šã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        device_type (str): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã®ç¨®é¡ (\"cuda\", \"mps\", \"cpu\")\n",
    "    Returns:\n",
    "        Tuple[bool, int, int, int, torch.device]:\n",
    "            - ddp (bool): Distributed Data ParallelãŒæœ‰åŠ¹ã‹ã©ã†ã‹\n",
    "            - ddp_rank (int): DDPã®ãƒ©ãƒ³ã‚¯\n",
    "            - ddp_local_rank (int): DDPã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ©ãƒ³ã‚¯\n",
    "            - ddp_world_size (int): DDPã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º\n",
    "            - device (torch.device): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ãƒ‡ãƒã‚¤ã‚¹ã®æ¤œè¨¼\n",
    "\n",
    "    assert device_type in [\"cuda\", \"mps\", \"cpu\"], \"Invalid device type atm\"\n",
    "\n",
    "    if device_type == \"cuda\":\n",
    "        assert torch.cuda.is_available(), \"Your PyTorch installation is not configured for CUDA but device_type is 'cuda'\"\n",
    "\n",
    "    if device_type == \"mps\":\n",
    "        assert torch.backends.mps.is_available(), \"Your PyTorch installation is not configured for MPS but device_type is 'mps'\"\n",
    "\n",
    "    # 2) å†ç¾æ€§ã®è¨­å®š\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.manual_seed(42)\n",
    "\n",
    "    # é€Ÿåº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å®Œå…¨ãªå†ç¾æ€§ã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # 3) è¨ˆç®—ç²¾åº¦ã®è¨­å®š\n",
    "\n",
    "    if device_type == \"cuda\":\n",
    "        # è¡Œåˆ—è¨ˆç®—ã®ç²¾åº¦ã‚’TF32ã«è¨­å®š\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "    # 4) åˆ†æ•£ç’°å¢ƒã®è¨­å®š\n",
    "\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "\n",
    "    # DDPãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "    if ddp and device_type == \"cuda\":\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ©ãƒ³ã‚¯ã«åŸºã¥ã„ã¦ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
    "        device = torch.device(\"cuda\", ddp_local_rank)\n",
    "        torch.cuda.set_device(device)\n",
    "\n",
    "        # åˆ†æ•£é€šä¿¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åˆæœŸåŒ–\n",
    "        dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "        dist.barrier()\n",
    "    # DDPãŒç„¡åŠ¹ãªå ´åˆ\n",
    "    else:\n",
    "        # å˜ä¸€ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨\n",
    "        device = torch.device(device_type)\n",
    "\n",
    "    if ddp_rank == 0:\n",
    "        logger.debug(f\"ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º {ddp_world_size=}\")\n",
    "\n",
    "    return ddp, ddp_rank, ddp_local_rank, ddp_world_size, device\n",
    "\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®åˆ¤å®š\n",
    "# ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ãƒ­ã‚°å‡ºåŠ›ãƒ»ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ»WandBã®è¨˜éŒ²ãªã©ã‚’æ‹…å½“\n",
    "master_process = ddp_rank == 0\n",
    "logger.debug(f\"ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ {master_process=} {ddp_rank=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAuto Mixed Precision, AMPï¼‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’è¨­å®š\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) \\\n",
    "    if device_type == \"cuda\" else nullcontext()\n",
    "\n",
    "logger.debug(f\"è‡ªå‹•æ··åˆç²¾åº¦ã‚’æœ‰åŠ¹åŒ– {device_type == 'cuda'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDAã®æ“ä½œã‚’åŒæœŸã™ã‚‹é–¢æ•°ã‚’è¨­å®š\n",
    "\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "logger.debug(f\"CUDAã®æ“ä½œã‚’åŒæœŸã™ã‚‹ {device_type == 'cuda'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ä¸­ã®æœ€å¤§VRAMä½¿ç”¨é‡ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã‚’è¨­å®š\n",
    "\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0\n",
    "logger.debug(f\"æœ€å¤§VRAMä½¿ç”¨é‡ã‚’å–å¾—ã™ã‚‹ {device_type == 'cuda'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077887d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights & Biasesã®åˆæœŸåŒ–\n",
    "\n",
    "class DummyWandb:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def log(self, *args, **kwargs):\n",
    "        pass\n",
    "    def finish(self):\n",
    "        pass\n",
    "\n",
    "use_dummy_wandb = run == \"dummy\" or not master_process\n",
    "\n",
    "# WandBã‚’åˆæœŸåŒ–\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=run, config=user_config)\n",
    "logger.debug(f\"WandBã‚’ä½¿ç”¨ã™ã‚‹ {not use_dummy_wandb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd41cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—\n",
    "tokenizer = get_tokenizer()\n",
    "logger.debug(f\"ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ {tokenizer.__class__.__name__}\")\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "logger.debug(f\"èªå½™æ•° {vocab_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bytes(device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    BPBè©•ä¾¡ã§ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ãƒã‚¤ãƒˆåˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "    Args:\n",
    "        device (str): ãƒ‡ãƒã‚¤ã‚¹ (\"cpu\" ã¾ãŸã¯ \"cuda\")\n",
    "    Returns:\n",
    "        Tensor: ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¤ãƒˆåˆ—ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "    assert os.path.exists(token_bytes_path), f\"Token bytes not found at {token_bytes_path}? It gets written by tok_train.py\"\n",
    "    with open(token_bytes_path, \"rb\") as f:\n",
    "        token_bytes = torch.load(f, map_location=device)\n",
    "    return token_bytes\n",
    "\n",
    "token_bytes = get_token_bytes(device=device)\n",
    "token_bytes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerã®å±¤ã®æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯20\n",
    "num_layers = depth\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ\n",
    "# ä¸€èˆ¬çš„ã«ã¯ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¯64ã§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã¨128ã«å¢—ã‚„ã™\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1280\n",
    "model_dim = depth * 64\n",
    "\n",
    "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ•°\n",
    "# å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒã‚’128ã«è¨­å®š\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10\n",
    "num_heads = max(1, (model_dim + 127) // 128)\n",
    "\n",
    "# ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã®ãƒ˜ãƒƒãƒ‰æ•°\n",
    "# GQAï¼ˆGroup Query Attentionï¼‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã¯num_headsã‚ˆã‚Šå°ã•ãè¨­å®š\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ï¼ˆç„¡åŠ¹ï¼‰\n",
    "num_kv_heads = num_heads\n",
    "\n",
    "logger.debug(f\"ãƒ¢ãƒ‡ãƒ«è¨­å®š {num_layers=} {model_dim=} {num_heads=} {num_kv_heads=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‹¾é…è“„ç©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "\n",
    "# 1å›ã®é †ä¼æ’­ãƒ»é€†ä¼æ’­ã§1GPUãŒå‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ç·æ•°\n",
    "# 8192\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len\n",
    "\n",
    "# 1å›ã®é †ä¼æ’­ãƒ»é€†ä¼æ’­ã§å…¨GPUãŒå‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ç·æ•°\n",
    "# 8192\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size\n",
    "\n",
    "# åˆè¨ˆãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°å˜ä½ï¼‰ãŒå‡¦ç†ã™ã‚‹ç·æ•°ã§å‰²ã‚Šåˆ‡ã‚Œã‚‹ã‹ã‚’æ¤œè¨¼\n",
    "assert total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "\n",
    "# å‹¾é…è“„ç©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "# 64\n",
    "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n",
    "\n",
    "logger.debug(f\"æœ€é©åŒ–è¨­å®š {device_batch_size=} {tokens_per_fwdbwd=} {world_tokens_per_fwdbwd=} {total_batch_size=} {grad_accum_steps=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852ba1e",
   "metadata": {},
   "source": [
    "### åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703a5bae",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7639e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPTConfigã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã‚’è¨­å®š\n",
    "model_config_kwargs = dict(\n",
    "    sequence_len=max_seq_len,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=num_layers,\n",
    "    n_head=num_heads,\n",
    "    n_kv_head=num_kv_heads,\n",
    "    n_embd=model_dim\n",
    ")\n",
    "\n",
    "# ãƒ¡ã‚¿ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§å®Ÿä½“åŒ–ã›ãšã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "\n",
    "# ãƒ¡ãƒ¢ãƒªé ˜åŸŸã‚’ç¢ºä¿\n",
    "model.to_empty(device=device)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "model.init_weights()\n",
    "\n",
    "# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å‰ã®ãƒ¢ãƒ‡ãƒ«ã¸ã®å‚ç…§ã‚’å–å¾—\n",
    "# state_dictã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "orig_model = model\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "# dynamic=Falseã®å ´åˆã€å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã‚‹\n",
    "model = torch.compile(model, dynamic=False)\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—\n",
    "# 560,988,160ï¼ˆ5.6å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«å¿…è¦ãªè¨ˆç®—é‡ï¼ˆFLOPsï¼‰ã‚’æ¨å®š\n",
    "# 3,491,758,080ï¼ˆ3.5GFLOPsï¼‰\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "\n",
    "logger.debug(f\"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº† {num_params=:,} {num_flops_per_token=:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¨ˆç®—\n",
    "\n",
    "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å„ªå…ˆåº¦ã§è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ±ºå®š\n",
    "\n",
    "if num_iterations > 0:\n",
    "    logger.debug(f\"ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ä½¿ç”¨ {num_iterations=:,}\")\n",
    "\n",
    "elif target_flops > 0:\n",
    "\n",
    "    # ç›®æ¨™FLOPs / ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®FLOPs * 1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼‰\n",
    "    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))\n",
    "    logger.debug(f\"ç›®æ¨™ã¨ã™ã‚‹ç·è¨ˆç®—é‡ï¼ˆFLOPsï¼‰ã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸè¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•° {num_iterations=:,}\")\n",
    "\n",
    "elif target_param_data_ratio > 0:\n",
    "\n",
    "    # Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«åŸºã¥ã„ã¦å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    # 11,219,763,200 = 112å„„ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "    target_tokens = target_param_data_ratio * num_params\n",
    "    logger.debug(f\"{target_tokens=:,}\")\n",
    "\n",
    "    # å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•° / 1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "    # 11,219,763,200 / 524,288 = 21,400ã‚¹ãƒ†ãƒƒãƒ—\n",
    "    num_iterations = target_tokens // total_batch_size\n",
    "\n",
    "    logger.debug(f\"Chinchillaã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã‹ã‚‰è¨ˆç®—ã•ã‚ŒãŸè¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•° {num_iterations=:,}\")\n",
    "else:\n",
    "    raise ValueError(\"No training horizon specified\")\n",
    "\n",
    "# ç·è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "# 11,219,763,200 = 112å„„ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "total_tokens = total_batch_size * num_iterations\n",
    "\n",
    "logger.debug(f\"ç·è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®— {total_tokens=:,}\")\n",
    "logger.debug(f\"Chinchillaã®æ¯”ç‡ {total_tokens / num_params:.2f} tokens per param\") # 20\n",
    "logger.debug(f\"è¨“ç·´ã®æ¨å®šç·è¨ˆç®—é‡ {num_flops_per_token * total_tokens=:e}\") # 3.917670e+19 = 39.2PFLOPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207523a",
   "metadata": {},
   "source": [
    "#### æœ€é©åŒ–é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74514c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€é©åŒ–é–¢æ•°ã‚’åˆæœŸåŒ–\n",
    "\n",
    "optimizers = model.setup_optimizers(\n",
    "    unembedding_lr=unembedding_lr,\n",
    "    embedding_lr=embedding_lr,\n",
    "    matrix_lr=matrix_lr,\n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a634c",
   "metadata": {},
   "source": [
    "#### ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "\n",
    "base_dir = get_base_dir()\n",
    "tokens_dir = os.path.join(base_dir, \"tokenized_data\")\n",
    "\n",
    "train_loader = tokenizing_distributed_data_loader(\n",
    "    device_batch_size,\n",
    "    max_seq_len,\n",
    "    split=\"train\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "build_val_loader = lambda: tokenizing_distributed_data_loader(\n",
    "    device_batch_size,\n",
    "    max_seq_len,\n",
    "    split=\"val\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# æ¤œè¨¼\n",
    "x, y = next(train_loader)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3825c",
   "metadata": {},
   "source": [
    "#### å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’å®šç¾©\n",
    "\n",
    "def get_lr_multiplier(it):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’ç‡ã®å€ç‡ã‚’å–å¾—\n",
    "\n",
    "    Args:\n",
    "        it (int): ç¾åœ¨ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    Returns:\n",
    "        float: å­¦ç¿’ç‡ã®å€ç‡\n",
    "    \"\"\"\n",
    "\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "\n",
    "    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®å ´åˆ\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "\n",
    "    # å®šå¸¸çŠ¶æ…‹ã®å ´åˆ\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "\n",
    "    # ã‚¦ã‚©ãƒ¼ãƒ ãƒ€ã‚¦ãƒ³ã®å ´åˆ\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0915485",
   "metadata": {},
   "source": [
    "#### ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muonã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹é–¢æ•°\n",
    "\n",
    "def get_muon_momentum(it):\n",
    "    \"\"\"\n",
    "    Muonã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’å–å¾—\n",
    "\n",
    "    Args:\n",
    "        it (int): ç¾åœ¨ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    Returns:\n",
    "        float: Muonã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ \n",
    "    \"\"\"\n",
    "\n",
    "    # 0.0ã‹ã‚‰1.0ã¾ã§300ã‚¹ãƒ†ãƒƒãƒ—ã§ç·šå½¢ã«å¢—åŠ ã™ã‚‹å‰²åˆã‚’è¨ˆç®—\n",
    "    frac = min(it / 300, 1)\n",
    "\n",
    "    # 0.85ã‹ã‚‰0.95ã¾ã§ç·šå½¢ã«å¢—åŠ ã™ã‚‹ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’è¨ˆç®—\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f620ca22",
   "metadata": {},
   "source": [
    "### BPBè©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be94d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_bpb(model, batches, steps, token_bytes):\n",
    "    \"\"\"\n",
    "    BPB (bits per byte)ã§ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡\n",
    "    åˆè¨ˆæå¤±ã¨åˆè¨ˆãƒã‚¤ãƒˆæ•°ã‚’è¨ˆç®—ã—ã€ãã‚Œã‚‰ã‚’å‰²ã‚Šç®—ã—ã¦æ­£è¦åŒ–ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) åˆæœŸåŒ–\n",
    "\n",
    "    # æå¤±ã‚’è¨˜éŒ²ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆæœŸåŒ–\n",
    "    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®åˆè¨ˆãƒã‚¤ãƒˆæ•°ã‚’è¨˜éŒ²ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆæœŸåŒ–\n",
    "    total_bytes = torch.tensor(0, dtype=torch.int64, device=model.get_device())\n",
    "\n",
    "    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½œæˆ\n",
    "    batch_iter = iter(batches)\n",
    "\n",
    "    # 2) è©•ä¾¡ãƒ«ãƒ¼ãƒ—\n",
    "\n",
    "    # æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—æ•°ã ã‘è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
    "    for _ in range(steps):\n",
    "\n",
    "        # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨æ­£è§£ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "        x, y = next(batch_iter)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æå¤±ã‚’è¨ˆç®—\n",
    "        # loss_reduction='none'ã«ã‚ˆã‚Šã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®æå¤±ãŒè¿”ã•ã‚Œã‚‹\n",
    "        # (B, T)\n",
    "        loss2d = model(x, y, loss_reduction='none')\n",
    "\n",
    "        # ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "        # (B*T,)\n",
    "        loss2d = loss2d.view(-1)\n",
    "\n",
    "        # æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "        # (B*T,)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        # æ­£è§£ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ignore_indexï¼ˆ-1ï¼‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é™¤ã„ã¦é›†è¨ˆ\n",
    "        if (y.int() < 0).any():\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "            valid = y >= 0\n",
    "\n",
    "            # ç„¡åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’0ã«ç½®ãæ›ãˆ\n",
    "            y_safe = torch.where(valid, y, torch.zeros_like(y))\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¤ãƒˆæ•°ã«å¤‰æ›\n",
    "            num_bytes2d = torch.where(\n",
    "                valid,\n",
    "                token_bytes[y_safe],\n",
    "                torch.zeros_like(y, dtype=token_bytes.dtype)\n",
    "            )\n",
    "\n",
    "            # ignore_indexã¨ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ã—ã¦ï¼ˆnum_bytes2d > 0ï¼‰ã€æå¤±ã‚’é›†è¨ˆ\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "\n",
    "            # ãƒã‚¤ãƒˆæ•°ã‚’é›†è¨ˆ\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "\n",
    "        # æ­£è§£ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ignore_indexãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã€æ™®é€šã«é›†è¨ˆ\n",
    "        else:\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¤ãƒˆæ•°ã«å¤‰æ›\n",
    "            num_bytes2d = token_bytes[y]\n",
    "\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤å¤–ã—ã¦ï¼ˆnum_bytes2d > 0ï¼‰ã€æå¤±ã‚’é›†è¨ˆ\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "\n",
    "            # ãƒã‚¤ãƒˆæ•°ã‚’é›†è¨ˆ\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "\n",
    "\n",
    "    # DDPãŒæœ‰åŠ¹ãªå ´åˆã€å…¨ãƒ—ãƒ­ã‚»ã‚¹ã§åˆè¨ˆã‚’é›†ç´„\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    if world_size > 1:\n",
    "        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    # æœ€çµ‚çš„ãªBPBã‚’è¨ˆç®—ã—ã¦è¿”ã™\n",
    "    total_nats = total_nats.item()\n",
    "    total_bytes = total_bytes.item()\n",
    "    if total_bytes == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    # BPB = ç·æå¤± / ln(2) / ç·ãƒã‚¤ãƒˆæ•°\n",
    "    bpb = total_nats / (math.log(2) * total_bytes)\n",
    "    return bpb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89eda2d",
   "metadata": {},
   "source": [
    "### COREãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d23d9dd",
   "metadata": {},
   "source": [
    "[DataComp-LM](https://arxiv.org/abs/2406.11794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import yaml\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from jinja2 import Template\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ece356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¾¡ãƒãƒ³ãƒ‰ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "EVAL_BUNDLE_URL = \"https://karpathy-public.s3.us-west-2.amazonaws.com/eval_bundle.zip\"\n",
    "\n",
    "base_dir = get_base_dir()\n",
    "eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "\n",
    "if not os.path.exists(eval_bundle_dir):\n",
    "    # è©•ä¾¡ãƒãƒ³ãƒ‰ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡\n",
    "    import requests\n",
    "    import zipfile\n",
    "    from io import BytesIO\n",
    "\n",
    "    logger.debug(f\"è©•ä¾¡ãƒãƒ³ãƒ‰ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­ {EVAL_BUNDLE_URL}...\")\n",
    "    response = requests.get(EVAL_BUNDLE_URL)\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "        z.extractall(base_dir)\n",
    "    logger.debug(f\"è©•ä¾¡ãƒãƒ³ãƒ‰ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨è§£å‡ãŒå®Œäº† {eval_bundle_dir}\")\n",
    "else:\n",
    "    logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®è©•ä¾¡ãƒãƒ³ãƒ‰ãƒ« {os.listdir(eval_bundle_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª\n",
    "\n",
    "config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "len(config[\"icl_tasks\"]), config[\"icl_tasks\"]\n",
    "\n",
    "# å®Ÿè¡Œã™ã‚‹22ã®ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã¨è¨­å®šãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹\n",
    "# label: ã‚¿ã‚¹ã‚¯å\n",
    "# dataset_uri: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´æ‰€ï¼ˆJSONL)\n",
    "# num_fewshot: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã‚‹å›ç­”ä¾‹ã®æ•°\n",
    "# inc_task_type: è©•ä¾¡æ–¹æ³•\n",
    "#   - language_modeling: è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç¶šãã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ï¼‰\n",
    "#   - multiple_choice: å¤šè‚¢é¸æŠå¼ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªé¸æŠè‚¢ã‚’é¸ã¶ã‚¿ã‚¹ã‚¯ï¼‰\n",
    "#   - schema: ã‚¹ã‚­ãƒ¼ãƒï¼ˆé¸æŠè‚¢ã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é¸ã¶ã‚¿ã‚¹ã‚¯ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598897d3",
   "metadata": {},
   "source": [
    "- World Knowledgeï¼ˆä¸–ç•Œã®çŸ¥è­˜ï¼‰\n",
    "    - jeopardy: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - bigbench_qa_wikidata: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - arc_easy: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - arc_challenge: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "- Commonsense Reasoningï¼ˆå¸¸è­˜çš„æ¨è«–ï¼‰\n",
    "    - copa: 0ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - commonsense_qa: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - piqa: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - openbook_qa: 0ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "- Language Understandingï¼ˆè¨€èªç†è§£ï¼‰\n",
    "    - hellaswag_zeroshot: 0ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - lambada_openai: 0ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - hellaswag: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - winograd: 0ã‚·ãƒ§ãƒƒãƒˆ, ã‚¹ã‚­ãƒ¼ãƒ\n",
    "    - winogrande: 0ã‚·ãƒ§ãƒƒãƒˆ, ã‚¹ã‚­ãƒ¼ãƒ\n",
    "    - bigbench_language_identification: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "- Symbolic Problem Solvingï¼ˆè¨˜å·çš„å•é¡Œè§£æ±ºï¼‰\n",
    "    - bigbench_dyck_languages: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - agi_eval_lsat_ar: 3ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ\n",
    "    - bigbench_cs_algorithms: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - bigbench_operators: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - bigbench_repeat_copy_logic: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "-  Reading Comprehensionï¼ˆèª­è§£ï¼‰\n",
    "    - squad: 10ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - coqa: 0ã‚·ãƒ§ãƒƒãƒˆ, è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°\n",
    "    - boolq: 10ã‚·ãƒ§ãƒƒãƒˆ, å¤šè‚¢é¸æŠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9572119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª\n",
    "\n",
    "data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "data_path = os.path.join(data_base_path,  \"world_knowledge\", \"jeopardy_all.jsonl\")\n",
    "with open(data_path, 'r') as f:\n",
    "    lm_sample = json.loads(f.readline().strip())\n",
    "\n",
    "lm_sample\n",
    "\n",
    "# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ã“ã®æµ·è»å¸ä»¤å®˜ã¯1929å¹´11æœˆ28æ—¥ã‹ã‚‰29æ—¥ã«ã‹ã‘ã¦ã€ãƒªãƒˆãƒ«ã‚¢ãƒ¡ãƒªã‚«ã®åŸºåœ°ã‹ã‚‰å—æ¥µç‚¹ã¾ã§ã‚’å¾€å¾©æ—…è¡Œã—ãŸ\n",
    "# ç¶šã: ãƒªãƒãƒ£ãƒ¼ãƒ‰ãƒ»ãƒãƒ¼ãƒ‰æç£\n",
    "    \n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¨è©•ä¾¡ã®æµã‚Œ:\n",
    "# 1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ç¶šããªã—ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ç¶šãã‚ã‚Šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "# 2. ç¶šããªã—ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã€ç¶šãã‚’ç”Ÿæˆ\n",
    "# 3. ç”Ÿæˆã•ã‚ŒãŸç¶šãã‚’ç¶šãã‚ã‚Šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æ¯”è¼ƒã—ã€å®Œå…¨ä¸€è‡´ã‹ã‚’åˆ¤å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d2e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè‚¢é¸æŠå¼ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª\n",
    "\n",
    "data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "data_path = os.path.join(data_base_path,  \"language_understanding\", \"hellaswag.jsonl\")\n",
    "with open(data_path, 'r') as f:\n",
    "    mc_sample = json.loads(f.readline().strip())\n",
    "\n",
    "mc_sample\n",
    "\n",
    "# ã‚¯ã‚¨ãƒª: ä¸€äººã®ç”·ãŒå±‹æ ¹ã®ä¸Šã«åº§ã£ã¦ã„ã‚‹ã€‚å½¼ã¯\n",
    "# é¸æŠè‚¢0: ã‚¹ã‚­ãƒ¼æ¿ã‚’ãƒ©ãƒƒãƒ—ã§åŒ…ã‚“ã§ã„ã‚‹\n",
    "# é¸æŠè‚¢1: æ°´å¹³ãªã‚¿ã‚¤ãƒ«ã‚’å‰¥ãŒã—ã¦ã„ã‚‹\n",
    "# é¸æŠè‚¢2: ãƒ«ãƒ¼ãƒ“ãƒƒã‚¯ã‚­ãƒ¥ãƒ¼ãƒ–ã‚’æŒã£ã¦ã„ã‚‹\n",
    "# é¸æŠè‚¢3: å±‹æ ¹æã‚’å¼•ãå‰¥ãŒã—å§‹ã‚ã¦ã„ã‚‹\n",
    "# æ­£è§£: é¸æŠè‚¢3\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¨è©•ä¾¡ã®æµã‚Œ\n",
    "# 1. ã‚¯ã‚¨ãƒªã¨é¸æŠè‚¢ã‚’çµåˆã—ãŸ4ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "# 2. é¸æŠè‚¢ã®éƒ¨åˆ†ã®å¹³å‡æå¤±ã‚’è¨ˆç®—\n",
    "# 3. æœ€ã‚‚ä½ã„æå¤±ã‚’ç¤ºã™é¸æŠè‚¢ã‚’ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ã™ã‚‹\n",
    "# 4. ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨æ­£è§£ã‚’æ¯”è¼ƒã—ã€æ­£èª¤ã‚’åˆ¤å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª\n",
    "\n",
    "data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "data_path = os.path.join(data_base_path,  \"language_understanding\", \"winograd_wsc.jsonl\")\n",
    "with open(data_path, 'r') as f:\n",
    "    schema_sample = json.loads(f.readline().strip())\n",
    "\n",
    "schema_sample\n",
    "\n",
    "# é¸æŠè‚¢1: å¸‚è­°ä¼šè­°å“¡ãŸã¡ã¯ãƒ‡ãƒ¢éšŠã«è¨±å¯ã‚’ä¸ãˆãªã‹ã£ãŸã€‚ãªãœãªã‚‰å¸‚è­°ä¼šè­°å“¡ãŸã¡ã¯\n",
    "# é¸æŠè‚¢2: ãƒ‡ãƒ¢éšŠã¯å¸‚è­°ä¼šè­°å“¡ãŸã¡ã«è¨±å¯ã‚’ä¸ãˆãªã‹ã£ãŸã€‚ãªãœãªã‚‰ãƒ‡ãƒ¢éšŠã¯\n",
    "# å…±é€šã®ç¶šã: æš´åŠ›ã‚’æã‚Œã¦ã„ãŸã‹ã‚‰ã ã€‚\n",
    "# æ­£è§£: 1\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¨è©•ä¾¡ã®æµã‚Œ:\n",
    "# 1. é¸æŠè‚¢ã¨å…±é€šã®ç¶šãã‚’çµåˆã—ãŸ2ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "# 2. é †ä¼æ’­ã§å…±é€šã®ç¶šãã®éƒ¨åˆ†ã®å¹³å‡æå¤±ã‚’è¨ˆç®—\n",
    "# 3. æœ€ã‚‚ä½ã„æå¤±ã‚’ç¤ºã™é¸æŠè‚¢ã‚’ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨ã™ã‚‹\n",
    "# 4. ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨æ­£è§£ã‚’æ¯”è¼ƒã—ã€æ­£èª¤ã‚’åˆ¤å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompts_mc(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"\n",
    "    å¤šè‚¢é¸æŠï¼ˆmultiple choiceï¼‰ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "\n",
    "    ã“ã®ã‚¿ã‚¹ã‚¯ã¯ã€é¸æŠè‚¢ã”ã¨ã«åˆ¥ã€…ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€æœ€ã‚‚ä½ã„æå¤±ã‚’æŒã¤é¸æŠè‚¢ã‚’ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã¨è¦‹ãªã™\n",
    "\n",
    "    Args:\n",
    "        item (dict): è©•ä¾¡ã‚¢ã‚¤ãƒ†ãƒ ã®è¾æ›¸\n",
    "        continuation_delimiter (str): ç¶šãã®åŒºåˆ‡ã‚Šæ–‡å­—åˆ—\n",
    "        fewshot_examples (List[dict], optional): å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã®ä¾‹ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        List[str]: å„é¸æŠè‚¢ã«å¯¾å¿œã™ã‚‹å®Œå…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å®šç¾©\n",
    "    # ãŠæ‰‹æœ¬ + è³ªå• + ç¶šãã®åŒºåˆ‡ã‚Šæ–‡å­—åˆ— + é¸æŠè‚¢\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.query }}{{ continuation_delimiter }}{{ example.choices[example.gold] }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ item.query }}{{ continuation_delimiter }}{{ choice }}\"\"\".strip()\n",
    "\n",
    "    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "    template = Template(template_str)\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "    prompts = [template.render(choice=choice, **context) for choice in item['choices']]\n",
    "    return prompts\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "mc_prompts = render_prompts_mc(\n",
    "    mc_sample,\n",
    "    continuation_delimiter=\"<ANSWER>\",\n",
    "    fewshot_examples=[\n",
    "        {\n",
    "            \"query\": \"QUERY\",\n",
    "            \"choices\": [\n",
    "                \"CHOICE_1\",\n",
    "                \"CHOICE_2\",\n",
    "                \"CHOICE_3\",\n",
    "                \"CHOICE_4\"\n",
    "            ],\n",
    "            \"gold\": 0\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "mc_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737334fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompts_schema(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"\n",
    "    ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "    ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ã¯ã€ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ç‰¹å®šã®å½¢å¼ã§å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "    ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã¯ã€ã€Œç•°ãªã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ + å…±é€šã®ç¶šãï¼ˆsuffixï¼‰ã€ã®å½¢å¼ã‚’ã¨ã‚‹\n",
    "\n",
    "    Args:\n",
    "        item (dict): ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿é …ç›®\n",
    "        continuation_delimiter (str): ç¶šãã®åŒºåˆ‡ã‚Šæ–‡å­—åˆ—\n",
    "        fewshot_examples (List[dict], optional): å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã®ä¾‹ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        List[str]: ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ–‡å­—åˆ—\n",
    "    # ãŠæ‰‹æœ¬ï¼ˆfewshot examplesï¼‰ + ç•°ãªã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ + å…±é€šã®ç¶šãï¼ˆsuffixï¼‰\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.context_options[example.gold] }}{{ continuation_delimiter }}{{ example.continuation }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ context }}{{ continuation_delimiter }}{{ item.continuation }}\"\"\".strip()\n",
    "\n",
    "    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ\n",
    "    template = Template(template_str)\n",
    "\n",
    "    # å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã®ä¾‹ã‚’åˆæœŸåŒ–\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "\n",
    "    # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "    prompts = [template.render(context=context_option, **context)\n",
    "               for context_option in item['context_options']]\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "schema_prompts = render_prompts_schema(\n",
    "    schema_sample,\n",
    "    continuation_delimiter=\"<ANSWER>\",\n",
    "    fewshot_examples=[{\n",
    "        \"context_options\": [\n",
    "            \"CONTEXT_1 \",\n",
    "            \"CONTEXT_2 \"\n",
    "        ],\n",
    "        \"continuation\": \"CONTINUATION\",\n",
    "        \"gold\": 0\n",
    "    }]  \n",
    ")\n",
    "schema_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompts_lm(item, continuation_delimiter, fewshot_examples=None):\n",
    "    \"\"\"\n",
    "    è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        item (dict): è©•ä¾¡ã‚¢ã‚¤ãƒ†ãƒ \n",
    "        continuation_delimiter (str): ç¶šãã®åŒºåˆ‡ã‚Šæ–‡å­—åˆ—\n",
    "        fewshot_examples (list, optional): å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã®ä¾‹ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        list: ç¶šããªã—ã¨ç¶šãã‚ã‚Šã®2ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
    "\n",
    "    æ³¨æ„: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰‹å‹•ã§ãƒˆãƒªãƒŸãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚\n",
    "          ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ«å°¾ã«ç©ºç™½ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‚ˆã†ã§ã€\n",
    "          ãã‚Œã‚’é¿ã‘ã‚‹ãŸã‚ã§ã™ã€‚\n",
    "\n",
    "\n",
    "    Render complete prompt for a language modeling task.\n",
    "    Notice that we manually trim the context in the template,\n",
    "    which in some datasets seems to have trailing whitespace (which we don't want).\n",
    "    \"\"\"\n",
    "\n",
    "    # Jinja2å½¢å¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ–‡å­—åˆ—ã‚’ä½œæˆ\n",
    "    # fewshot example + ... + å•é¡Œæ–‡ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰+ ç¶šãã®åŒºåˆ‡ã‚Šæ–‡å­—åˆ— + ç¶šãï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "    # ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ«å°¾ã«ç©ºç™½ãŒå«ã¾ã‚Œã‚‹ãŸã‚trimãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨\n",
    "    template_str = \"\"\"\n",
    "{%- for example in fewshot_examples -%}\n",
    "{{ example.context | trim }}{{ continuation_delimiter }}{{ example.continuation }}\n",
    "\n",
    "{% endfor -%}\n",
    "{{ item.context | trim }}{{ continuation_delimiter }}{% if include_continuation %}{{ item.continuation }}{% endif %}\"\"\".strip()\n",
    "\n",
    "    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«\n",
    "    template = Template(template_str)\n",
    "\n",
    "    fewshot_examples = fewshot_examples or []\n",
    "\n",
    "    context = {\n",
    "        'fewshot_examples': fewshot_examples,\n",
    "        'continuation_delimiter': continuation_delimiter,\n",
    "        'item': item\n",
    "    }\n",
    "\n",
    "    # æ­£è§£ã®ç¶šããŒå«ã¾ã‚Œãªã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "    prompt_without = template.render(include_continuation=False, **context)\n",
    "\n",
    "    # æ­£è§£ã®ç¶šããŒå«ã¾ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "    prompt_with = template.render(include_continuation=True, **context)\n",
    "\n",
    "    # prompot_withoutã®æœ«å°¾ã«ç©ºç™½ãŒæ®‹ã‚‹ãƒã‚°ãŒã‚ã‚‹ãŸã‚æ˜ç¤ºçš„ã«ç©ºç™½ã‚’å‰Šé™¤\n",
    "    prompt_without = prompt_without.strip()\n",
    "\n",
    "    return [prompt_without, prompt_with]\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "lm_prompts = render_prompts_lm(\n",
    "    lm_sample,\n",
    "    continuation_delimiter=\"<ANSWER>\",\n",
    "    fewshot_examples=[{\n",
    "        \"context\": \"SAMPLE_CONTEXT\",\n",
    "        \"continuation\": \"SAMPLE_CONTINUATION\"\n",
    "    }\n",
    "])\n",
    "lm_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ff444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_length(token_sequences, direction='left'):\n",
    "    \"\"\"\n",
    "    ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€å…±é€šã®æ¥é ­è¾ï¼ˆprefixï¼‰ã¾ãŸã¯æ¥å°¾è¾ï¼ˆsuffixï¼‰ã®é•·ã•ã‚’è¿”ã™\n",
    "\n",
    "    å¤šè‚¢é¸æŠã‚¿ã‚¹ã‚¯ã¯è³ªå•æ–‡ãŒå…±é€šï¼ˆæ¥é ­è¾ï¼‰ = right\n",
    "    ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ã¯ç¶šãã®éƒ¨åˆ†ãŒå…±é€šï¼ˆæ¥å°¾è¾ï¼‰ = left\n",
    "\n",
    "    Args:\n",
    "        token_sequences (List[List[int]]): ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "        direction (str): 'left'ï¼ˆæ¥é ­è¾ï¼‰ã¾ãŸã¯ 'right'ï¼ˆæ¥å°¾è¾ï¼‰\n",
    "    Returns:\n",
    "        int: å…±é€šã®æ¥é ­è¾ã¾ãŸã¯æ¥å°¾è¾ã®é•·ã•\n",
    "    \"\"\"\n",
    "\n",
    "    # æœ€å°ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’å–å¾—\n",
    "    min_len = min(len(seq) for seq in token_sequences)\n",
    "\n",
    "    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ç¯„å›²ã‚’è¨­å®š\n",
    "    # leftã®å ´åˆ [0, 1, 2, ..., min_len-1]\n",
    "    # rightã®å ´åˆ [-1, -2, -3, ..., -min_len]\n",
    "    indices = {\n",
    "        'left': range(min_len),\n",
    "        'right': range(-1, -min_len-1, -1)\n",
    "    }[direction]\n",
    "\n",
    "    # å…±é€šã®é•·ã•ã‚’è¦‹ã¤ã‘ã‚‹\n",
    "    for i, idx in enumerate(indices):\n",
    "\n",
    "        # åŸºæº–ã¨ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "        token = token_sequences[0][idx]\n",
    "\n",
    "        # ã™ã¹ã¦ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§idxç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒç•°ãªã£ãŸå ´åˆ\n",
    "        if not all(seq[idx] == token for seq in token_sequences):\n",
    "            # ãã‚Œã¾ã§ã«ä¸€è‡´ã—ãŸå›æ•°ã‚’è¿”ã™ï¼ˆå…±é€šã®é•·ã•ï¼‰\n",
    "            return i\n",
    "\n",
    "    # ä¸ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã€æœ€ã‚‚çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¿”ã™\n",
    "    return min_len\n",
    "\n",
    "# æ¤œè¨¼\n",
    "mc_tokens = tokenizer(mc_prompts, prepend=tokenizer.get_bos_token_id())\n",
    "answer_start_idx = find_common_length(mc_tokens, direction='left')\n",
    "answer_start_idx\n",
    "# 4ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã†ã¡ã€32ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãŒé¸æŠè‚¢éƒ¨åˆ†ï¼ˆæå¤±è¨ˆç®—å¯¾è±¡ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3abf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_sequences(tokens, pad_token_id):\n",
    "    \"\"\"\n",
    "    é•·ã•ãŒç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒªã‚¹ãƒˆã‚’1ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«ã«ã¾ã¨ã‚ã‚‹ï¼ˆstackï¼‰\n",
    "\n",
    "    Args:\n",
    "        tokens (List[List[int]]): ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "        pad_token_id (int): ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ID\n",
    "    Returns:\n",
    "        Tensor: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¨ˆç®—\n",
    "    bsz, seq_len = len(tokens), max(len(x) for x in tokens)\n",
    "\n",
    "    # ãƒ‘ãƒƒãƒ‡ã‚£ãƒ³ã‚°ã§åˆæœŸåŒ–ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "    # (batch_size, seq_len)\n",
    "    input_ids = torch.full((bsz, seq_len), pad_token_id, dtype=torch.long)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«ä¸Šæ›¸ã\n",
    "    for i, x in enumerate(tokens):\n",
    "        input_ids[i, :len(x)] = torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "# æ¤œè¨¼\n",
    "pad_token_id = tokenizer.get_bos_token_id()\n",
    "print(len(mc_tokens[0]), len(mc_tokens[1]), len(mc_tokens[2]), len(mc_tokens[3]))\n",
    "stack_sequences(mc_tokens, pad_token_id).shape\n",
    "# ç•°ãªã‚‹é•·ã•ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequences_mc(tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    å¤šè‚¢é¸æŠï¼ˆmultiple choiceï¼‰ã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹\n",
    "    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
    "    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã¾ã‚Œã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯åŒã˜ã ãŒã€é¸æŠè‚¢ãŒç•°ãªã‚‹å½¢å¼\n",
    "\n",
    "    Args:\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹\n",
    "        prompts (List[str]): å„é¸æŠè‚¢ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], List[int], List[int]]:\n",
    "            - tokens (List[List[int]]): å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            - start_indices (List[int]): å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "            - end_indices (List[int]): å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æå¤±è¨ˆç®—å¯¾è±¡ã®çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "\n",
    "    # å…±é€šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®é•·ã•ã‚’å–å¾—ã—ã€æå¤±å¯¾è±¡ã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    answer_start_idx = find_common_length(tokens, direction='left')\n",
    "\n",
    "    # é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¤‡è£½\n",
    "    start_indices = [answer_start_idx] * len(prompts)\n",
    "\n",
    "    # çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã¨ã™ã‚‹\n",
    "    end_indices = [len(x) for x in tokens]\n",
    "\n",
    "    return tokens, start_indices, end_indices\n",
    "\n",
    "# æ¤œè¨¼\n",
    "mc_tokens, mc_start_indices, mc_end_indices = batch_sequences_mc(tokenizer, mc_prompts)\n",
    "len(mc_tokens), mc_start_indices, mc_end_indices\n",
    "# 4ã¤ã®é¸æŠè‚¢ãã‚Œãã‚Œã«å¯¾ã—ã¦ã€æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99277c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequences_schema(tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    ã‚¹ã‚­ãƒ¼ãƒã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        prompts (List[str]): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], List[int], List[int]]:\n",
    "            - tokens: å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            - start_indices: è©•ä¾¡å¯¾è±¡ã®ç¶šãã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "            - end_indices: è©•ä¾¡å¯¾è±¡ã®ç¶šãã®çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¨ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "\n",
    "    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç®—å‡º\n",
    "    suffix_length = find_common_length(tokens, direction='right')\n",
    "\n",
    "    # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã‚’å–å¾—\n",
    "    end_indices = [len(x) for x in tokens]\n",
    "\n",
    "    # æ¥å°¾è¾ã®é•·ã•ã‚’å¼•ã„ã¦ç¶šãã®é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç®—å‡º\n",
    "    start_indices = [ei - suffix_length for ei in end_indices]\n",
    "\n",
    "    return tokens, start_indices, end_indices\n",
    "\n",
    "# æ¤œè¨¼\n",
    "schema_tokens, schema_start_indices, schema_end_indices = batch_sequences_schema(tokenizer, schema_prompts)\n",
    "len(schema_tokens), schema_start_indices, schema_end_indices\n",
    "# 2ã¤ã®é¸æŠè‚¢ãã‚Œãã‚Œã«å¯¾ã—ã¦ã€æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequences_lm(tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "        prompts (List[str]): ã€Œæ–‡è„ˆã®ã¿ã€ã¨ã€Œæ–‡è„ˆã¨ãã®ç¶šãã®æ­£è§£ã€ã‚’å«ã‚€2ã¤ã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        Tuple[List[List[int]], List[int], List[int]]\n",
    "        ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆã€é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã€çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # 2ã¤ã®æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    tokens = tokenizer(prompts, prepend=tokenizer.get_bos_token_id())\n",
    "\n",
    "    # ã€Œæ–‡è„ˆã®ã¿ã€ã¨ã€Œæ–‡è„ˆã¨ãã®ç¶šãã®æ­£è§£ã€ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’å–å¾—\n",
    "    tokens_without, tokens_with = tokens\n",
    "\n",
    "    # æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "    start_idx, end_idx = len(tokens_without), len(tokens_with)\n",
    "\n",
    "    assert start_idx < end_idx, \"prompt without is supposed to be a prefix of prompt with\"\n",
    "    assert tokens_without == tokens_with[:start_idx], \"prompt without is supposed to be a prefix of prompt with\"\n",
    "\n",
    "    # æ­£è§£ã®ç¶šãã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã€æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¿”ã™\n",
    "    return [tokens_with], [start_idx], [end_idx]\n",
    "\n",
    "# æ¤œè¨¼\n",
    "lm_tokens, lm_start_indices, lm_end_indices = batch_sequences_lm(tokenizer, lm_prompts)\n",
    "len(lm_tokens), lm_start_indices, lm_end_indices\n",
    "# 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¯¾ã—ã¦ã€æå¤±è¨ˆç®—å¯¾è±¡ã®é–‹å§‹ãƒ»çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¾—ã‚‰ã‚Œã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "def forward_model(model, input_ids):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’è¡Œã„ã€ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®æå¤±ã¨äºˆæ¸¬ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model: ãƒ¢ãƒ‡ãƒ«\n",
    "        input_ids (Tensor): (B, T)ã®ãƒˆãƒ¼ã‚¯ãƒ³IDãƒ†ãƒ³ã‚½ãƒ«\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: (B, T)ã®æå¤±ãƒ†ãƒ³ã‚½ãƒ«ã¨argmaxäºˆæ¸¬ãƒ†ãƒ³ã‚½ãƒ«\n",
    "\n",
    "    Take BxT tensor of token ids, return BxT tensor of losses and argmax predictions.\n",
    "    The last column of losses is set to nan because we don't have autoregressive targets there.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len = input_ids.size()\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã¦å‡ºåŠ›ã‚’å–å¾—\n",
    "    # (batch_size, seq_len, vocab_size)\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å·¦ã«1ã¤ã‚·ãƒ•ãƒˆã—ã¦æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "    target_ids = torch.roll(input_ids, shifts=-1, dims=1)\n",
    "\n",
    "    # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—\n",
    "    losses = torch.nn.functional.cross_entropy(\n",
    "        outputs.view(batch_size * seq_len, -1), # (batch_size*seq_len, vocab_size)\n",
    "        target_ids.view(batch_size * seq_len), # (batch_size*seq_len,)\n",
    "        reduction='none', # ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®æå¤±ã‚’å–å¾—\n",
    "    ).view(batch_size, seq_len) # (batch_size, seq_len)\n",
    "\n",
    "    # æœ€å¾Œã®ä½ç½®ã¯æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€æå¤±ã‚’NaNã«è¨­å®š\n",
    "    losses[:, -1] = float('nan')\n",
    "\n",
    "    # å„ä½ç½®ã§ã®argmaxäºˆæ¸¬ã‚’å–å¾—\n",
    "    # (batch_size, seq_len)\n",
    "    predictions = outputs.argmax(dim=-1)\n",
    "\n",
    "    return losses, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "def evaluate_example(idx, model, tokenizer, data, device, task_meta):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è©•ä¾¡ã—ã€æ­£è§£ãªã‚‰Trueã€ãã†ã§ãªã‘ã‚Œã°Falseã‚’è¿”ã™\n",
    "\n",
    "    Args:\n",
    "        idx (int): è©•ä¾¡ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        model (torch.nn.Module): è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "        data (list): è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "        device (torch.device): ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "        task_meta (dict): ã‚¿ã‚¹ã‚¯ã®ãƒ¡ã‚¿æƒ…å ±\n",
    "    Returns:\n",
    "        bool: æ­£è§£ãªã‚‰Trueã€ãã†ã§ãªã‘ã‚Œã°False\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Few-shotã‚µãƒ³ãƒ—ãƒ«ã®æº–å‚™\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—\n",
    "    item = data[idx]\n",
    "\n",
    "    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’å–å¾—\n",
    "    task_type = task_meta['task_type']\n",
    "\n",
    "    # Few-shotã®æ•°ã‚’å–å¾—\n",
    "    num_fewshot = task_meta['num_fewshot']\n",
    "\n",
    "    # \n",
    "    continuation_delimiter = task_meta['continuation_delimiter']\n",
    "\n",
    "    # ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«ä»¥å¤–ã®few-shotã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "    fewshot_examples = []\n",
    "    if num_fewshot > 0:\n",
    "        rng = random.Random(1234 + idx)\n",
    "        available_indices = [i for i in range(len(data)) if i != idx]\n",
    "        fewshot_indices = rng.sample(available_indices, num_fewshot)\n",
    "        fewshot_examples = [data[i] for i in fewshot_indices]\n",
    "\n",
    "    # 2) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "\n",
    "    # å¤šè‚¢é¸æŠã®å ´åˆ\n",
    "    if task_type == 'multiple_choice':\n",
    "\n",
    "        # å¤šè‚¢é¸æŠã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "        prompts = render_prompts_mc(item, continuation_delimiter, fewshot_examples)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æå¤±è¨ˆç®—å¯¾è±¡ã®ç¯„å›²ã‚’å–å¾—\n",
    "        tokens, start_idxs, end_idxs = batch_sequences_mc(tokenizer, prompts)\n",
    "\n",
    "    # ã‚¹ã‚­ãƒ¼ãƒã®å ´åˆ \n",
    "    elif task_type == 'schema':\n",
    "\n",
    "        # ã‚¹ã‚­ãƒ¼ãƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "        prompts = render_prompts_schema(item, continuation_delimiter, fewshot_examples)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æå¤±è¨ˆç®—å¯¾è±¡ã®ç¯„å›²ã‚’å–å¾—\n",
    "        tokens, start_idxs, end_idxs = batch_sequences_schema(tokenizer, prompts)\n",
    "\n",
    "    # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å ´åˆ\n",
    "    elif task_type == 'language_modeling':\n",
    "\n",
    "        # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°\n",
    "        prompts = render_prompts_lm(item, continuation_delimiter, fewshot_examples)\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨æå¤±è¨ˆç®—å¯¾è±¡ã®ç¯„å›²ã‚’å–å¾—\n",
    "        tokens, start_idxs, end_idxs = batch_sequences_lm(tokenizer, prompts)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "    # 3) ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆãƒãƒƒãƒåŒ–ï¼‰\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã«æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®å±æ€§ãŒã‚ã‚‹å ´åˆ\n",
    "    if hasattr(model, 'max_seq_len') and model.max_seq_len is not None:\n",
    "\n",
    "        max_tokens = model.max_seq_len\n",
    "        new_tokens, new_start_idxs, new_end_idxs = [], [], []\n",
    "\n",
    "        # å„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç¢ºèªã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°\n",
    "        for t, s, e in zip(tokens, start_idxs, end_idxs):\n",
    "\n",
    "            # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¶…ãˆã‚‹å ´åˆ\n",
    "            if len(t) > max_tokens:\n",
    "\n",
    "                # åˆ‡ã‚Šè©°ã‚ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "                num_to_crop = len(t) - max_tokens\n",
    "\n",
    "                # æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¨ã¦ã¦åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "                new_tokens.append(t[-max_tokens:])\n",
    "\n",
    "                # æå¤±è¨ˆç®—å¯¾è±¡ã®ç¯„å›²ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª¿æ•´\n",
    "                new_start_idxs.append(s - num_to_crop)\n",
    "                new_end_idxs.append(e - num_to_crop)\n",
    "                assert s - num_to_crop >= 0, \"this should never happen right?\"\n",
    "                assert e - num_to_crop >= 0, \"this should never happen right?\"\n",
    "\n",
    "            # æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’è¶…ãˆãªã„å ´åˆ\n",
    "            else:\n",
    "                # å¤‰æ›´ãªã—\n",
    "                new_tokens.append(t)\n",
    "                new_start_idxs.append(s)\n",
    "                new_end_idxs.append(e)\n",
    "\n",
    "        # ä¸Šæ›¸ã\n",
    "        tokens, start_idxs, end_idxs = new_tokens, new_start_idxs, new_end_idxs\n",
    "\n",
    "    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "    pad_token_id = tokenizer.get_bos_token_id()\n",
    "\n",
    "    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã§é•·ã•ã‚’æƒãˆã¦ãƒãƒƒãƒã«ã¾ã¨ã‚ã‚‹\n",
    "    input_ids = stack_sequences(tokens, pad_token_id)\n",
    "\n",
    "    # ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    # 4) ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã®è‡ªå·±å›å¸°çš„æå¤±ã¨argmaxäºˆæ¸¬ã‚’å–å¾—\n",
    "    losses, predictions = forward_model(model, input_ids)\n",
    "\n",
    "    # 5) æ­£è§£ã®åˆ¤å®š\n",
    "\n",
    "    # è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®å ´åˆ\n",
    "    if task_type == 'language_modeling':\n",
    "\n",
    "        # æå¤±å¯¾è±¡ç¯„å›²ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "        si = start_idxs[0]\n",
    "        ei = end_idxs[0]\n",
    "\n",
    "        # äºˆæ¸¬ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "        predicted_tokens = predictions[0, si-1:ei-1]\n",
    "\n",
    "        # æ­£è§£ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "        actual_tokens = input_ids[0, si:ei]\n",
    "\n",
    "        # äºˆæ¸¬ãŒæ­£è§£ã¨å®Œå…¨ã«ä¸€è‡´ã™ã‚‹ã‹ã‚’åˆ¤å®š\n",
    "        is_correct = torch.all(predicted_tokens == actual_tokens).item()\n",
    "\n",
    "    # å¤šè‚¢é¸æŠãƒ»ã‚¹ã‚­ãƒ¼ãƒã®å ´åˆ\n",
    "    elif task_type in ['multiple_choice', 'schema']:\n",
    "\n",
    "        # å„é¸æŠè‚¢ã®å¹³å‡æå¤±ã‚’è¨ˆç®—\n",
    "        mean_losses = [losses[i, si-1:ei-1].mean().item()\n",
    "                        for i, (si, ei) in enumerate(zip(start_idxs, end_idxs))]\n",
    "\n",
    "        # æœ€ã‚‚æå¤±ãŒä½ã„é¸æŠè‚¢ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒå°¤ã‚‚ã‚‰ã—ã„ã¨åˆ¤æ–­ã—ãŸé¸æŠè‚¢ï¼‰\n",
    "        pred_idx = mean_losses.index(min(mean_losses))\n",
    "\n",
    "        # äºˆæ¸¬ãŒæ­£è§£ã¨ä¸€è‡´ã™ã‚‹ã‹ã‚’åˆ¤å®š\n",
    "        is_correct = pred_idx == item['gold']\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported task type: {task_type}\")\n",
    "\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b0a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(model, tokenizer, data, device, task_meta):\n",
    "    \"\"\"\n",
    "    1ã¤ã®ã‚¿ã‚¹ã‚¯ã®å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€æœ€çµ‚çš„ãªæ­£è§£ç‡ã‚’è¨ˆç®—ã™ã‚‹\n",
    "    åˆ†æ•£å‡¦ç†ã«ã‚‚å¯¾å¿œ\n",
    "\n",
    "    Args:\n",
    "        model: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "        data: ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "        device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "        task_meta: ã‚¿ã‚¹ã‚¯ã®ãƒ¡ã‚¿æƒ…å ±\n",
    "    Returns:\n",
    "        float: ã‚¿ã‚¹ã‚¯ã®æ­£è§£ç‡\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) åˆ†æ•£ç’°å¢ƒã®åˆæœŸåŒ–\n",
    "\n",
    "    # åˆ†æ•£ç’°å¢ƒã®ãƒ©ãƒ³ã‚¯ï¼ˆãƒ—ãƒ­ã‚»ã‚¹IDï¼‰ã‚’å–å¾—\n",
    "    rank = dist.get_rank() if dist.is_initialized() else 0\n",
    "\n",
    "    # åˆ†æ•£ç’°å¢ƒã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰ã‚’å–å¾—\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "\n",
    "    # çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’0ã§åˆæœŸåŒ–\n",
    "    correct = torch.zeros(len(data), dtype=torch.float32, device=device)\n",
    "\n",
    "    # 2) ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡\n",
    "\n",
    "    # å„ãƒ—ãƒ­ã‚»ã‚¹ãŒæ‹…å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’é †ç•ªã«å‡¦ç†\n",
    "    # world_size=8ã®å ´åˆ\n",
    "    # rank 0 -> idx = 0, 8, 16, ...\n",
    "    # rank 1 -> idx = 1, 9, 17, ...\n",
    "    # rank 7 -> idx = 7, 15, 23, ...\n",
    "    for idx in range(rank, len(data), world_size):\n",
    "\n",
    "        # 1ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡\n",
    "        is_correct = evaluate_example(idx, model, tokenizer, data, device, task_meta)\n",
    "\n",
    "        # å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«çµæœã‚’æ ¼ç´\n",
    "        correct[idx] = float(is_correct)\n",
    "\n",
    "    # 3) çµæœã®é›†ç´„\n",
    "\n",
    "    if world_size > 1:\n",
    "\n",
    "        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "        dist.barrier()\n",
    "\n",
    "        # å…¨ãƒ—ãƒ­ã‚»ã‚¹ã®çµæœã‚’é›†ç´„ï¼ˆåˆè¨ˆã‚’è¨ˆç®—ï¼‰\n",
    "        dist.all_reduce(correct, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    # æ­£è§£ç‡ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "    mean_correct = correct.mean().item()\n",
    "\n",
    "    return mean_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693dbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, device, max_per_task=-1):\n",
    "    \"\"\"\n",
    "    COREãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    HellaSwagã€ ARC-Easyãªã©ã®å…¨22ã‚¿ã‚¹ã‚¯ã‚’é †ç•ªã«å®Ÿè¡Œã—ã€æ­£è§£ç‡ã‚’è¨ˆç®—ã—ã€ç·åˆã‚¹ã‚³ã‚¢ã«é›†ç´„ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model: è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "        tokenizer: ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "        device: ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹\n",
    "        max_per_task (int): å„ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®æœ€å¤§ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ-1ã§ç„¡åˆ¶é™ï¼‰\n",
    "    Returns:\n",
    "        dict: å„ã‚¿ã‚¹ã‚¯ã®æ­£è§£ç‡ã€ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ­£è§£ç‡ã€ãŠã‚ˆã³COREãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’å«ã‚€è¾æ›¸\n",
    "            {\n",
    "                \"results\": {task_label: accuracy, ...},\n",
    "                \"centered_results\": {task_label: centered_accuracy, ...},\n",
    "                \"core_metric\": core_metric\n",
    "            }\n",
    "    \"\"\"\n",
    "    logger.debug(f\"COREãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ {max_per_task=}\")\n",
    "\n",
    "    # 1) è¨­å®šã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "    base_dir = get_base_dir()\n",
    "    eval_bundle_dir = os.path.join(base_dir, \"eval_bundle\")\n",
    "    data_base_path = os.path.join(eval_bundle_dir, \"eval_data\")\n",
    "\n",
    "    # å®Ÿè¡Œã™ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã®ãƒ‘ã‚¹\n",
    "    config_path = os.path.join(eval_bundle_dir, \"core.yaml\")\n",
    "\n",
    "    # å„ã‚¿ã‚¹ã‚¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    eval_meta_data = os.path.join(eval_bundle_dir, \"eval_meta_data.csv\")\n",
    "\n",
    "    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    # ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    tasks = config['icl_tasks']\n",
    "\n",
    "    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "    eval_metadata = pd.read_csv(eval_meta_data)\n",
    "\n",
    "    # 2) ã‚¿ã‚¹ã‚¯ã”ã¨ã«è©•ä¾¡ã‚’å®Ÿè¡Œ\n",
    "\n",
    "    # ç”Ÿã®æ­£è§£ç‡ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸\n",
    "    results = {}\n",
    "\n",
    "    # è£œæ­£æ¸ˆã¿ã®æ­£è§£ç‡ã‚’ä¿å­˜ã™ã‚‹è¾æ›¸\n",
    "    centered_results = {}\n",
    "\n",
    "    # å„ã‚¿ã‚¹ã‚¯ã‚’é †ç•ªã«è©•ä¾¡\n",
    "    for task in tasks:\n",
    "\n",
    "        start_time = time.time()\n",
    "        label = task['label']\n",
    "        task_meta = {\n",
    "            'task_type': task['icl_task_type'],\n",
    "            'dataset_uri': task['dataset_uri'],\n",
    "            'num_fewshot': task['num_fewshot'][0],\n",
    "            'continuation_delimiter': task.get('continuation_delimiter', ' ')\n",
    "        }\n",
    "        logger.info(f\"Evaluating: {label} ({task_meta['num_fewshot']}-shot, type: {task_meta['task_type']})... \")\n",
    "\n",
    "        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "        data_path = os.path.join(data_base_path, task_meta['dataset_uri'])\n",
    "        with open(data_path, 'r') as f:\n",
    "            data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "        # ä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "        shuffle_rng = random.Random(1337)\n",
    "        shuffle_rng.shuffle(data)\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚’å…ˆé ­ã‹ã‚‰æŠ½å‡º\n",
    "        if max_per_task > 0:\n",
    "            data = data[:max_per_task]\n",
    "\n",
    "        # ã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã€æ­£è§£ç‡ã‚’å–å¾—\n",
    "        accuracy = evaluate_task(model, tokenizer, data, device, task_meta)\n",
    "\n",
    "        # ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹æ­£è§£ç‡ã‚’ä¿å­˜\n",
    "        results[label] = accuracy\n",
    "\n",
    "        # è£œæ­£æ¸ˆã¿ã®æ­£è§£ç‡ã‚’è¨ˆç®—\n",
    "        row = eval_metadata[eval_metadata[\"Eval Task\"] == label]\n",
    "        random_baseline = row[\"Random baseline\"].values[0]\n",
    "        centered_result = (accuracy - 0.01 * random_baseline) / (1.0 - 0.01 * random_baseline)\n",
    "\n",
    "        # ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹è£œæ­£æ¸ˆã¿ã®æ­£è§£ç‡ã‚’ä¿å­˜\n",
    "        centered_results[label] = centered_result\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        logger.info(f\"accuracy: {accuracy:.4f} | centered: {centered_result:.4f} | time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "    # 3) çµæœã‚’é›†è¨ˆ\n",
    "\n",
    "    # ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®æ­£è§£ç‡ã‚’å¹³å‡åŒ–\n",
    "    core_metric = sum(centered_results.values()) / len(centered_results)\n",
    "\n",
    "    # çµæœã‚’æ•´å½¢\n",
    "    out = {\n",
    "        \"results\": results,\n",
    "        \"centered_results\": centered_results,\n",
    "        \"core_metric\": core_metric\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3043dd2",
   "metadata": {},
   "source": [
    "### è¨“ç·´ã®å®Ÿè¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ãƒ«ãƒ¼ãƒ—\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# BPBã®è©•ä¾¡çµæœã‚’åˆæœŸåŒ–\n",
    "min_val_bpb = float(\"inf\")\n",
    "\n",
    "# è¨“ç·´æå¤±ã®æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆEMAï¼‰ã‚’åˆæœŸåŒ–\n",
    "smooth_train_loss = 0\n",
    "\n",
    "# EMAã®æ¸›è¡°ä¿‚æ•°\n",
    "ema_beta = 0.9\n",
    "\n",
    "# ç·è¨“ç·´æ™‚é–“ï¼ˆwall-clock timeï¼‰ã‚’åˆæœŸåŒ–\n",
    "total_training_time = 0\n",
    "\n",
    "# è¨“ç·´ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹ï¼ˆ+1ã¯ã€æœ€å¾Œã«è©•ä¾¡ã¨ä¿å­˜ã‚’è¡Œã†ãŸã‚ï¼‰\n",
    "for step in range(num_iterations + 1):\n",
    "\n",
    "    # 1) è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚º\n",
    "\n",
    "    # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã®åˆ¤å®š\n",
    "    last_step = step == num_iterations\n",
    "\n",
    "    # ã“ã‚Œã¾ã§ã®FLOPsã‚’è¨ˆç®—\n",
    "    flops_so_far = num_flops_per_token * total_batch_size * step\n",
    "\n",
    "    # BPBè©•ä¾¡ã‚’è¡Œã†å ´åˆï¼ˆã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ãŒå¯¾è±¡ï¼‰\n",
    "    if last_step or step % eval_every == 0:\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ\n",
    "        val_loader = build_val_loader()\n",
    "\n",
    "        # è©•ä¾¡ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "\n",
    "        # è‡ªå‹•æ··åˆç²¾åº¦ï¼ˆAuto Mixed Precision, AMPï¼‰ã‚’æœ‰åŠ¹åŒ–\n",
    "        with autocast_ctx:\n",
    "            # BPBã‚’è©•ä¾¡\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "\n",
    "        logger.info(f\"ã‚¹ãƒ†ãƒƒãƒ—æ•° {step=:05d} æ¤œè¨¼BPB {val_bpb=:.4f}\")\n",
    "\n",
    "        # BPBãŒæ”¹å–„ã—ãŸå ´åˆ\n",
    "        if val_bpb < min_val_bpb:\n",
    "\n",
    "            # æœ€å°BPBã‚’æ›´æ–°\n",
    "            min_val_bpb = val_bpb\n",
    "\n",
    "        # WandBã«ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "\n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™\n",
    "        model.train()\n",
    "\n",
    "    # COREãƒ¡ãƒˆãƒªãƒƒã‚¯ã®è©•ä¾¡çµæœã‚’åˆæœŸåŒ–\n",
    "    results = {}\n",
    "\n",
    "    # COREãƒ¡ãƒˆãƒªãƒƒã‚¯è©•ä¾¡ã‚’è¡Œã†å ´åˆï¼ˆã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ãŒå¯¾è±¡ï¼‰\n",
    "    if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "\n",
    "        with autocast_ctx:\n",
    "            # COREãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è©•ä¾¡\n",
    "            # æ³¨æ„: å…¥åŠ›ã®å½¢çŠ¶ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
    "\n",
    "        logger.info(f\"ã‚¹ãƒ†ãƒƒãƒ—æ•° {step=:05d} | COREãƒ¡ãƒˆãƒªãƒƒã‚¯ {results['core_metric']:.4f}\")\n",
    "\n",
    "        # WandBã«ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"core_metric\": results[\"core_metric\"],\n",
    "            \"centered_results\": results[\"centered_results\"],\n",
    "        })\n",
    "\n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™\n",
    "        model.train()\n",
    "\n",
    "    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã†å ´åˆï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ï¼‰\n",
    "    if master_process and (last_step or (step > 0 and step % sample_every == 0)):\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        model.eval()\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "\n",
    "        # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–\n",
    "        # æ³¨æ„: å…¥åŠ›ã®å½¢çŠ¶ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨\n",
    "        engine = Engine(orig_model, tokenizer)\n",
    "\n",
    "        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ\n",
    "        for prompt in prompts:\n",
    "\n",
    "            # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "\n",
    "            # è‡ªå‹•æ··åˆç²¾åº¦ã‚’æœ‰åŠ¹åŒ–\n",
    "            with autocast_ctx:\n",
    "\n",
    "                # ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ\n",
    "                sample, _ = engine.generate_batch(\n",
    "                    tokens,\n",
    "                    num_samples=1,\n",
    "                    max_tokens=16, # 16ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ\n",
    "                    temperature=0 # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.decode(sample[0])\n",
    "            logger.info(f\"ã‚¹ãƒ†ãƒƒãƒ—æ•° {step=:05d} ã‚µãƒ³ãƒ—ãƒ« {decoded=}\")\n",
    "            \n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™\n",
    "        model.train()\n",
    "\n",
    "    # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ï¼‰\n",
    "    if master_process and last_step:\n",
    "\n",
    "        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’æ±ºå®š\n",
    "        output_dirname = model_tag if model_tag else f\"d{depth}\"\n",
    "\n",
    "        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "        checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n",
    "\n",
    "        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(),\n",
    "            [opt.state_dict() for opt in optimizers],\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # BPBè©•ä¾¡çµæœ\n",
    "                \"model_config\": model_config_kwargs,\n",
    "                \"user_config\": user_config, # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š\n",
    "                \"device_batch_size\": device_batch_size,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆ\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # 2) è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º\n",
    "\n",
    "    # GPUã®æ“ä½œã‚’åŒæœŸ\n",
    "    synchronize()\n",
    "\n",
    "    # è¨ˆç®—æ™‚é–“ã®è¨ˆæ¸¬é–‹å§‹\n",
    "    t0 = time.time()\n",
    "\n",
    "    # å‹¾é…ã‚’è“„ç©ï¼ˆgradient accumulationï¼‰\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "\n",
    "        # è‡ªå‹•æ··åˆç²¾åº¦ã‚’æœ‰åŠ¹åŒ–ï¼ˆAuto Mixed Precision, AMPï¼‰\n",
    "        with autocast_ctx:\n",
    "\n",
    "            # é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã€æå¤±ã‚’è¨ˆç®—\n",
    "            loss = model(x, y)\n",
    "\n",
    "        # ãƒ­ã‚°ç”¨ã«è¨“ç·´æå¤±ã®å€¤ã‚’å–å¾—\n",
    "        train_loss = loss.detach()\n",
    "\n",
    "        # æå¤±ã‚’è“„ç©å›æ•°ã§å‰²ã£ã¦æ­£è¦åŒ–\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "\n",
    "        # é€†ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "        loss.backward()\n",
    "\n",
    "        # GPUãŒé †ä¼æ’­ãƒ»é€†ä¼æ’­ã‚’å®Ÿè¡Œã—ã¦ã„ã‚‹é–“ã«ã€æ¬¡ã®ãƒãƒƒãƒã‚’ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ\n",
    "        x, y = next(train_loader)\n",
    "\n",
    "    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ãŒæœ‰åŠ¹ãªå ´åˆï¼ˆgradient clippingï¼‰\n",
    "    grad_clip_enabled = grad_clip > 0.0\n",
    "    if grad_clip_enabled:\n",
    "        # grad_clipã‚’è¶…ãˆã¦ã„ã‚Œã°ã€å‹¾é…ã‚’ã‚¯ãƒªãƒƒãƒ—\n",
    "        grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "\n",
    "        # å‹¾é…ãƒãƒ«ãƒ ã‚’CPUã«è»¢é€ã—ã¦floatã«å¤‰æ›\n",
    "        grad_norm = grad_norm_tensor.item()\n",
    "\n",
    "    # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ç‡ã®å€ç‡ã‚’å–å¾—\n",
    "    lrm = get_lr_multiplier(step)\n",
    "\n",
    "    # å…¨ã¦ã®æœ€é©åŒ–é–¢æ•°ã®å­¦ç¿’ç‡ã‚’æ›´æ–°\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "\n",
    "    # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã®Muonã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’å–å¾—\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "\n",
    "    # Muonã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’æ›´æ–°\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "\n",
    "    # å‹¾é…ã‚’ä½¿ã£ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "\n",
    "    # å‹¾é…ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    # å…¨ã¦ã®GPUã®æ“ä½œã‚’åŒæœŸ\n",
    "    synchronize()\n",
    "\n",
    "    # è¨ˆç®—æ™‚é–“ã®è¨ˆæ¸¬çµ‚äº†\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    # 3. ãƒ­ã‚®ãƒ³ã‚°\n",
    "\n",
    "    # è¨“ç·´æå¤±ã‚’æŒ‡æ•°ç§»å‹•å¹³å‡ï¼ˆEMAï¼‰ã§æ›´æ–°\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() \n",
    "\n",
    "    # EMA the training loss\n",
    "\n",
    "    # debias the EMA\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1))\n",
    "\n",
    "    pct_done = 100 * step / num_iterations\n",
    "\n",
    "    # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆãƒˆãƒ¼ã‚¯ãƒ³/ç§’ï¼‰ã‚’è¨ˆç®—\n",
    "    tok_per_sec = int(total_batch_size / dt)\n",
    "\n",
    "    # MFUï¼ˆModel FLOPs Utilizationï¼‰ã‚’è¨ˆç®—\n",
    "    # GPUã®ç†è«–ä¸Šã®æœ€å¤§FLOPsã«å¯¾ã—ã¦ã€å®Ÿéš›ã«é”æˆã—ãŸFLOPsã®å‰²åˆ\n",
    "    flops_per_sec = num_flops_per_token * total_batch_size / dt\n",
    "    # bfloat16 H100 SXM and without 2:4 sparsity\n",
    "    promised_flops_per_sec_h100 = 989e12 * ddp_world_size\n",
    "    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %\n",
    "\n",
    "    # 10ã‚¹ãƒ†ãƒƒãƒ—ç›®ä»¥é™ã®å ´åˆ\n",
    "    if step > 10:\n",
    "        # ç·è¨“ç·´æ™‚é–“ã‚’æ›´æ–°\n",
    "        # æœ€åˆã®10ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãªã©ã§é…ã„ãŸã‚\n",
    "        total_training_time += dt\n",
    "\n",
    "    print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\n",
    "\n",
    "    logger.info(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n",
    "\n",
    "    # 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨\n",
    "    if step % 100 == 0:\n",
    "\n",
    "        # WandBã«ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "        log_data = {\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "        }\n",
    "\n",
    "        if grad_clip_enabled:\n",
    "            log_data[\"train/grad_norm\"] = grad_norm\n",
    "\n",
    "        wandb_run.log(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
