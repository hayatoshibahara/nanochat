{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5c39d5d",
   "metadata": {},
   "source": [
    "# nanochat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5884fe2",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "\n",
    "%pip install maturin\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"nanochat\"):\n",
    "    !git clone https://github.com/karpathy/nanochat\n",
    "\n",
    "try:\n",
    "    # Google Colabã®å ´åˆ\n",
    "    from google.colab import userdata\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§Rustã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && . \"$HOME/.cargo/env\"\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§RustBPEã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # maturin build --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "    # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ç”Ÿæˆã•ã‚ŒãŸwhlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "    # pip install nanochat/rustbpe/target/wheels/*.whl\n",
    "\n",
    "    if not os.path.exists(\"nanochat/rustbpe/target\"):\n",
    "        raise FileNotFoundError(\"rustbpeã®ãƒ“ãƒ«ãƒ‰ã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "except ImportError:\n",
    "    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã®å ´åˆ\n",
    "    !maturin develop --release --manifest-path nanochat/rustbpe/Cargo.toml\n",
    "\n",
    "import rustbpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "import logging as logging\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = 'ğŸŸ¦'\n",
    "        case logging.INFO:\n",
    "            level = 'ğŸŸ©'\n",
    "        case logging.WARNING:\n",
    "            level = 'ğŸŸ¨'\n",
    "        case logging.ERROR:\n",
    "            level = 'ğŸŸ¥'\n",
    "        case logging.CRITICAL:\n",
    "            level = 'ğŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.info(\"ãƒ­ã‚°ã‚’åˆæœŸåŒ–\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "\n",
    "def get_base_dir():\n",
    "    \"\"\"\n",
    "    ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹\n",
    "    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€~/.cache/nanochat\n",
    "    NANOCHAT_BASE_DIRç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½\n",
    "\n",
    "    Returns:\n",
    "        str: ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"NANOCHAT_BASE_DIR\"):\n",
    "        nanochat_dir = os.environ.get(\"NANOCHAT_BASE_DIR\")\n",
    "    else:\n",
    "        home_dir = os.path.expanduser(\"~\")\n",
    "        cache_dir = os.path.join(home_dir, \".cache\")\n",
    "        nanochat_dir = os.path.join(cache_dir, \"nanochat\")\n",
    "    os.makedirs(nanochat_dir, exist_ok=True)\n",
    "    return nanochat_dir\n",
    "\n",
    "logger.info(f\"ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {get_base_dir()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa00863e",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c3e368",
   "metadata": {},
   "source": [
    "äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã¨ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "\n",
    "[karpathy/fineweb-edu-100b-shuffle][2]ã‚’ä½¿ç”¨:\n",
    "\n",
    "- [FineWeb-EDUãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ][1]ã‚’ã€ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã‚·ãƒ£ãƒ¼ãƒ‰ï¼ˆæ–­ç‰‡ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã«å†ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸\n",
    "- å„ã‚·ãƒ£ãƒ¼ãƒ‰ã¯25ä¸‡æ–‡å­—ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã§ã€ç´„100MB\n",
    "- åˆè¨ˆã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯1822\n",
    "- depth=20ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚·ãƒ£ãƒ¼ãƒ‰æ•°ã¯240ã§ã€ç´„24GB\n",
    "\n",
    "[1]: https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu\n",
    "[2]: https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c1aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4501892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®URL\n",
    "BASE_URL = \"https://huggingface.co/datasets/karpathy/fineweb-edu-100b-shuffle/resolve/main\"\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆURL: {BASE_URL}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¤§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "# fineweb-edu-100b-shuffleã®å ´åˆã€1823å€‹ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãŒã‚ã‚Šã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯0ã‹ã‚‰1822ã¾ã§\n",
    "MAX_SHARD = 1822\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€å¤§ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {MAX_SHARD}\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®æ•°\n",
    "num_files = 8\n",
    "logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰æ•°: {num_files}\")\n",
    "\n",
    "# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "num_workers = 4\n",
    "logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
    "base_dir = get_base_dir()\n",
    "DATA_DIR = os.path.join(base_dir, \"base_data\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2749ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_filename(index):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ\n",
    "\n",
    "    Args:\n",
    "        index (int): ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    Returns:\n",
    "        str: ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "    \"\"\"\n",
    "    return f\"shard_{index:05d}.parquet\"\n",
    "\n",
    "index_to_filename(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_single_file(index):\n",
    "    \"\"\"\n",
    "    å˜ä¸€ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        index (int): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    Returns:\n",
    "        bool: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹\n",
    "    \"\"\"\n",
    "    logger.info(f\"ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ {index=}\")\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    filename = index_to_filename(index)\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    logger.debug(f\"ä¿å­˜å…ˆ {filepath=}\")\n",
    "\n",
    "    # æ—¢ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "    if os.path.exists(filepath):\n",
    "        logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— {index=}\")\n",
    "        return True\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã‚’ä½œæˆ\n",
    "    url = f\"{BASE_URL}/{filename}\"\n",
    "    logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL {url=}\")\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å›ã®ãƒªãƒˆãƒ©ã‚¤ï¼‰\n",
    "    max_attempts = 5\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒ¢ãƒ¼ãƒ‰ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "            logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ {attempt}/{max_attempts} {index=}\")\n",
    "            response = requests.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            temp_path = filepath + f\".tmp\"\n",
    "            logger.debug(f\"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ {temp_path=}\")\n",
    "            with open(temp_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "\n",
    "            os.rename(temp_path, filepath)\n",
    "            logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† {index=}\")\n",
    "            return True\n",
    "\n",
    "        except (requests.RequestException, IOError) as e:\n",
    "            logger.warning(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ {e} {index=} {attempt=}\")\n",
    "\n",
    "            # ä¸å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤\n",
    "            for path in [filepath + f\".tmp\", filepath]:\n",
    "                if os.path.exists(path):\n",
    "                    try:\n",
    "                        os.remove(path)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            # ãƒãƒƒã‚¯ã‚ªãƒ•\n",
    "            # 2^0, 2^1, 2^2, ...ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤\n",
    "            if attempt < max_attempts:\n",
    "                wait_time = 2 ** attempt\n",
    "                logger.info(f\"ãƒãƒƒã‚¯ã‚ªãƒ•: {wait_time} ç§’å¾…æ©Ÿä¸­\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                logger.info(f\"ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {index} ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—\")\n",
    "                return False\n",
    "\n",
    "    return False\n",
    "\n",
    "download_single_file(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "# ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "num = MAX_SHARD + 1 if num_files == -1 else min(num_files, MAX_SHARD + 1)\n",
    "ids_to_download = list(range(num))\n",
    "logger.info(f\"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚·ãƒ£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {ids_to_download=}\")\n",
    "\n",
    "# ä¸¦åˆ—å‡¦ç†ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\n",
    "with Pool(processes=num_workers) as pool:\n",
    "    results = pool.map(download_single_file, ids_to_download)\n",
    "\n",
    "# çµæœã‚’è¡¨ç¤º\n",
    "successful = sum(1 for success in results if success)\n",
    "logger.info(f\"ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† {successful/len(ids_to_download)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fcf16b",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d486f",
   "metadata": {},
   "source": [
    "RustBPETokenizerã¯ã€rustbpeã‚’ãƒ©ãƒƒãƒ—ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import rustbpe\n",
    "import tiktoken\n",
    "import os\n",
    "import copy\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\n",
    "    # äº‹å‰å­¦ç¿’ã§ä½¿ç”¨\n",
    "    \"<|bos|>\", # æ–‡ã®é–‹å§‹\n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ä½¿ç”¨\n",
    "    \"<|user_start|>\", # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "    \"<|user_end|>\",\n",
    "    \"<|assistant_start|>\", # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "    \"<|assistant_end|>\",\n",
    "    \"<|python_start|>\", # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒPython REPLãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™\n",
    "    \"<|python_end|>\",\n",
    "    \"<|output_start|>\", # Python REPLãŒã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å‡ºåŠ›ã‚’è¿”ã™\n",
    "    \"<|output_end|>\",\n",
    "]\n",
    "\n",
    "logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³: {SPECIAL_TOKENS=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fda07",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,2}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "logger.info(f\"äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ­£è¦è¡¨ç¾: {SPLIT_PATTERN=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RustBPETokenizer:\n",
    "    \"è¨“ç·´æ™‚ã¯rustbpeã‚’ä½¿ã„ã€æ¨è«–æ™‚ã¯tiktokenã‚’ä½¿ã†ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹\"\n",
    "\n",
    "    def __init__(self, enc, bos_token):\n",
    "        logger.info(f\"RustBPETokenizeråˆæœŸåŒ–é–‹å§‹ {enc=} {bos_token=}\")\n",
    "\n",
    "        # tiktokenã®Encodingã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        self.enc = enc \n",
    "\n",
    "        # BOSãƒˆãƒ¼ã‚¯ãƒ³IDã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        self.bos_token_id = self.encode_special(bos_token)\n",
    "        logger.info(f\"RustBPETokenizeråˆæœŸåŒ–å®Œäº† {self.bos_token_id=}\")\n",
    "\n",
    "    @classmethod\n",
    "    def train_from_iterator(cls, text_iterator, vocab_size):\n",
    "        logger.info(f\"RustBPETokenizerã®è¨“ç·´é–‹å§‹ {vocab_size=}\")\n",
    "\n",
    "        # 1) rustbpeã‚’è¨“ç·´\n",
    "\n",
    "        # rustbpeã®Tokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "        tokenizer = rustbpe.Tokenizer()\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯å¾Œã§__init__ã§æŒ¿å…¥ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã§ã¯è¨“ç·´ã—ãªã„\n",
    "        vocab_size_no_special = vocab_size - len(SPECIAL_TOKENS)\n",
    "\n",
    "        assert vocab_size_no_special >= 256, f\"vocab_size_no_special must be at least 256, got {vocab_size_no_special}\"\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "        tokenizer.train_from_iterator(text_iterator, vocab_size_no_special, pattern=SPLIT_PATTERN)\n",
    "\n",
    "        # 2) tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰\n",
    "\n",
    "        # äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        pattern = tokenizer.get_pattern()\n",
    "\n",
    "        # ãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«\n",
    "        # {ãƒã‚¤ãƒˆåˆ—: ãƒãƒ¼ã‚¸ã®å„ªå…ˆé †ä½ãƒ©ãƒ³ã‚¯}\n",
    "        mergeable_ranks_list = tokenizer.get_mergeable_ranks()\n",
    "        mergeable_ranks = {bytes(k): v for k, v in mergeable_ranks_list}\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«ã®å¾Œã«è¿½åŠ \n",
    "        # {ãƒˆãƒ¼ã‚¯ãƒ³å: ãƒˆãƒ¼ã‚¯ãƒ³ID}\n",
    "        tokens_offset = len(mergeable_ranks)\n",
    "        special_tokens = {name: tokens_offset + i for i, name in enumerate(SPECIAL_TOKENS)}\n",
    "\n",
    "        # tiktokenã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ§‹ç¯‰\n",
    "        enc = tiktoken.Encoding(\n",
    "            name=\"rustbpe\",\n",
    "            pat_str=pattern,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=special_tokens,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"RustBPETokenizerã®è¨“ç·´å®Œäº†\")\n",
    "\n",
    "        # RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_directory(cls, tokenizer_dir):\n",
    "        logger.info(f\"RustBPETokenizerã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿é–‹å§‹ {tokenizer_dir=}\")\n",
    "\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {pickle_path=}\")\n",
    "\n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            enc = pickle.load(f)\n",
    "\n",
    "        logger.info(f\"RustBPETokenizerã‚’ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰èª­ã¿è¾¼ã¿å®Œäº† {enc=}\")\n",
    "        return cls(enc, \"<|bos|>\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, tiktoken_name):\n",
    "        logger.info(f\"å­¦ç¿’æ¸ˆã¿ã®tiktokenãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {tiktoken_name=}\")\n",
    "\n",
    "        # https://github.com/openai/tiktoken/blob/eedc8563/tiktoken_ext/openai_public.py\n",
    "        enc = tiktoken.get_encoding(tiktoken_name)\n",
    "\n",
    "        # nanochatã§ã¯<|bos|>ã‚’ä½¿ç”¨ã™ã‚‹ãŒã€tiktokenã®gpt2ãªã©ã§ã¯<|endoftext|>ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãŸã‚\n",
    "\n",
    "        logger.info(f\"å­¦ç¿’æ¸ˆã¿ã®tiktokenãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿å®Œäº† {enc=}\")\n",
    "        return cls(enc, \"<|endoftext|>\")\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.enc.n_vocab\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        return self.enc.special_tokens_set\n",
    "\n",
    "    def id_to_token(self, id):\n",
    "        return self.enc.decode([id])\n",
    "\n",
    "    @lru_cache(maxsize=32)\n",
    "    def encode_special(self, text):\n",
    "        return self.enc.encode_single_token(text)\n",
    "\n",
    "    def get_bos_token_id(self):\n",
    "        return self.bos_token_id\n",
    "\n",
    "    def encode(self, text, prepend=None, append=None, num_threads=8):\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é–‹å§‹ {text=} {prepend=} {append=} {num_threads=}\")\n",
    "\n",
    "        # æ¥é ­è¾ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "        if prepend is not None:\n",
    "            prepend_id = prepend if isinstance(prepend, int) else self.encode_special(prepend)\n",
    "            logger.debug(f\"{prepend_id=}\")\n",
    "\n",
    "        # æ¥å°¾è¾ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³IDã‚’å–å¾—\n",
    "        if append is not None:\n",
    "            append_id = append if isinstance(append, int) else self.encode_special(append)\n",
    "            logger.debug(f\"{append_id=}\")\n",
    "\n",
    "        # å…¥åŠ›ãŒæ–‡å­—åˆ—ã®å ´åˆ\n",
    "        if isinstance(text, str):\n",
    "\n",
    "            # tiktokenã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            ids = self.enc.encode_ordinary(text)\n",
    "\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "            if prepend is not None:\n",
    "                ids.insert(0, prepend_id) # TODO: slightly inefficient here? :( hmm\n",
    "            if append is not None:\n",
    "                ids.append(append_id)\n",
    "\n",
    "        # å…¥åŠ›ãŒæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "        elif isinstance(text, list):\n",
    "\n",
    "            # tiktokenã§ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            ids = self.enc.encode_ordinary_batch(text, num_threads=num_threads)\n",
    "\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "            if prepend is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.insert(0, prepend_id) # TODO: same\n",
    "            if append is not None:\n",
    "                for ids_row in ids:\n",
    "                    ids_row.append(append_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid input type: {type(text)}\")\n",
    "\n",
    "        logger.info(f\"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å®Œäº† {ids=}\")\n",
    "        return ids\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.encode(*args, **kwargs)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        logger.info(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰é–‹å§‹ {ids=}\")\n",
    "\n",
    "        # tiktokenã§ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "        res = self.enc.decode(ids)\n",
    "\n",
    "        logger.info(f\"ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº† {res=}\")\n",
    "        return res\n",
    "\n",
    "    def save(self, tokenizer_dir):\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜é–‹å§‹ {tokenizer_dir=}\")\n",
    "\n",
    "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "        pickle_path = os.path.join(tokenizer_dir, \"tokenizer.pkl\")\n",
    "        logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {pickle_path=}\")\n",
    "\n",
    "        with open(pickle_path, \"wb\") as f:\n",
    "            pickle.dump(self.enc, f)\n",
    "\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜å®Œäº† {pickle_path=}\")\n",
    "\n",
    "    def render_conversation(self, conversation, max_tokens=2048):\n",
    "        \"\"\"\n",
    "        ãƒãƒ£ãƒƒãƒˆå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            conversation: dict ãƒãƒ£ãƒƒãƒˆä¼šè©±ãƒ‡ãƒ¼ã‚¿\n",
    "            max_tokens: int æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "        Returns:\n",
    "            ids: list[int] ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒªã‚¹ãƒˆ\n",
    "            mask: list[int] åŒã˜é•·ã•ã®ãƒã‚¹ã‚¯ãƒªã‚¹ãƒˆã€mask=1ã¯ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãŒå­¦ç¿’ã™ã¹ããƒˆãƒ¼ã‚¯ãƒ³\n",
    "        \"\"\"\n",
    "        logger.info(f\"ä¼šè©±ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°é–‹å§‹ {conversation=} {max_tokens=}\")\n",
    "\n",
    "        # ids, masks that we will return and a helper function to help build them up.\n",
    "        ids, mask = [], []\n",
    "\n",
    "        def add_tokens(token_ids, mask_val):\n",
    "            if isinstance(token_ids, int):\n",
    "                token_ids = [token_ids]\n",
    "            ids.extend(token_ids)\n",
    "            mask.extend([mask_val] * len(token_ids))\n",
    "\n",
    "        # sometimes the first message is a system message...\n",
    "        # => just merge it with the second (user) message\n",
    "        if conversation[\"messages\"][0][\"role\"] == \"system\":\n",
    "            # some conversation surgery is necessary here for now...\n",
    "            conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "            messages = conversation[\"messages\"]\n",
    "            assert messages[1][\"role\"] == \"user\", \"System message must be followed by a user message\"\n",
    "            messages[1][\"content\"] = messages[0][\"content\"] + \"\\n\\n\" + messages[1][\"content\"]\n",
    "            messages = messages[1:]\n",
    "        else:\n",
    "            messages = conversation[\"messages\"]\n",
    "\n",
    "        assert len(messages) >= 1, f\"Conversation has less than 1 message: {messages}\"\n",
    "\n",
    "        # fetch all the special tokens we need\n",
    "        bos = self.get_bos_token_id()\n",
    "        user_start, user_end = self.encode_special(\"<|user_start|>\"), self.encode_special(\"<|user_end|>\")\n",
    "        assistant_start, assistant_end = self.encode_special(\"<|assistant_start|>\"), self.encode_special(\"<|assistant_end|>\")\n",
    "        python_start, python_end = self.encode_special(\"<|python_start|>\"), self.encode_special(\"<|python_end|>\")\n",
    "        output_start, output_end = self.encode_special(\"<|output_start|>\"), self.encode_special(\"<|output_end|>\")\n",
    "\n",
    "        # é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "        add_tokens(bos, 0)\n",
    "\n",
    "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é †ã«å‡¦ç†\n",
    "        for i, message in enumerate(messages):\n",
    "\n",
    "            # some sanity checking here around assumptions, to prevent footguns\n",
    "            must_be_from = \"user\" if i % 2 == 0 else \"assistant\"\n",
    "            assert message[\"role\"] == must_be_from, f\"Message {i} is from {message['role']} but should be from {must_be_from}\"\n",
    "\n",
    "            # content can be either a simple string or a list of parts (e.g. containing tool calls)\n",
    "            content = message[\"content\"]\n",
    "\n",
    "            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ\n",
    "            if message[\"role\"] == \"user\":\n",
    "                assert isinstance(content, str), \"User messages are simply expected to be strings\"\n",
    "                value_ids = self.encode(content)\n",
    "\n",
    "                # æå¤±è¨ˆç®—ã«å«ã‚ãªã„\n",
    "                add_tokens(user_start, 0)\n",
    "                add_tokens(value_ids, 0)\n",
    "                add_tokens(user_end, 0)\n",
    "\n",
    "            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å ´åˆ\n",
    "            elif message[\"role\"] == \"assistant\":\n",
    "\n",
    "                # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                add_tokens(assistant_start, 0)\n",
    "\n",
    "                if isinstance(content, str):\n",
    "                    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                    value_ids = self.encode(content)\n",
    "\n",
    "                    # æå¤±è¨ˆç®—ã«å«ã‚ã‚‹\n",
    "                    add_tokens(value_ids, 1)\n",
    "\n",
    "                elif isinstance(content, list):\n",
    "                    for part in content:\n",
    "                        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "                        value_ids = self.encode(part[\"text\"])\n",
    "\n",
    "                        # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã®å ´åˆ\n",
    "                        if part[\"type\"] == \"text\":\n",
    "\n",
    "                            # æå¤±è¨ˆç®—ã«å«ã‚ã‚‹\n",
    "                            add_tokens(value_ids, 1)\n",
    "\n",
    "                        # Pythonã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®å ´åˆ\n",
    "                        elif part[\"type\"] == \"python\":\n",
    "                            # ãƒ„ãƒ¼ãƒ«ã®é–‹å§‹ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                            add_tokens(python_start, 1)\n",
    "                            add_tokens(value_ids, 1)\n",
    "                            add_tokens(python_end, 1)\n",
    "\n",
    "                        # Pythonã®å‡ºåŠ›ã®å ´åˆ\n",
    "                        elif part[\"type\"] == \"python_output\":\n",
    "                            # ãƒ„ãƒ¼ãƒ«ã®å‡ºåŠ›ã®é–‹å§‹ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "                            # æ¨è«–æ™‚ã¯Pythonã®å‡ºåŠ›ã‚’ç”¨ã„ã‚‹ãŸã‚æå¤±è¨ˆç®—ã«å«ã‚ãªã„\n",
    "                            add_tokens(output_start, 0)\n",
    "                            add_tokens(value_ids, 0)\n",
    "                            add_tokens(output_end, 0)\n",
    "                        else:\n",
    "                            raise ValueError(f\"Unknown part type: {part['type']}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown content type: {type(content)}\")\n",
    "                add_tokens(assistant_end, 1)\n",
    "\n",
    "        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "        ids = ids[:max_tokens]\n",
    "        mask = mask[:max_tokens]\n",
    "\n",
    "        logger.info(f\"ä¼šè©±ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº† {ids=} {mask=}\")\n",
    "        return ids, mask\n",
    "\n",
    "    def visualize_tokenization(self, ids, mask, with_token_id=False):\n",
    "        \"\"\"Small helper function useful in debugging: visualize the tokenization of render_conversation\"\"\"\n",
    "        RED = '\\033[91m'\n",
    "        GREEN = '\\033[92m'\n",
    "        RESET = '\\033[0m'\n",
    "        GRAY = '\\033[90m'\n",
    "        tokens = []\n",
    "        for i, (token_id, mask_val) in enumerate(zip(ids, mask)):\n",
    "            token_str = self.decode([token_id])\n",
    "            color = GREEN if mask_val == 1 else RED\n",
    "            tokens.append(f\"{color}{token_str}{RESET}\")\n",
    "            if with_token_id:\n",
    "                tokens.append(f\"{GRAY}({token_id}){RESET}\")\n",
    "        return '|'.join(tokens)\n",
    "\n",
    "    def render_for_completion(self, conversation):\n",
    "        \"\"\"\n",
    "        Used during Reinforcement Learning. In that setting, we want to\n",
    "        render the conversation priming the Assistant for a completion.\n",
    "        Unlike the Chat SFT case, we don't need to return the mask.\n",
    "        \"\"\"\n",
    "        # We have some surgery to do: we need to pop the last message (of the Assistant)\n",
    "        conversation = copy.deepcopy(conversation) # avoid mutating the original\n",
    "        messages = conversation[\"messages\"]\n",
    "        assert messages[-1][\"role\"] == \"assistant\", \"Last message must be from the Assistant\"\n",
    "        messages.pop() # remove the last message (of the Assistant) inplace\n",
    "\n",
    "        # Now tokenize the conversation\n",
    "        ids, mask = self.render_conversation(conversation)\n",
    "\n",
    "        # Finally, to prime the Assistant for a completion, append the Assistant start token\n",
    "        assistant_start = self.encode_special(\"<|assistant_start|>\")\n",
    "        ids.append(assistant_start)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28551ac0",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa89209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10Bï¼ˆ100å„„ï¼‰æ–‡å­—ã ãŒã€å¤šã„ã®ã§2Bï¼ˆ20å„„ï¼‰æ–‡å­—ã«è¨­å®š\n",
    "max_chars = 2_000_000_000\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã«ä½¿ç”¨ã™ã‚‹æœ€å¤§æ–‡å­—æ•°: {max_chars=}\")\n",
    "\n",
    "# 1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯10,000æ–‡å­—\n",
    "doc_cap = 10_000\n",
    "logger.info(f\"1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°: {doc_cap=}\")\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°\n",
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯65,536ï¼ˆ2ã®16ä¹—ï¼‰\n",
    "vocab_size = 65_536\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°: {vocab_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_parquet_files(data_dir=None):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™\n",
    "    Args:\n",
    "        data_dir (str, optional): ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®DATA_DIRã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "    Returns:\n",
    "        List[str]: parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    # ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "    data_dir = DATA_DIR if data_dir is None else data_dir\n",
    "\n",
    "    # parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—\n",
    "    parquet_files = sorted([\n",
    "        f for f in os.listdir(data_dir)\n",
    "        if f.endswith('.parquet') and not f.endswith('.tmp')\n",
    "    ])\n",
    "\n",
    "    # ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    parquet_paths = [os.path.join(data_dir, f) for f in parquet_files]\n",
    "\n",
    "    return parquet_paths\n",
    "\n",
    "list_parquet_files()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08792b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquets_iter_batched(split, start=0, step=1):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒãƒƒãƒã§èª­ã¿è¾¼ã‚€ãŸã‚ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "\n",
    "    splitã¯ã€\"train\"ã¾ãŸã¯\"val\"ã‚’æŒ‡å®š\n",
    "    \"train\"ã®å ´åˆã€æœ€åˆã®N-1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    \"val\"ã®å ´åˆã€æœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨\n",
    "    startã¨stepã¯ã€åˆ†æ•£ãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆDistributed Data Processingï¼‰ã§ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹éš›ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        split (str): \"train\"ã¾ãŸã¯\"val\"\n",
    "        start (int, optional): é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0ã€‚\n",
    "        step (int, optional): ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚ºã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚\n",
    "    Yields:\n",
    "        List[str]: å„ãƒãƒƒãƒã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    logger.info(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿é–‹å§‹ {split=} {start=} {step=}\")\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    # ã™ã¹ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "    parquet_paths = list_parquet_files()\n",
    "    logger.debug(f\"å…¨ã¦ã®parquetãƒ•ã‚¡ã‚¤ãƒ« {parquet_paths=}\")\n",
    "\n",
    "    # splitã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ\n",
    "    parquet_paths = parquet_paths[:-1] if split == \"train\" else parquet_paths[-1:]\n",
    "    logger.info(f\"ä½¿ç”¨ã™ã‚‹parquetãƒ•ã‚¡ã‚¤ãƒ«: {parquet_paths=}\")\n",
    "\n",
    "    for filepath in parquet_paths:\n",
    "        # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        pf = pq.ParquetFile(filepath)\n",
    "        logger.debug(f\"{filepath=} {pf.num_row_groups=}\")\n",
    "\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¡Œã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆrow groupsï¼‰ã§ãƒ«ãƒ¼ãƒ—\n",
    "        # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã¯ã€æ•°åƒã‹ã‚‰æ•°ä¸‡è¡Œã®ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒ£ãƒ³ã‚¯\n",
    "        for rg_idx in range(start, pf.num_row_groups, step):\n",
    "\n",
    "            # è¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’èª­ã¿è¾¼ã‚€\n",
    "            rg = pf.read_row_group(rg_idx)\n",
    "            logger.debug(f\"{type(rg)=} {len(rg)=}\")\n",
    "\n",
    "            # ãƒ†ã‚­ã‚¹ãƒˆã‚«ãƒ©ãƒ ã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦å–å¾—ã—ã¦yield\n",
    "            texts = rg.column('text').to_pylist()\n",
    "            logger.debug(f\"{type(texts)=} {len(texts)=} {len(texts[0])=}\")\n",
    "\n",
    "            # 1ã‚°ãƒ«ãƒ¼ãƒ—åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™\n",
    "            yield texts\n",
    "\n",
    "text = parquets_iter_batched(\"train\")\n",
    "first_row_group = next(text)\n",
    "first_row_group[0][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_iterator():\n",
    "    \"\"\"\n",
    "    ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’1ãšã¤ä¾›çµ¦ã™ã‚‹ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿\n",
    "\n",
    "    1) Flatten the batches into a single iterator\n",
    "    2) Crop every document to args.doc_cap characters\n",
    "    3) Break when we've seen args.max_chars characters\n",
    "    \"\"\"\n",
    "    # æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ã‚¿ã‚’åˆæœŸåŒ–\n",
    "    nchars = 0\n",
    "\n",
    "    # ã‚·ãƒ£ãƒ¼ãƒ‰ã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "    for batch in parquets_iter_batched(split=\"train\"):\n",
    "\n",
    "        # ã‚·ãƒ£ãƒ¼ãƒ‰ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for doc in batch:\n",
    "\n",
    "            doc_text = doc\n",
    "            logger.debug(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾— {len(doc_text)=}\")\n",
    "\n",
    "            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ–‡å­—æ•°ãŒ10,000æ–‡å­—ã‚’è¶…ãˆã‚‹å ´åˆã€åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "            if len(doc_text) > doc_cap:\n",
    "                doc_text = doc_text[:doc_cap]\n",
    "                logger.debug(f\"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ‡ã‚Šè©°ã‚ {len(doc_text)=}\")\n",
    "\n",
    "            # æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ã‚¿ã‚’æ›´æ–°\n",
    "            nchars += len(doc_text)\n",
    "            logger.debug(f\"ç´¯ç©æ–‡å­—æ•°ã‚’æ›´æ–° {nchars=}\")\n",
    "\n",
    "            yield doc_text\n",
    "\n",
    "            if nchars > max_chars:\n",
    "                logger.info(f\"æœ€å¤§æ–‡å­—æ•°ã«é”ã—ãŸãŸã‚çµ‚äº† {nchars=}\")\n",
    "                return\n",
    "\n",
    "text_iter = text_iterator()\n",
    "first_doc = next(text_iter)\n",
    "len(first_doc), first_doc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743bb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã¦ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "# ï¼ˆè¨“ç·´æ¸ˆã¿ã®å ´åˆã¯æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿ï¼‰\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š\n",
    "base_dir = get_base_dir()\n",
    "tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {tokenizer_dir}\")\n",
    "\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    logger.debug(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´ã‚’é–‹å§‹\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "    text_iter = text_iterator()\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è¨“ç·´\n",
    "    tokenizer = RustBPETokenizer.train_from_iterator(text_iter, vocab_size)\n",
    "\n",
    "    t1 = time.time()\n",
    "    train_time = t1 - t0\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.debug(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è¨“ç·´å®Œäº† {train_time=}ç§’\") # 26ç§’\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜\n",
    "    tokenizer.save(tokenizer_dir)\n",
    "else:\n",
    "    logger.debug(\"æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\")\n",
    "    tokenizer = RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ¤œè¨¼\n",
    "\n",
    "test_text = \"\"\"Hello world! This is a test.\n",
    "Numbers: 123, 4567, 89\n",
    "Contractions: I'm, you're, it's\n",
    "Special chars: @#$%^&*()\n",
    "Unicode: ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ ğŸŒ\"\"\"\n",
    "\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "assert decoded == test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPBï¼ˆBits Per Byteï¼‰ã®è©•ä¾¡ã«å¿…è¦ãªæ—©è¦‹è¡¨ã‚’ä½œæˆ\n",
    "# é€šå¸¸ã®æå¤±è¨ˆç®—ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å¹³å‡æå¤±ã ãŒã€BPBã¯ãƒã‚¤ãƒˆã”ã¨ã®å¹³å‡æå¤±\n",
    "# BPB = ç·æå¤±/ç·ãƒã‚¤ãƒˆæ•°\n",
    "# ã“ã‚Œã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ãŸå ´åˆã§ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å…¬å¹³ã«æ¯”è¼ƒã§ãã‚‹\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èªå½™æ•°: {vocab_size=}\")\n",
    "\n",
    "special_set = set(tokenizer.get_special_tokens())\n",
    "logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³æ•°: {len(special_set)=}\")\n",
    "\n",
    "token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "logger.info(f\"æ—©è¦‹è¡¨ã®ä¿å­˜ãƒ‘ã‚¹: {token_bytes_path=}\")\n",
    "\n",
    "if not os.path.exists(token_bytes_path):\n",
    "    logger.setLevel(logging.ERROR)\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’æ–‡å­—åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰\n",
    "    token_strings = [tokenizer.decode([token_id]) for token_id in range(vocab_size)]\n",
    "    token_bytes = []\n",
    "\n",
    "    for token_id in range(vocab_size):\n",
    "        # æ–‡å­—åˆ—ã‚’å–å¾—\n",
    "        token_str = token_strings[token_id]\n",
    "\n",
    "        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆ\n",
    "        if token_str in special_set:\n",
    "            # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯0ãƒã‚¤ãƒˆã¨ã—ã¦æ—©è¦‹è¡¨ã«è¿½åŠ \n",
    "            token_bytes.append(0)\n",
    "        else:\n",
    "            # UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸéš›ã®ãƒã‚¤ãƒˆæ•°ã‚’è¨ˆç®—ã—æ—©è¦‹è¡¨ã«è¿½åŠ \n",
    "            id_bytes = len(token_str.encode(\"utf-8\"))\n",
    "            token_bytes.append(id_bytes)\n",
    "\n",
    "    # ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "    token_bytes = torch.tensor(token_bytes, dtype=torch.int32, device='cpu')\n",
    "\n",
    "    # æ—©è¦‹è¡¨ã‚’ä¿å­˜\n",
    "    with open(token_bytes_path, \"wb\") as f:\n",
    "        torch.save(token_bytes, f)\n",
    "\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "else:\n",
    "    token_bytes = torch.load(token_bytes_path)\n",
    "\n",
    "# æ—©è¦‹è¡¨ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º\n",
    "token_bytes_nonzero = (token_bytes[token_bytes > 0]).to(dtype=torch.float32)\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®æœ€å°ãƒã‚¤ãƒˆæ•° {int(token_bytes_nonzero.min().item())}\")\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®æœ€å¤§ãƒã‚¤ãƒˆæ•° {int(token_bytes_nonzero.max().item())}\")\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒã‚¤ãƒˆæ•°ã®å¹³å‡ {token_bytes_nonzero.mean().item()}\")\n",
    "logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ãƒã‚¤ãƒˆæ•°ã®æ¨™æº–åå·® {token_bytes_nonzero.std().item()}\")\n",
    "\n",
    "# æ¤œè¨¼\n",
    "token_id = 5000\n",
    "token_str = tokenizer.decode([token_id])\n",
    "num_bytes = token_bytes[token_id].item()\n",
    "logger.info(f\"Token ID: {token_id}, Token: '{token_str}', Bytes: {num_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1f17",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®è©•ä¾¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "news_text = r\"\"\"\n",
    "(Washington, D.C., July 9, 2025)- Yesterday, Mexicoâ€™s National Service of Agro-Alimentary Health, Safety, and Quality (SENASICA) reported a new case of New World Screwworm (NWS) in Ixhuatlan de Madero, Veracruz in Mexico, which is approximately 160 miles northward of the current sterile fly dispersal grid, on the eastern side of the country and 370 miles south of the U.S./Mexico border. This new northward detection comes approximately two months after northern detections were reported in Oaxaca and Veracruz, less than 700 miles away from the U.S. border, which triggered the closure of our ports to Mexican cattle, bison, and horses on May 11, 2025.\n",
    "\n",
    "While USDA announced a risk-based phased port re-opening strategy for cattle, bison, and equine from Mexico beginning as early as July 7, 2025, this newly reported NWS case raises significant concern about the previously reported information shared by Mexican officials and severely compromises the outlined port reopening schedule of five ports from July 7-September 15. Therefore, in order to protect American livestock and our nationâ€™s food supply, Secretary Rollins has ordered the closure of livestock trade through southern ports of entry effective immediately.\n",
    "\n",
    "â€œThe United States has promised to be vigilant â€” and after detecting this new NWS case, we are pausing the planned port reopeningâ€™s to further quarantine and target this deadly pest in Mexico. We must see additional progress combatting NWS in Veracruz and other nearby Mexican states in order to reopen livestock ports along the Southern border,â€ said U.S. Secretary of Agriculture Brooke L. Rollins. â€œThanks to the aggressive monitoring by USDA staff in the U.S. and in Mexico, we have been able to take quick and decisive action to respond to the spread of this deadly pest.â€\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeca588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨éè‹±èªãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "korean_text = r\"\"\"\n",
    "ì •ì§í•œ ì‚¬ì‹¤ ìœ„ì—, ê³µì •í•œ ì‹œì„ ì„ ë”í•˜ë‹¤\n",
    "Herald Korea Times\n",
    "\n",
    "í—¤ëŸ´ë“œì½”ë¦¬ì•„íƒ€ì„ì¦ˆëŠ” ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ë¬¸í™” ë“± í•œêµ­ ì‚¬íšŒ ì „ë°˜ì˜ ì£¼ìš” ì´ìŠˆë¥¼ ì‹¬ë„ ìˆê²Œ ë‹¤ë£¨ëŠ” ì¢…í•© ì˜¨ë¼ì¸ ì‹ ë¬¸ì‚¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ë‹¨ìˆœíˆ ë‰´ìŠ¤ë¥¼ ì „ë‹¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì‚¬ì‹¤(Fact)ì— ê¸°ë°˜í•œ ì–‘ì¸¡ì˜ ì‹œê°ì„ ê· í˜• ìˆê²Œ ì¡°ëª…í•˜ë©°, ë…ì ì—¬ëŸ¬ë¶„ì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•  ìˆ˜ ìˆëŠ” â€˜ì •ë³´ì˜ ê· í˜•â€™ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "í•œêµ­ ì–¸ë¡ ì˜ ì˜¤ëœ ë¬¸ì œë¡œ ì§€ì ë˜ì–´ ì˜¨ ì •ì¹˜ì  í¸í–¥, ì´ë…ì  ì™œê³¡ì—ì„œ ë²—ì–´ë‚˜\n",
    "ì˜¤ì§ ì •ì§í•¨ê³¼ ê³µì •í•¨ì„ ì›ì¹™ìœ¼ë¡œ ì‚¼ëŠ” ì–¸ë¡ ì„ ì§€í–¥í•©ë‹ˆë‹¤.\n",
    "ì–´ëŠ í•œìª½ì˜ ì£¼ì¥ë§Œì„ í™•ëŒ€í•˜ê±°ë‚˜ ê°ì¶”ì§€ ì•Šê³ ,\n",
    "**ëª¨ë“  ìŸì ì— ëŒ€í•´ â€˜ë¬´ì—‡ì´ ìŸì ì¸ì§€â€™, â€˜ëˆ„ê°€ ë¬´ì—‡ì„ ì£¼ì¥í•˜ëŠ”ì§€â€™, â€˜ì‚¬ì‹¤ì€ ë¬´ì—‡ì¸ì§€â€™**ë¥¼ ëª…í™•íˆ ì „ë‹¬í•˜ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆ\n",
    "\n",
    "code_text = r\"\"\"\n",
    "class BasicTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # input text preprocessing\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count up the number of times every consecutive pair appears\n",
    "            stats = get_stats(ids)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids, pair, idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ã®æ•°å­¦ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "math_text = r\"\"\"\n",
    "\\documentclass[12pt]{article}\n",
    "\\usepackage{amsmath,amsthm,amssymb}\n",
    "\\usepackage[margin=1in]{geometry}\n",
    "\n",
    "\\newtheorem{theorem}{Theorem}\n",
    "\\newtheorem*{remark}{Remark}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\begin{center}\n",
    "{\\Large A Cute Identity: The Sum of Cubes is a Square}\n",
    "\\end{center}\n",
    "\n",
    "\\begin{theorem}\n",
    "For every integer $n \\ge 1$,\n",
    "\\[\n",
    "\\sum_{k=1}^{n} k^{3} \\;=\\; \\left(\\frac{n(n+1)}{2}\\right)^{2}.\n",
    "\\]\n",
    "\\end{theorem}\n",
    "\n",
    "\\begin{proof}[Proof 1 (Induction)]\n",
    "Let $S(n) = \\sum_{k=1}^{n} k^3$. For $n=1$, $S(1)=1=(1\\cdot 2/2)^2$, so the base case holds.\n",
    "\n",
    "Assume $S(n)=\\big(\\tfrac{n(n+1)}{2}\\big)^2$ for some $n\\ge 1$.\n",
    "Then\n",
    "\\[\n",
    "S(n+1)\n",
    "= S(n) + (n+1)^3\n",
    "= \\left(\\frac{n(n+1)}{2}\\right)^2 + (n+1)^3.\n",
    "\\]\n",
    "Factor out $(n+1)^2$:\n",
    "\\[\n",
    "S(n+1)\n",
    "= (n+1)^2\\left( \\frac{n^2}{4} + (n+1) \\right)\n",
    "= (n+1)^2\\left( \\frac{n^2 + 4n + 4}{4} \\right)\n",
    "= (n+1)^2\\left( \\frac{(n+2)^2}{4} \\right).\n",
    "\\]\n",
    "Thus\n",
    "\\[\n",
    "S(n+1)=\\left(\\frac{(n+1)(n+2)}{2}\\right)^2,\n",
    "\\]\n",
    "which matches the claimed formula with $n$ replaced by $n+1$. By induction, the identity holds for all $n\\ge 1$.\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{proof}[Proof 2 (Algebraic telescoping)]\n",
    "Recall the binomial identity\n",
    "\\[\n",
    "(k+1)^4 - k^4 = 4k^3 + 6k^2 + 4k + 1.\n",
    "\\]\n",
    "Summing both sides from $k=0$ to $n$ telescopes:\n",
    "\\[\n",
    "(n+1)^4 - 0^4\n",
    "= \\sum_{k=0}^{n}\\big(4k^3 + 6k^2 + 4k + 1\\big)\n",
    "= 4\\sum_{k=1}^{n}k^3 + 6\\sum_{k=1}^{n}k^2 + 4\\sum_{k=1}^{n}k + (n+1).\n",
    "\\]\n",
    "Using the standard sums\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k = \\frac{n(n+1)}{2}\n",
    "\\quad\\text{and}\\quad\n",
    "\\sum_{k=1}^{n}k^2 = \\frac{n(n+1)(2n+1)}{6},\n",
    "\\]\n",
    "solve for $\\sum_{k=1}^{n}k^3$ to get\n",
    "\\[\n",
    "\\sum_{k=1}^{n}k^3 = \\left(\\frac{n(n+1)}{2}\\right)^2.\n",
    "\\]\n",
    "\\end{proof}\n",
    "\n",
    "\\begin{remark}\n",
    "Geometrically, the identity says: ``adding up $1^3,2^3,\\dots,n^3$ builds a perfect squareâ€™â€™â€”namely the square of the $n$th triangular number. This is why one sometimes calls it the \\emph{sum-of-cubes is a square} phenomenon.\n",
    "\\end{remark}\n",
    "\n",
    "\\end{document}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚¹ãƒˆç”¨ã®ç§‘å­¦ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "\n",
    "science_text = r\"\"\"\n",
    "Photosynthesis is a photochemical energy transduction process in which light-harvesting pigmentâ€“protein complexes within the thylakoid membranes of oxygenic phototrophs absorb photons and initiate charge separation at the reaction center, driving the linear electron transport chain from water to NADPâº via photosystem II, the cytochrome bâ‚†f complex, and photosystem I, concomitantly generating a trans-thylakoid proton motive force utilized by chloroplastic ATP synthase. The light-dependent reactions produce ATP and NADPH, which fuel the Calvinâ€“Bensonâ€“Bassham cycle in the stroma, wherein ribulose-1,5-bisphosphate is carboxylated by ribulose-1,5-bisphosphate carboxylase/oxygenase (RuBisCO) to form 3-phosphoglycerate, subsequently reduced and regenerated through a series of enzymatic steps, enabling net assimilation of COâ‚‚ into triose phosphates and ultimately carbohydrates. This process is tightly regulated by photoprotective mechanisms, redox feedback, and metabolite flux, representing a central biochemical pathway coupling solar energy capture to the biosphereâ€™s primary productivity.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142e9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰è¨“ç·´ç”¨ã¨æ¤œè¨¼ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "\n",
    "train_docs = next(parquets_iter_batched(split=\"train\"))\n",
    "train_text = \"\\n\".join(train_docs)\n",
    "val_docs = next(parquets_iter_batched(split=\"val\"))\n",
    "val_text = \"\\n\".join(val_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…¨ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«é›†ç´„\n",
    "\n",
    "all_text = [\n",
    "    (\"news\", news_text),\n",
    "    (\"korean\", korean_text),\n",
    "    (\"code\", code_text),\n",
    "    (\"math\", math_text),\n",
    "    (\"science\", science_text),\n",
    "    (\"fwe-train\", train_text),\n",
    "    (\"fwe-val\", val_text),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1803ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨“ç·´æ¸ˆã¿ã®Rust\n",
    "\n",
    "def get_tokenizer():\n",
    "    \"\"\"\n",
    "    RustBPETokenizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™\n",
    "\n",
    "    Returns:\n",
    "        RustBPETokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    # return HuggingFaceTokenizer.from_directory(tokenizer_dir)\n",
    "    return RustBPETokenizer.from_directory(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9ca71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2ã¨GPT-4ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨æ¯”è¼ƒ\n",
    "\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "tokenizer_results = {}\n",
    "vocab_sizes = {}\n",
    "\n",
    "for tokenizer_name in [\"gpt2\", \"gpt4\", \"ours\"]:\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—\n",
    "    if tokenizer_name == \"gpt2\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"gpt2\")\n",
    "    elif tokenizer_name == \"gpt4\":\n",
    "        tokenizer = RustBPETokenizer.from_pretrained(\"cl100k_base\")\n",
    "    else:\n",
    "        tokenizer = get_tokenizer()\n",
    "\n",
    "    vocab_sizes[tokenizer_name] = tokenizer.get_vocab_size()\n",
    "    tokenizer_results[tokenizer_name] = {}\n",
    "\n",
    "    for name, text in all_text:\n",
    "        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¨ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’è¡Œã„çµæœã‚’æ¤œè¨¼\n",
    "        encoded = tokenizer.encode(text)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert decoded == text\n",
    "\n",
    "        # UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸéš›ã®ãƒã‚¤ãƒˆæ•°ã‚’è¨ˆç®—\n",
    "        encoded_bytes = text.encode('utf-8')\n",
    "        ratio = len(encoded_bytes) / len(encoded)\n",
    "        tokenizer_results[tokenizer_name][name] = {\n",
    "            'bytes': len(encoded_bytes),\n",
    "            'tokens': len(encoded),\n",
    "            'ratio': ratio\n",
    "        }\n",
    "\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è¦–åŒ–\n",
    "\n",
    "print(f\"\\nèªå½™æ•°:\")\n",
    "print(f\"GPT-2: {vocab_sizes['gpt2']}\")\n",
    "print(f\"GPT-4: {vocab_sizes['gpt4']}\")\n",
    "print(f\"Ours: {vocab_sizes['ours']}\")\n",
    "\n",
    "# ANSI color codes\n",
    "GREEN = '\\033[92m'\n",
    "RED = '\\033[91m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "def print_comparison(baseline_name, baseline_results, ours_results, all_text):\n",
    "    print(f\"\\n{baseline_name}ã¨ã®æ¯”è¼ƒ:\")\n",
    "    print(\"=\" * 95)\n",
    "    print(f\"{'Text Type':<10} {'Bytes':<8} {baseline_name:<15} {'Ours':<15} {'Relative':<12} {'Better':<10}\")\n",
    "    print(f\"{'':10} {'':8} {'Tokens':<7} {'Ratio':<7} {'Tokens':<7} {'Ratio':<7} {'Diff %':<12}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, text in all_text:\n",
    "        baseline_data = baseline_results[name]\n",
    "        ours_data = ours_results[name]\n",
    "\n",
    "        # Calculate relative difference (positive means ours is better, negative means worse)\n",
    "        # Using tokens: fewer tokens is better, so we calculate (baseline_tokens - ours_tokens) / baseline_tokens\n",
    "        relative_diff = ((baseline_data['tokens'] - ours_data['tokens']) / baseline_data['tokens']) * 100\n",
    "\n",
    "        # Determine which has better compression (higher ratio = better)\n",
    "        if baseline_data['ratio'] > ours_data['ratio']:\n",
    "            baseline_color, ours_color = GREEN, RED\n",
    "            better = baseline_name\n",
    "            diff_color = RED\n",
    "        elif ours_data['ratio'] > baseline_data['ratio']:\n",
    "            baseline_color, ours_color = RED, GREEN\n",
    "            better = \"Ours\"\n",
    "            diff_color = GREEN\n",
    "        else:\n",
    "            baseline_color, ours_color = \"\", \"\"\n",
    "            better = \"Tie\"\n",
    "            diff_color = \"\"\n",
    "\n",
    "        print(f\"{name:<10} {baseline_data['bytes']:<8} \"\n",
    "              f\"{baseline_color}{baseline_data['tokens']:<7}{RESET} \"\n",
    "              f\"{baseline_color}{baseline_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['tokens']:<7}{RESET} \"\n",
    "              f\"{ours_color}{ours_data['ratio']:<7.2f}{RESET} \"\n",
    "              f\"{diff_color}{relative_diff:+7.1f}%{RESET}     \"\n",
    "              f\"{better:<10}\")\n",
    "\n",
    "# Print comparisons\n",
    "print_comparison(\"GPT-2\", tokenizer_results['gpt2'], tokenizer_results['ours'], all_text)\n",
    "print_comparison(\"GPT-4\", tokenizer_results['gpt4'], tokenizer_results['ours'], all_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59e90f",
   "metadata": {},
   "source": [
    "## æœ€é©åŒ–é–¢æ•°ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2299c2",
   "metadata": {},
   "source": [
    "æœ€é©åŒ–é–¢æ•°ã¯[Muonï¼ˆãƒŸãƒ¥ãƒ¼ã‚ªãƒ³, Momentum Orthgonalized by Newton-Schulzï¼‰][1]ã‚’ä½¿ç”¨\n",
    "\n",
    "[1]: https://arxiv.org/abs/2502.16982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile # PyTorch2.0ã®JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã‚’ä½¿ç”¨\n",
    "def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©æ³•ã§å‹¾é…ã®ç›´è¡ŒåŒ–ã‚’è¿‘ä¼¼ã™ã‚‹\n",
    "\n",
    "    è¡Œåˆ—Gã®ç›´äº¤è¡Œåˆ—ã¯ã€Gã®ç‰¹ç•°å€¤åˆ†è§£G = USV^Tã«å¯¾ã—ã¦UV^Tã§è¨ˆç®—ã§ãã‚‹\n",
    "    ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã¯è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ãŸã‚è¿‘ä¼¼æ‰‹æ³•ã‚’ç”¨ã„ã‚‹\n",
    "    åŸç‚¹ã«ãŠã‘ã‚‹å‚¾ãã‚’æœ€å¤§åŒ–ã™ã‚‹ã‚ˆã†ã«é¸æŠã•ã‚ŒãŸä¿‚æ•°ã‚’æŒã¤5æ¬¡ã®åå¾©æ³•ã‚’æ¡ç”¨\n",
    "\n",
    "    Args: \n",
    "        G (Tensor): ç›´è¡ŒåŒ–ã™ã‚‹è¡Œåˆ—ã€å½¢çŠ¶ã¯(..., m, n)\n",
    "        steps (int): åå¾©å›æ•°\n",
    "    Returns:\n",
    "        Tensor: ç›´äº¤åŒ–ã•ã‚ŒãŸè¡Œåˆ—ã€å½¢çŠ¶ã¯(..., m, n)\n",
    "    \"\"\"\n",
    "\n",
    "    assert G.ndim >= 2\n",
    "\n",
    "    # 5æ¬¡åå¾©æ³•ã®ãŸã‚ã®ä¿‚æ•°\n",
    "    a, b, c = (3.4445, -4.7750,  2.0315)\n",
    "\n",
    "    # å‹¾é…ã‚’bfloat16ã«ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "    X = G.bfloat16()\n",
    "\n",
    "    # å®‰å®šåŒ–ã®ãŸã‚è¡Œåˆ—ã‚’æ¨ªé•·ã«ã™ã‚‹\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ï¼ˆæœ€å¤§ã®ç‰¹ç•°å€¤ï¼‰ã‚’1ä»¥ä¸‹ã«æ­£è¦åŒ–\n",
    "    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)\n",
    "\n",
    "    # ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ¯ãƒ«ãƒ„åå¾©æ³•ã‚’é©ç”¨\n",
    "    for _ in range(steps):\n",
    "        # A = X X^T\n",
    "        A = X @ X.mT\n",
    "\n",
    "        # B = b(X X^T) + c(X X^T)^2\n",
    "        B = b * A + c * A @ A\n",
    "\n",
    "        # X_k+1 = a X_k + (B) X_k \n",
    "        X = a * X + B @ X\n",
    "\n",
    "    # å…ƒã®å½¢çŠ¶ã«æˆ»ã™\n",
    "    if G.size(-2) > G.size(-1):\n",
    "        X = X.mT\n",
    "\n",
    "    # ç›´äº¤è¡Œåˆ—ã‚’è¿”ã™\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2075e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muonæœ€é©åŒ–é–¢æ•° https://kellerjordan.github.io/posts/muon/\n",
    "\n",
    "    å†…éƒ¨çš„ã«æ¨™æº–çš„ãªSGDãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’å®Ÿè¡Œã—ã€ãã®å¾Œã«ç›´äº¤åŒ–ã®å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹\n",
    "    ç›´è¡ŒåŒ–ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€å„2Dãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ãŒæœ€ã‚‚è¿‘ã„ç›´äº¤è¡Œåˆ—ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹\n",
    "    å„æ›´æ–°ã‚’åŠ¹ç‡çš„ã«ç›´äº¤åŒ–ã™ã‚‹ãŸã‚ã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    ã“ã‚Œã«ã‚ˆã‚Šã€GPUä¸Šã§bfloat16ã§å®‰å®šã—ã¦å®Ÿè¡Œã§ãã‚‹åˆ©ç‚¹ãŒã‚ã‚‹\n",
    "\n",
    "    æ³¨æ„:\n",
    "    - ã“ã®æœ€é©åŒ–é–¢æ•°ã¯ã€åŸ‹ã‚è¾¼ã¿å±¤ã€æœ€çµ‚å…¨çµåˆå±¤ã€0æ¬¡å…ƒãƒ»1æ¬¡å…ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯ä½¿ç”¨ã—ãªã„ï¼ˆAdamWãªã©ã‚’ä½¿ç”¨ï¼‰\n",
    "    - 4Dã®ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã«ä½¿ç”¨ã™ã‚‹å ´åˆã€æœ€å¾Œã®3ã¤ã®æ¬¡å…ƒã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã™ã‚‹ã¨æ©Ÿèƒ½ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        lr (float): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹å­¦ç¿’ç‡\n",
    "        momentum (float): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ \n",
    "        nesterov (bool): å†…éƒ¨SGDã§Nesterovã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆæ¨å¥¨ï¼‰\n",
    "        ns_steps (int): ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):\n",
    "        \"\"\"\n",
    "        Muonæœ€é©åŒ–é–¢æ•°ã®åˆæœŸåŒ–\n",
    "\n",
    "        Args:\n",
    "            params (iterable): æœ€é©åŒ–ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "            lr (float, optional): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹å­¦ç¿’ç‡\n",
    "            momentum (float, optional): å†…éƒ¨SGDã§ä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ \n",
    "            nesterov (bool, optional): å†…éƒ¨SGDã§Nesterovã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ns_steps (int, optional): ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ãƒ»ã‚·ãƒ¥ãƒ«ãƒ„åå¾©ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "        \"\"\"\n",
    "        # æœ€é©åŒ–é–¢æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "\n",
    "        params: list[Tensor] = [*params]\n",
    "\n",
    "        param_groups = []\n",
    "\n",
    "        # åŒã˜è¦ç´ æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "        for size in {p.numel() for p in params}:\n",
    "            group = dict(params=[p for p in params if p.numel() == size])\n",
    "            param_groups.append(group)\n",
    "\n",
    "        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Muonæœ€é©åŒ–é–¢æ•°ã®1ã‚¹ãƒ†ãƒƒãƒ—ã®æ›´æ–°ã‚’å®Ÿè¡Œ\n",
    "        \"\"\"\n",
    "\n",
    "        # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—ã§ãƒ«ãƒ¼ãƒ—\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "\n",
    "            # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ«ãƒ¼ãƒ—\n",
    "            for p in params:\n",
    "                # å‹¾é…ã‚’å–å¾—\n",
    "                g = p.grad\n",
    "                assert g is not None\n",
    "\n",
    "                # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒãƒƒãƒ•ã‚¡ã‚’å–å¾—ã¾ãŸã¯åˆæœŸåŒ–\n",
    "                state = self.state[p]\n",
    "                if \"momentum_buffer\" not in state:\n",
    "                    state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                buf: Tensor = state[\"momentum_buffer\"]\n",
    "\n",
    "                # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒãƒƒãƒ•ã‚¡ã‚’æœ€æ–°ã®å‹¾é…ã§æ›´æ–°ï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰\n",
    "                buf.lerp_(g, 1 - group[\"momentum\"])\n",
    "\n",
    "                # ãƒã‚¹ãƒ†ãƒ­ãƒ•ãƒ»ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å‹¾é…ã‚’èª¿æ•´\n",
    "                g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "\n",
    "                # å‹¾é…ã‚’ç›´äº¤åŒ–\n",
    "                g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "                # -group[\"lr\"]: å­¦ç¿’ç‡ã‚’è² ã«ã—ã¦æ¸›å°‘æ–¹å‘ã«æ›´æ–°\n",
    "                # max(1, p.size(-2) / p.size(-1))**0.5: è¡Œåˆ—ã®å½¢çŠ¶ã«åŸºã¥ã„ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "                p.add_(g, alpha=-group[\"lr\"] * max(1, p.size(-2) / p.size(-1))**0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeacbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistMuon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon: SGD-momentum + (optional) Nesterov, then orthogonalize the 2D update via Newtonâ€“Schulz,\n",
    "    finally apply aspect-ratio scaled step. Performs its own distributed synchronization:\n",
    "      - reduce_scatter(AVG) for gradient averaging\n",
    "      - all_gather to replicate updated weights\n",
    "\n",
    "    Notes:\n",
    "      * Designed for 2D parameters (e.g., linear/conv kernels reshaped to 2D). Do not use for 0D/1D\n",
    "        params like embeddings or scalars.\n",
    "      * Momentum buffers are maintained only on the 'owner' rank for each parameter (rank chosen\n",
    "        by block-cyclic assignment below). If you checkpoint optimizer state on a single rank,\n",
    "        consolidate states beforehand.\n",
    "\n",
    "    Args:\n",
    "        params: iterable of Tensors\n",
    "        lr: learning rate\n",
    "        momentum: momentum coefficient in [0,1)\n",
    "        nesterov: if True, Nesterov-style update (g <- lerp(g, buf, momentum)); else use buf\n",
    "        ns_steps: number of Newtonâ€“Schulz iterations for the orthogonalization\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr: float = 0.02, momentum: float = 0.95,\n",
    "                 nesterov: bool = True, ns_steps: int = 5):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)\n",
    "        params = list(params)\n",
    "        assert all(p.ndim == 2 for p in params), \"Muon expects 2D parameters only\"\n",
    "        rank = dist.get_rank()\n",
    "        # Group all parameters by their shape\n",
    "        shapes = sorted({p.shape for p in params}) # sort to ensure consistent / deterministic ordering\n",
    "        param_groups = []\n",
    "        for shape in shapes:\n",
    "            group_params = [p for p in params if p.shape == shape]\n",
    "            device, dtype = group_params[0].device, group_params[0].dtype\n",
    "            assert all(p.device == device for p in group_params)\n",
    "            assert all(p.dtype == dtype for p in group_params)\n",
    "            if rank == 0:\n",
    "                print(f\"Muon: Grouping {len(group_params)} params of shape {shape}, device {device}, dtype {dtype}\")\n",
    "            param_groups.append(dict(params=group_params, zero_buffer=torch.zeros_like(group_params[0])))\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Ensure all grads exist\n",
    "        assert all(p.grad is not None for group in self.param_groups for p in group[\"params\"]), \"All params must have grads\"\n",
    "\n",
    "        # Kick off all the reduce scatter operations to average up the gradients across all ranks\n",
    "        all_reduce_futures = []\n",
    "        for group in self.param_groups:\n",
    "            params = group[\"params\"]\n",
    "            zero_buffer = group[\"zero_buffer\"]\n",
    "            # Go through params in groups of world_size.\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                # The compute owner of each param is rank i % world_size\n",
    "                owner_idx = base_i + rank\n",
    "                # each rank stacks up its chunk of world_size params into a list\n",
    "                rs_input = [p.grad for p in params[base_i:base_i + world_size]]\n",
    "                # pad rs_input with the zero buffer to complete the group\n",
    "                rs_input.extend([zero_buffer] * (world_size - len(rs_input)))\n",
    "                # the output buffer gets strided across the group based on the rank\n",
    "                rs_output = params[owner_idx].grad if owner_idx < len(params) else torch.empty_like(zero_buffer)\n",
    "                # reduce scatter the gradients within this group of world_size params\n",
    "                work = dist.reduce_scatter(rs_output, rs_input, op=dist.ReduceOp.AVG, async_op=True).get_future()\n",
    "                all_reduce_futures.append(work)\n",
    "\n",
    "        # Now each rank computes the update and gathers\n",
    "        future_idx = 0\n",
    "        all_gather_futures = []\n",
    "        for group in self.param_groups:\n",
    "            params = group[\"params\"]\n",
    "            zero_buffer = group[\"zero_buffer\"]\n",
    "            # Go through params in groups of world_size.\n",
    "            for base_i in range(0, len(params), world_size):\n",
    "                # The compute owner of each param is rank i % world_size\n",
    "                owner_idx = base_i + rank # calculate the index of the param that this rank owns\n",
    "                # Wait for the reduce scatter to complete\n",
    "                all_reduce_futures[future_idx].wait() # possibly later we could use wait_any polling instead\n",
    "                future_idx += 1\n",
    "                # Owner computes the Muon update, result is in its param\n",
    "                if owner_idx < len(params):\n",
    "                    p = params[owner_idx]\n",
    "                    g = p.grad  # now averaged across ranks\n",
    "                    state = self.state[p]\n",
    "                    if \"momentum_buffer\" not in state:\n",
    "                        state[\"momentum_buffer\"] = torch.zeros_like(g)\n",
    "                    buf: Tensor = state[\"momentum_buffer\"]\n",
    "                    buf.lerp_(g, 1.0 - group[\"momentum\"])\n",
    "                    g = g.lerp_(buf, group[\"momentum\"]) if group[\"nesterov\"] else buf\n",
    "                    g = zeropower_via_newtonschulz5(g, steps=group[\"ns_steps\"])\n",
    "                    scale = (max(1.0, p.size(-2) / p.size(-1)) ** 0.5)\n",
    "                    p.add_(g, alpha=-group[\"lr\"] * scale)\n",
    "                # Replicate updated parameters to all ranks\n",
    "                ag_input = params[owner_idx] if owner_idx < len(params) else zero_buffer\n",
    "                ag_output = params[base_i:base_i + world_size]\n",
    "                ag_output.extend([torch.empty_like(zero_buffer) for _ in range(world_size - len(ag_output))]) # pad\n",
    "                work = dist.all_gather(ag_output, ag_input, async_op=True).get_future()\n",
    "                all_gather_futures.append(work)\n",
    "\n",
    "        # Wait for all work to finish\n",
    "        torch.futures.collect_all(all_gather_futures).wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0db608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistAdamW(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Distributed AdamW optimizer.\n",
    "    In the style of ZeRO-2, i.e. sharded optimizer states and gradient reduction\n",
    "    \"\"\"\n",
    "    def __init__(self, param_groups, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super().__init__(param_groups, defaults)\n",
    "\n",
    "    @torch.compile\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        reduce_scatter_futures: list[torch.Future] = []\n",
    "        all_reduce_futures: list[torch.Future] = []\n",
    "        grad_slices = []\n",
    "        for group in self.param_groups:\n",
    "            params: list[Tensor] = group[\"params\"]\n",
    "            for base_i in range(len(params)):\n",
    "                grad = params[base_i].grad\n",
    "                rank_size = grad.shape[0] // world_size\n",
    "                grad_slice = torch.empty_like(grad[:rank_size])\n",
    "                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())\n",
    "                grad_slices.append(grad_slice)\n",
    "\n",
    "        idx = 0\n",
    "        for group in self.param_groups:\n",
    "            beta1, beta2 = group['betas']\n",
    "            eps = group['eps']\n",
    "            wd = group['weight_decay']\n",
    "            params = group['params']\n",
    "            for base in range(len(params)):\n",
    "                reduce_scatter_futures[idx].wait()\n",
    "                p = params[base]\n",
    "                rank_size = p.shape[0] // world_size\n",
    "                p_slice = p[rank * rank_size:(rank + 1) * rank_size]\n",
    "                lr = group['lr'] * getattr(p, \"lr_mul\", 1.0)\n",
    "                state = self.state[p]\n",
    "                g_slice = grad_slices[idx]\n",
    "                # State init\n",
    "                if not state:\n",
    "                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)\n",
    "                    state['exp_avg'] = torch.zeros_like(p_slice)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_slice)\n",
    "                exp_avg = state['exp_avg']\n",
    "                exp_avg_sq = state['exp_avg_sq']\n",
    "                state['step'] += 1\n",
    "                t = state['step']\n",
    "                # weight decay\n",
    "                if wd != 0:\n",
    "                    eff_weight_decay = lr * wd * getattr(p, \"wd_mul\", 1.0)\n",
    "                    p_slice.mul_(1 - eff_weight_decay)\n",
    "                # update running averages\n",
    "                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)\n",
    "                # bias corrections\n",
    "                bias1 = 1 - beta1 ** t\n",
    "                bias2 = 1 - beta2 ** t\n",
    "                # compute step\n",
    "                denom = exp_avg_sq.sqrt().add_(eps)\n",
    "                step_size = lr * (torch.sqrt(bias2) / bias1)\n",
    "                update = exp_avg.div(denom).mul_(step_size)\n",
    "                p_slice.add_(other=update, alpha=-1.0)\n",
    "                idx += 1\n",
    "                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())\n",
    "        torch.futures.collect_all(all_reduce_futures).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3c30d",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea258fa0",
   "metadata": {},
   "source": [
    "### ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f83803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ddp():\n",
    "    \"\"\"\n",
    "    åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDistributed Data Parallel, DDPï¼‰ãŒæœ‰åŠ¹ã‹ã©ã†ã‹\n",
    "\n",
    "    Returns:\n",
    "        bool: DDPãŒæœ‰åŠ¹ãªå ´åˆTrueã€ãã†ã§ãªã„å ´åˆFalse\n",
    "    \"\"\"\n",
    "\n",
    "    # RANKç’°å¢ƒå¤‰æ•°ã¯torchrunãªã©ã«ã‚ˆã£ã¦è¨­å®šã•ã‚Œã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®è­˜åˆ¥å­\n",
    "    return int(os.environ.get('RANK', -1)) != -1\n",
    "\n",
    "is_ddp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_info():\n",
    "    \"\"\"\n",
    "    åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDistributed Data Parallel, DDPï¼‰è¨“ç·´ã®ç’°å¢ƒå¤‰æ•°ã‚’å–å¾—\n",
    "    \"\"\"\n",
    "    if is_ddp():\n",
    "        assert all(var in os.environ for var in ['RANK', 'LOCAL_RANK', 'WORLD_SIZE'])\n",
    "\n",
    "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªãƒ—ãƒ­ã‚»ã‚¹è­˜åˆ¥å­\n",
    "        ddp_rank = int(os.environ['RANK'])\n",
    "\n",
    "        # ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆåŒä¸€ãƒãƒ¼ãƒ‰å†…ï¼‰ã®ãƒ—ãƒ­ã‚»ã‚¹è­˜åˆ¥å­\n",
    "        ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "        # ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºï¼ˆå…¨ãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼‰\n",
    "        ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "        return True, ddp_rank, ddp_local_rank, ddp_world_size\n",
    "    else:\n",
    "        return False, 0, 0, 1\n",
    "\n",
    "get_dist_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    \"\"\"\n",
    "    å¹³æ–¹æ ¹å¹³å‡äºŒä¹—ãƒãƒ«ãƒ æ­£è¦åŒ–ï¼ˆRoot Mean Square Normalization, RMSNormï¼‰ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    Returns:\n",
    "        Tensor: æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"RMSNormã‚’é©ç”¨ {x.shape=}\")\n",
    "\n",
    "    # ã‚²ã‚¤ãƒ³ã‚„ãƒã‚¤ã‚¢ã‚¹ã‚’æŒãŸãªã„ãŸã‚é«˜é€Ÿ\n",
    "    res = F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "    logger.info(f\"RMSNormé©ç”¨å®Œäº† {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x, cos, sin):\n",
    "    \"\"\"\n",
    "    å›è»¢ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆRoPE, Rotary Positional Embeddingï¼‰ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        cos (Tensor): ã‚³ã‚µã‚¤ãƒ³æˆåˆ†ã€å½¢çŠ¶ã¯(1, seq_len, 1, head_dim/2)\n",
    "        sin (Tensor): ã‚µã‚¤ãƒ³æˆåˆ†ã€å½¢çŠ¶ã¯(1, seq_len, 1, head_dim/2)\n",
    "    Returns:\n",
    "        Tensor: RoPEãŒé©ç”¨ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯(batch_size, seq_len, num_heads, head_dim)\n",
    "    \"\"\"\n",
    "    logger.info(f\"RoPEã‚’é©ç”¨ {x.shape=}, {x.dtype=} {cos.shape=}, {cos.dtype=} {sin.shape=} {sin.dtype=}\")\n",
    "\n",
    "    assert x.ndim == 4\n",
    "\n",
    "    # head_dimã‚’2ã§åˆ†å‰²\n",
    "    d = x.shape[3] // 2\n",
    "\n",
    "    # æœ€å¾Œã®æ¬¡å…ƒã‚’2ã¤ã«åˆ†å‰²\n",
    "    x1, x2 = x[..., :d], x[..., d:]\n",
    "\n",
    "    # 2æ¬¡å…ƒå›è»¢è¡Œåˆ—ã‚’é©ç”¨\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    out = torch.cat([y1, y2], 3) # æœ€å¾Œã®æ¬¡å…ƒã§çµåˆ\n",
    "\n",
    "    # sinã¨cosã¯float32ã§è¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ãŸã‚ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿å‹ã«æˆ»ã™\n",
    "    out = out.to(x.dtype)\n",
    "\n",
    "    logger.info(f\"RoPEé©ç”¨å®Œäº† {out.shape=}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e339f",
   "metadata": {},
   "source": [
    "### GPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4ef441",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    sequence_len: int = 1024\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 6 # number of query heads\n",
    "    n_kv_head: int = 6 # number of key/value heads (MQA)\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355ebe2",
   "metadata": {},
   "source": [
    "### CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ba25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    GQAï¼ˆGroup Query Attentionï¼‰ã‚’å®Ÿè£…ã—ãŸå› æœã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx):\n",
    "        logger.info(f\"CausalSelfAttentionã‚’åˆæœŸåŒ– {config.n_head=} {config.n_kv_head=} {config.n_embd=} {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        logger.debug(f\"{head_dim=}\")\n",
    "\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        assert self.n_kv_head <= self.n_head and self.n_head % self.n_kv_head == 0\n",
    "\n",
    "        self.c_q = nn.Linear(self.n_embd, self.n_head * self.head_dim, bias=False)\n",
    "\n",
    "        self.c_k = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "\n",
    "        self.c_v = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim, bias=False)\n",
    "\n",
    "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "\n",
    "        logger.info(\"CausalSelfAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        logger.info(f\"CausalSelfAttentionã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=} {cos_sin[0].shape=} {cos_sin[0].dtype=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # 1) ã‚¯ã‚¨ãƒªãƒ»ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®—\n",
    "\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "\n",
    "        k = self.c_k(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        v = self.c_v(x).view(B, T, self.n_kv_head, self.head_dim)\n",
    "\n",
    "        # 2) ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã«RoPEã‚’é©ç”¨\n",
    "\n",
    "        cos, sin = cos_sin\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n",
    "\n",
    "        # 3) RMSNormã‚’é©ç”¨\n",
    "\n",
    "        q, k = norm(q), norm(k)\n",
    "\n",
    "        # 4) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é©ç”¨\n",
    "\n",
    "        # (B, T, H, D) -> (B, H, T, D)\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆï¼ˆæ¨è«–æ™‚ï¼‰\n",
    "        if kv_cache is not None:\n",
    "            # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ã—ã€ã“ã‚Œã¾ã§ã®å…¨ã¦ã®KVã‚’å–å¾—\n",
    "            k, v = kv_cache.insert_kv(self.layer_idx, k, v)\n",
    "            logger.debug(f\"KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° {k.shape=} {v.shape=}\")\n",
    "\n",
    "        # 5) ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‰ãƒ»ãƒ‰ãƒƒãƒˆãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒ»ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã®æ•°\n",
    "        Tq = q.size(2)\n",
    "        logger.debug(f\"{Tq=}\")\n",
    "\n",
    "        # ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼ã®æ•°ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…ã‚‚å«ã‚€ï¼‰\n",
    "        Tk = k.size(2)\n",
    "        logger.debug(f\"{Tk=}\")\n",
    "\n",
    "        # Attention: queries attend to keys/values autoregressively. A few cases to handle:\n",
    "\n",
    "        # GQAã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        enable_gqa = self.n_head != self.n_kv_head\n",
    "        logger.debug(f\"GQAã‚’æœ‰åŠ¹åŒ–: {enable_gqa=}\")\n",
    "\n",
    "        # è¨“ç·´æ™‚ã¾ãŸã¯ã‚¯ã‚¨ãƒªæ•°ã¨ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼æ•°ãŒç­‰ã—ã„å ´åˆ\n",
    "        if kv_cache is None or Tq == Tk:\n",
    "            logger.debug(\"é€šå¸¸ã®å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True, enable_gqa=enable_gqa)\n",
    "\n",
    "        # æ¨è«–æ™‚ã§ã‚¯ã‚¨ãƒªãŒ1ã¤ã ã‘ã®å ´åˆ\n",
    "        elif Tq == 1:\n",
    "            logger.debug(\"ãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ãªã„å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=False, enable_gqa=enable_gqa)\n",
    "\n",
    "        # æ¨è«–æ™‚ã§è¤‡æ•°ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹å ´åˆï¼ˆprefillï¼‰\n",
    "        else:\n",
    "            logger.debug(\"ãƒã‚¹ã‚¯ã‚’æ‰‹å‹•ã§ä½œæˆã—ã¦å› æœã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\")\n",
    "\n",
    "            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’åˆæœŸåŒ–\n",
    "            attn_mask = torch.zeros((Tq, Tk), dtype=torch.bool, device=q.device)\n",
    "            logger.debug(f\"{attn_mask.shape=}\")\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã®é•·ã•ã‚’å–å¾—\n",
    "            prefix_len = Tk - Tq\n",
    "            logger.debug(f\"{prefix_len=}\")\n",
    "\n",
    "            if prefix_len > 0:\n",
    "                # ãƒã‚¹ã‚¯ã®å·¦å´ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥éƒ¨åˆ†ï¼‰ã‚’ãƒã‚¹ã‚¯ã—ãªã„\n",
    "                attn_mask[:, :prefix_len] = True\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã®å³å´ã«ä¸‹ä¸‰è§’è¡Œåˆ—ã‚’è¨­å®š\n",
    "            attn_mask[:, prefix_len:] = torch.tril(\n",
    "                torch.ones((Tq, Tq), dtype=torch.bool, device=q.device)\n",
    "            )\n",
    "\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask, enable_gqa=enable_gqa)\n",
    "\n",
    "        # 6) å‡ºåŠ›å‡¦ç†\n",
    "\n",
    "        # (B, H, T, D) -> (B, T, H, D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ {y.shape=}\")\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "        logger.debug(f\"æœ€çµ‚ç·šå½¢å¤‰æ›ã‚’é©ç”¨ {y.shape=}\")\n",
    "\n",
    "        logger.info(f\"CausalSelfAttentionã®é †ä¼æ¬å®Œäº† {y.shape=}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257364b",
   "metadata": {},
   "source": [
    "### MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbcfed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFFNï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"MLPã‚’åˆæœŸåŒ– {config.n_embd=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        logger.info(\"MLPã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logger.info(f\"MLPã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=}\")\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        logger.debug(f\"{x.shape=}\")\n",
    "\n",
    "        # Squared ReLUæ´»æ€§åŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        # GELUã‚„SiLUï¼ˆSwishï¼‰ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„\n",
    "        x = F.relu(x).square()\n",
    "\n",
    "        x = self.c_proj(x)\n",
    "        logger.info(f\"MLPã®é †ä¼æ¬å®Œäº† {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd7f0a",
   "metadata": {},
   "source": [
    "### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformerãƒ–ãƒ­ãƒƒã‚¯ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx):\n",
    "        logger.info(f\"Transformer Blockã‚’åˆæœŸåŒ– {config.n_embd=} {layer_idx=}\")\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config, layer_idx)\n",
    "        self.mlp = MLP(config)\n",
    "        logger.info(\"Transformer Blockã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, x, cos_sin, kv_cache):\n",
    "        logger.info(f\"Transformer Blockã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {x.shape=} {x.dtype=} {cos_sin[0].shape=} {cos_sin[0].dtype=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        x = x + self.attn(norm(x), cos_sin, kv_cache)\n",
    "\n",
    "        x = x + self.mlp(norm(x))\n",
    "\n",
    "        logger.info(f\"Transformer Blockã®é †ä¼æ¬å®Œäº† {x.shape=}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac54f3e",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdec548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"\n",
    "    GPTãƒ¢ãƒ‡ãƒ«å…¨ä½“\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"GPTãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ– {config.n_layer=} {config.n_head=} {config.n_embd=} {config.vocab_size=} {config.sequence_len=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # Transformerã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼éƒ¨åˆ†\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            # å˜èªåŸ‹ã‚è¾¼ã¿å±¤\n",
    "            \"wte\": nn.Embedding(config.vocab_size, config.n_embd),\n",
    "\n",
    "            # Transformerãƒ–ãƒ­ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆ\n",
    "            \"h\": nn.ModuleList(\n",
    "                [Block(config, layer_idx) for layer_idx in range(config.n_layer)]\n",
    "            ),\n",
    "        })\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤ï¼ˆLanguage Model Headï¼‰\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # To support meta device initialization, we init the rotary embeddings here, but it's fake\n",
    "        # As for rotary_seq_len, these rotary embeddings are pretty small/cheap in memory,\n",
    "        # so let's just over-compute them, but assert fail if we ever reach that amount.\n",
    "        # In the future we can dynamically grow the cache, for now it's fine.\n",
    "\n",
    "        self.rotary_seq_len = config.sequence_len * 10 # 10X over-compute should be enough, TODO make nicer?\n",
    "        logger.debug(f\"{self.rotary_seq_len=}\")\n",
    "\n",
    "        head_dim = config.n_embd // config.n_head\n",
    "        logger.debug(f\"{head_dim=}\")\n",
    "\n",
    "        # RoPEã®cosã¨sinã‚’äº‹å‰è¨ˆç®—\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "\n",
    "        # cosã‚’ãƒãƒƒãƒ•ã‚¡ã«ç™»éŒ²ï¼ˆstate_dictã«ã¯ä¿å­˜ã—ãªã„ï¼‰\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "\n",
    "        # sinã‚’ãƒãƒƒãƒ•ã‚¡ã«ç™»éŒ²ï¼ˆstate_dictã«ã¯ä¿å­˜ã—ãªã„ï¼‰\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "        logger.info(\"GPTãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def init_weights(self):\n",
    "        logger.info(\"GPTãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¯¾ã—ã¦åˆæœŸåŒ–é–¢æ•°ã‚’é©ç”¨\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # å‡ºåŠ›å±¤ã®é‡ã¿ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–\n",
    "        torch.nn.init.zeros_(self.lm_head.weight)\n",
    "\n",
    "        # å…¨ã¦ã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›å±¤ã®é‡ã¿ã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–\n",
    "        for block in self.transformer.h:\n",
    "            torch.nn.init.zeros_(block.mlp.c_proj.weight)\n",
    "            torch.nn.init.zeros_(block.attn.c_proj.weight)\n",
    "\n",
    "        # RoPEã®cosã¨sinã‚’å†è¨ˆç®—\n",
    "        head_dim = self.config.n_embd // self.config.n_head\n",
    "        cos, sin = self._precompute_rotary_embeddings(self.rotary_seq_len, head_dim)\n",
    "        self.cos, self.sin = cos, sin\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®é‡ã¿ã‚’bfloat16ã«ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›\n",
    "        if self.transformer.wte.weight.device.type == \"cuda\":\n",
    "            self.transformer.wte.to(dtype=torch.bfloat16)\n",
    "\n",
    "        logger.info(\"GPTãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        logger.info(f\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é‡ã¿ã‚’åˆæœŸåŒ– {module.__class__.__name__=}\")\n",
    "\n",
    "        # å…¨çµåˆå±¤ã®å ´åˆ\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # è«–æ–‡ã«åŸºã¥ãæ–¹æ³•ã§é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "            # https://arxiv.org/pdf/2310.17813\n",
    "            fan_out = module.weight.size(0)\n",
    "            fan_in = module.weight.size(1)\n",
    "            std = 1.0 / math.sqrt(fan_in) * min(1.0, math.sqrt(fan_out / fan_in))\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã®å ´åˆ\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # æ¨™æº–æ­£è¦åˆ†å¸ƒã§é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=1.0)\n",
    "\n",
    "        logger.info(\"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®é‡ã¿ã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    # TODO: bump base theta more, e.g. 100K is more common more recently\n",
    "    def _precompute_rotary_embeddings(self, seq_len, head_dim, base=10000, device=None):\n",
    "        logger.info(f\"RoPEã®äº‹å‰è¨ˆç®—ã‚’å®Ÿè¡Œ {seq_len=} {head_dim=} {base=}\")\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "        if device is None:\n",
    "            # è‡ªå‹•æ¤œå‡º\n",
    "            device = self.transformer.wte.weight.device\n",
    "\n",
    "        # stride the channels\n",
    "        channel_range = torch.arange(0, head_dim, 2, dtype=torch.float32, device=device)\n",
    "\n",
    "        # é€†å‘¨æ³¢æ•°ã‚’è¨ˆç®—\n",
    "        inv_freq = 1.0 / (base ** (channel_range / head_dim))\n",
    "        logger.debug(f\"{inv_freq.shape=}\")\n",
    "\n",
    "        # stride the time steps\n",
    "        t = torch.arange(seq_len, dtype=torch.float32, device=device)\n",
    "        logger.debug(f\"{t.shape=}\")\n",
    "\n",
    "        # å‘¨æ³¢æ•°ã‚’è¨ˆç®—\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        logger.debug(f\"{freqs.shape=}\")\n",
    "\n",
    "        # cosã¨sinã‚’è¨ˆç®—\n",
    "        cos, sin = freqs.cos(), freqs.sin()\n",
    "\n",
    "        # bfloat16ã«å¤‰æ›\n",
    "        cos, sin = cos.bfloat16(), sin.bfloat16()\n",
    "\n",
    "        # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã®ãŸã‚ã«æ¬¡å…ƒã‚’è¿½åŠ \n",
    "        cos, sin = cos[None, :, None, :], sin[None, :, None, :]\n",
    "\n",
    "        logger.info(f\"RoPEã®äº‹å‰è¨ˆç®—å®Œäº† {cos.shape=} {sin.shape=}\")\n",
    "        return cos, sin\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.transformer.wte.weight.device\n",
    "\n",
    "    def estimate_flops(self):\n",
    "        \"\"\"\n",
    "        ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®FLOPsã‚’æ¨å®šã™ã‚‹\n",
    "        https://arxiv.org/abs/2204.02311\n",
    "        \"\"\"\n",
    "        logger.info(\"ãƒ¢ãƒ‡ãƒ«ã®FLOPsã‚’æ¨å®šé–‹å§‹\")\n",
    "\n",
    "        nparams = sum(p.numel() for p in self.parameters())\n",
    "        logger.debug(f\"{nparams=}\")\n",
    "\n",
    "        nparams_embedding = self.transformer.wte.weight.numel()\n",
    "        logger.debug(f\"{nparams_embedding=}\")\n",
    "\n",
    "        l, h, q, t = self.config.n_layer, self.config.n_head, self.config.n_embd // self.config.n_head, self.config.sequence_len\n",
    "        logger.debug(f\"{l=}, {h=}, {q=}, {t=}\")\n",
    "\n",
    "        num_flops_per_token = 6 * (nparams - nparams_embedding) + 12 * l * h * q * t\n",
    "\n",
    "        logger.info(f\"ãƒ¢ãƒ‡ãƒ«ã®FLOPsã®æ¨å®šå®Œäº† {num_flops_per_token=}\")\n",
    "        return num_flops_per_token\n",
    "\n",
    "    def setup_optimizers(self, unembedding_lr=0.004, embedding_lr=0.2, matrix_lr=0.02, weight_decay=0.0):\n",
    "        logger.info(f\"æœ€é©åŒ–é–¢æ•°ã‚’è¨­å®šé–‹å§‹ {unembedding_lr=} {embedding_lr=} {matrix_lr=} {weight_decay=}\")\n",
    "\n",
    "        model_dim = self.config.n_embd\n",
    "\n",
    "        ddp, rank, local_rank, world_size = get_dist_info()\n",
    "\n",
    "        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        matrix_params = list(self.transformer.h.parameters())\n",
    "        logger.debug(f\"{len(matrix_params)=}\")\n",
    "\n",
    "        # ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        embedding_params = list(self.transformer.wte.parameters())\n",
    "\n",
    "        # å‡ºåŠ›å±¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        lm_head_params = list(self.lm_head.parameters())\n",
    "\n",
    "        assert len(list(self.parameters())) == len(matrix_params) + len(embedding_params) + len(lm_head_params)\n",
    "\n",
    "        # Create the AdamW optimizer for the embedding and lm_head\n",
    "        # Scale the LR for the AdamW parameters by âˆ1/âˆšdmodel (having tuned the LRs for 768 dim model)\n",
    "\n",
    "        # åŸ‹ã‚è¾¼ã¿å±¤ã¨å‡ºåŠ›å±¤ã¯AdamWã‚’ä½¿ç”¨\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ã®æ¬¡å…ƒã«åŸºã¥ã„ã¦å­¦ç¿’ç‡ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        dmodel_lr_scale = (model_dim / 768) ** -0.5\n",
    "        if rank == 0:\n",
    "            printug(f\"Scaling the LR for the AdamW parameters âˆ1/âˆš({model_dim}/768) = {dmodel_lr_scale:.6f}\")\n",
    "\n",
    "        adam_groups = [\n",
    "            dict(params=lm_head_params, lr=unembedding_lr * dmodel_lr_scale),\n",
    "            dict(params=embedding_params, lr=embedding_lr * dmodel_lr_scale),\n",
    "        ]\n",
    "\n",
    "        adamw_kwargs = dict(betas=(0.8, 0.95), eps=1e-10, weight_decay=weight_decay)\n",
    "\n",
    "        AdamWFactory = DistAdamW if ddp else partial(torch.optim.AdamW, fused=True)\n",
    "\n",
    "        adamw_optimizer = AdamWFactory(adam_groups, **adamw_kwargs)\n",
    "\n",
    "        # è¡Œåˆ—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯Muonã‚’ä½¿ç”¨\n",
    "\n",
    "        muon_kwargs = dict(lr=matrix_lr, momentum=0.95)\n",
    "\n",
    "        MuonFactory = DistMuon if ddp else Muon\n",
    "\n",
    "        muon_optimizer = MuonFactory(matrix_params, **muon_kwargs)\n",
    "\n",
    "\n",
    "        # 2ã¤ã®æœ€é©åŒ–é–¢æ•°ã‚’1ã¤ã®ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹\n",
    "\n",
    "        optimizers = [adamw_optimizer, muon_optimizer]\n",
    "\n",
    "        for opt in optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                group[\"initial_lr\"] = group[\"lr\"]\n",
    "\n",
    "        logger.info(\"æœ€é©åŒ–é–¢æ•°ã®è¨­å®šå®Œäº†\")\n",
    "        return optimizers\n",
    "\n",
    "    def forward(self, idx, targets=None, kv_cache=None, loss_reduction='mean'):\n",
    "        logger.info(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œ {idx.shape=} {idx.dtype=} {targets.shape if targets is not None else None=} {kv_cache if kv_cache is not None else None=}\")\n",
    "\n",
    "        B, T = idx.size()\n",
    "\n",
    "        # RoPEã®æº–å‚™\n",
    "\n",
    "        # Grab the rotary embeddings for the current sequence length (they are of shape (1, seq_len, 1, head_dim))\n",
    "        assert T <= self.cos.size(1), f\"Sequence length grew beyond the rotary embeddings cache: {T} > {self.cos.size(1)}\"\n",
    "\n",
    "        assert idx.device == self.cos.device, f\"Rotary embeddings and idx are on different devices: {idx.device} != {self.cos.device}\"\n",
    "\n",
    "        assert self.cos.dtype == torch.bfloat16, \"Rotary embeddings must be in bfloat16\"\n",
    "\n",
    "        # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…ã®ä½ç½®ã«RoPEã‚’ã‚ªãƒ•ã‚»ãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹\n",
    "        T0 = 0 if kv_cache is None else kv_cache.get_pos()\n",
    "\n",
    "        # äº‹å‰è¨ˆç®—ã•ã‚ŒãŸRoPEã‚’ç¾åœ¨ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "        cos_sin = self.cos[:, T0:T0+T], self.sin[:, T0:T0+T]\n",
    "        logger.debug(f\"RoPEã®åˆ‡ã‚Šå‡ºã—å®Œäº† {cos_sin[0].shape=} {cos_sin[1].shape=}\")\n",
    "\n",
    "        # Transformerã®é †ä¼æ’­\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›\n",
    "        x = self.transformer.wte(idx)\n",
    "\n",
    "        # æœ€åˆã®RMSNormã‚’é©ç”¨ï¼ˆPre-LNï¼‰\n",
    "        x = norm(x)\n",
    "\n",
    "        # å„Transformerãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«é©ç”¨\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x, cos_sin, kv_cache)\n",
    "\n",
    "        # æœ€å¾Œã®RMSNormã‚’é©ç”¨\n",
    "        x = norm(x)\n",
    "\n",
    "        # å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "\n",
    "        # Logit Softcapping\n",
    "        # ãƒ­ã‚¸ãƒƒãƒˆã®å€¤ãŒ+/-15ã‚’è¶…ãˆãªã„ã‚ˆã†ã«åˆ¶é™ã™ã‚‹\n",
    "        softcap = 15\n",
    "\n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "        if targets is not None:\n",
    "\n",
    "            # å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            # logitsã«ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ã‚’é©ç”¨\n",
    "            logits = softcap * torch.tanh(logits / softcap)\n",
    "\n",
    "            # logitsã‚’float32ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "            logits = logits.float()\n",
    "\n",
    "            # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’è¨ˆç®—\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1),\n",
    "                ignore_index=-1,\n",
    "                reduction=loss_reduction\n",
    "            )\n",
    "\n",
    "            logger.info(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬å®Œäº†ï¼ˆè¨“ç·´ãƒ¢ãƒ¼ãƒ‰ï¼‰ {loss=}\")\n",
    "            return loss\n",
    "\n",
    "        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ\n",
    "        else:\n",
    "\n",
    "            # å‡ºåŠ›ã®ç·šå½¢å±¤ã‚’é©ç”¨\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            # logitsã«ã‚½ãƒ•ãƒˆã‚­ãƒ£ãƒƒãƒ—ã‚’é©ç”¨\n",
    "            logits = softcap * torch.tanh(logits / softcap)\n",
    "\n",
    "            logger.info(f\"GPTãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬å®Œäº†ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰ï¼‰ {logits.shape=}\")\n",
    "            return logits\n",
    "\n",
    "    @torch.inference_mode() # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "    def generate(self, tokens, max_tokens, temperature=1.0, top_k=None, seed=42):\n",
    "        \"\"\"\n",
    "        ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ\n",
    "\n",
    "        Naive autoregressive streaming inference.\n",
    "        To make it super simple, let's assume:\n",
    "        - batch size is 1\n",
    "        - ids and the yielded tokens are simple Python lists and ints\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ {tokens=} {max_tokens=} {temperature=} {top_k=} {seed=}\")\n",
    "\n",
    "        assert isinstance(tokens, list)\n",
    "\n",
    "        device = self.get_device()\n",
    "        logger.debug(f\"{device=}\")\n",
    "\n",
    "        rng = None\n",
    "\n",
    "        if temperature > 0:\n",
    "            rng = torch.Generator(device=device)\n",
    "            rng.manual_seed(seed)\n",
    "\n",
    "        ids = torch.tensor([tokens], dtype=torch.long, device=device) # add batch dim\n",
    "\n",
    "        # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¾ã§ç”Ÿæˆ\n",
    "        for _ in range(max_tokens):\n",
    "\n",
    "            # 1ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "            # (B, T, vocab_size)\n",
    "            logits = self.forward(ids)\n",
    "\n",
    "            # ç›´è¿‘ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾—\n",
    "            # (B, vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "            if top_k is not None:\n",
    "                # ä¸Šä½Kå€‹ä»¥å¤–ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’ç„¡é™å°ã«è¨­å®š\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            # æ¸©åº¦ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "            if temperature > 0:\n",
    "                # ç¢ºç‡åˆ†å¸ƒã‚’ãªã‚ã‚‰ã‹ã«ã—ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸æŠ\n",
    "                logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                next_ids = torch.multinomial(probs, num_samples=1, generator=rng)\n",
    "\n",
    "            # æ¸©åº¦ãŒ0ã®å ´åˆ\n",
    "            else:\n",
    "                # æœ€ã‚‚é«˜ã„ç¢ºç‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠ\n",
    "                next_ids = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "            # ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«è¿½åŠ \n",
    "            ids = torch.cat((ids, next_ids), dim=1)\n",
    "\n",
    "            # ç”Ÿæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’Pythonã®intã«å¤‰æ›\n",
    "            token = next_ids.item()\n",
    "\n",
    "            logger.info(f\"ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³: {token}\")\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae35199",
   "metadata": {},
   "source": [
    "## ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e5d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_distributed_data_loader(B, T, split, tokenizer_threads=4, tokenizer_batch_size=128, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€è¨“ç·´ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼\n",
    "\n",
    "    Args:\n",
    "        B (int): ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        T (int): ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "        split (str): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å‰² (\"train\" ã¾ãŸã¯ \"val\")\n",
    "        tokenizer_threads (int): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚¹ãƒ¬ãƒƒãƒ‰æ•°\n",
    "        tokenizer_batch_size (int): ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "        device (str): ãƒ‡ãƒã‚¤ã‚¹ (\"cuda\" ã¾ãŸã¯ \"cpu\")\n",
    "\n",
    "    Yields:\n",
    "        Tuple[Tensor, Tensor]: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¿ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "    logger.info(f\"åˆ†æ•£ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ– {B=} {T=} {split=} {tokenizer_threads=} {tokenizer_batch_size=} {device=}\")\n",
    "\n",
    "    assert split in [\"train\", \"val\"], \"split must be 'train' or 'val'\"\n",
    "\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "\n",
    "    # å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—\n",
    "    # æœ€å¾Œã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å«ã‚ã‚‹ãŸã‚1ã‚’åŠ ç®—\n",
    "    needed_tokens = B * T + 1\n",
    "    logger.debug(f\"{needed_tokens=}\")\n",
    "\n",
    "    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    # BOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—\n",
    "    bos_token = tokenizer.get_bos_token_id()\n",
    "\n",
    "    # scratch buffer holds the tokens for one iteration\n",
    "    token_buffer = deque() # we stream tokens on the right and pop from the left\n",
    "\n",
    "    # infinite iterator over document batches\n",
    "    def document_batches():\n",
    "        while True:\n",
    "            # batch will iterate in group size of the parquet files, usually e.g. 1024 rows\n",
    "            for batch in parquets_iter_batched(split=split, start=ddp_rank, step=ddp_world_size):\n",
    "                # for the tokenizer we might want to go in usually smaller batches, e.g. 128 rows\n",
    "                for i in range(0, len(batch), tokenizer_batch_size):\n",
    "                    yield batch[i:i+tokenizer_batch_size]\n",
    "    batches = document_batches()\n",
    "\n",
    "    batch_index = 0\n",
    "    while True:\n",
    "        # Accumulate enough tokens for one iteration before yielding.\n",
    "        while len(token_buffer) < needed_tokens:\n",
    "            doc_batch = next(batches)\n",
    "            token_lists = tokenizer.encode(doc_batch, prepend=bos_token, num_threads=tokenizer_threads)\n",
    "            for tokens in token_lists:\n",
    "                token_buffer.extend(tokens)\n",
    "            batch_index += 1\n",
    "        # Move tokens from the deque into the scratch buffer\n",
    "        tokens = [token_buffer.popleft() for _ in range(needed_tokens)]\n",
    "        # CUDA supports memory pinning for faster transfers between CPU and GPU:\n",
    "        scratch = torch.tensor(tokens, dtype=torch.int64, pin_memory=(device == \"cuda\"))\n",
    "        # Create the inputs/targets as 1D tensors\n",
    "        inputs_cpu = scratch[:-1].to(dtype=torch.int32)\n",
    "        targets_cpu = scratch[1:]\n",
    "        # Reshape to 2D and move to GPU async\n",
    "        inputs = inputs_cpu.view(B, T).to(device=device, dtype=torch.int32, non_blocking=True)\n",
    "        targets = targets_cpu.view(B, T).to(device=device, dtype=torch.int64, non_blocking=True)\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81361726",
   "metadata": {},
   "source": [
    "## ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3725fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of processes/GPUs to use\n",
    "# NPROC_PER_NODE=8\n",
    "NPROC_PER_NODE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e0235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User settings\n",
    "run = \"dummy\" # wandb run name default (\"dummy\" is special - we won't log to wandb)\n",
    "\n",
    "# Runtime\n",
    "device_type = \"\" # cuda|cpu|mps (empty => autodetect good device type default, in order: CUDA > MPS > CPU)\n",
    "\n",
    "# Model architecture\n",
    "depth = 20 # the depth of the Transformer model to train, rest of the kwargs are derived\n",
    "max_seq_len = 2048 # max context length\n",
    "# Training horizon. Only one of these 3 will be used, in this order of precedence.\n",
    "num_iterations = -1 # explicit number of steps of the optimization (-1 = disable)\n",
    "target_flops = -1.0 # calculate num_iterations to reach target_flops. Useful for scaling laws experiments (-1 = disable)\n",
    "target_param_data_ratio = 20 # calculate num_iterations to maintain fixed data:param ratio (Chinchilla=20) (-1 = disable)\n",
    "\n",
    "# Optimization\n",
    "# device_batch_size = 32 # per-device batch size (set to not OOM)\n",
    "device_batch_size = 4 # per-device batch size (set to not OOM)\n",
    "\n",
    "total_batch_size = 524288 # total desired batch size, in #tokens\n",
    "embedding_lr = 0.2 # learning rate for the embedding parameters (Adam)\n",
    "unembedding_lr = 0.004 # learning rate for the unembedding parameters (Adam)\n",
    "weight_decay = 0.0 # weight decay for the embedding/unembedding parameters (Adam)\n",
    "matrix_lr = 0.02 # learning rate for the matrix parameters (Muon)\n",
    "grad_clip = 1.0 # gradient clipping value (0.0 = disabled)\n",
    "warmup_ratio = 0.0 # ratio of iterations for LR warmup\n",
    "warmdown_ratio = 0.2 # ratio of iterations for LR warmdown\n",
    "final_lr_frac = 0.0 # final LR is this fraction of the initial LR\n",
    "# Evaluation\n",
    "eval_every = 250 # every how many steps to evaluate the model for val bpb\n",
    "eval_tokens = 20*524288 # number of tokens to evaluate val loss on\n",
    "core_metric_every = 2000 # every how many steps to evaluate the core metric (-1 = disable)\n",
    "core_metric_max_per_task = 500 # examples per task in estimating the core metric\n",
    "sample_every = 2000 # every how many steps to sample from the model\n",
    "# Output\n",
    "model_tag = \"\" # optionally override the model tag for the output checkpoint directory name\n",
    "# now allow CLI to override the settings via the configurator lol\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "# exec(open(os.path.join('nanochat', 'configurator.py')).read()) # overrides from command line or config file\n",
    "user_config = {k: globals()[k] for k in config_keys} # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autodetect_device_type():\n",
    "    # prefer to use CUDA if available, otherwise use MPS, otherwise fallback on CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = \"mps\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    print0(f\"Autodetected device type: {device_type}\")\n",
    "    return device_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12952377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_init(device_type=\"cuda\"): # cuda|cpu|mps\n",
    "    \"\"\"Basic initialization that we keep doing over and over, so make common.\"\"\"\n",
    "\n",
    "    assert device_type in [\"cuda\", \"mps\", \"cpu\"], \"Invalid device type atm\"\n",
    "    if device_type == \"cuda\":\n",
    "        assert torch.cuda.is_available(), \"Your PyTorch installation is not configured for CUDA but device_type is 'cuda'\"\n",
    "    if device_type == \"mps\":\n",
    "        assert torch.backends.mps.is_available(), \"Your PyTorch installation is not configured for MPS but device_type is 'mps'\"\n",
    "\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.manual_seed(42)\n",
    "    # skipping full reproducibility for now, possibly investigate slowdown later\n",
    "    # torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # Precision\n",
    "    if device_type == \"cuda\":\n",
    "        torch.set_float32_matmul_precision(\"high\") # uses tf32 instead of fp32 for matmuls\n",
    "\n",
    "    # Distributed setup: Distributed Data Parallel (DDP), optional, and requires CUDA\n",
    "    ddp, ddp_rank, ddp_local_rank, ddp_world_size = get_dist_info()\n",
    "    if ddp and device_type == \"cuda\":\n",
    "        device = torch.device(\"cuda\", ddp_local_rank)\n",
    "        torch.cuda.set_device(device)  # make \"cuda\" default to this device\n",
    "        dist.init_process_group(backend=\"nccl\", device_id=device)\n",
    "        dist.barrier()\n",
    "    else:\n",
    "        device = torch.device(device_type) # mps|cpu\n",
    "\n",
    "    if ddp_rank == 0:\n",
    "        logger.info(f\"Distributed world size: {ddp_world_size}\")\n",
    "\n",
    "    return ddp, ddp_rank, ddp_local_rank, ddp_world_size, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print0(s=\"\",**kwargs):\n",
    "    ddp_rank = int(os.environ.get('RANK', 0))\n",
    "    if ddp_rank == 0:\n",
    "        print(s, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute init\n",
    "device_type = autodetect_device_type() if device_type == \"\" else device_type\n",
    "ddp, ddp_rank, ddp_local_rank, ddp_world_size, device = compute_init(device_type)\n",
    "master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "autocast_ctx = torch.amp.autocast(device_type=device_type, dtype=torch.bfloat16) if device_type == \"cuda\" else nullcontext()\n",
    "synchronize = torch.cuda.synchronize if device_type == \"cuda\" else lambda: None\n",
    "get_max_memory = torch.cuda.max_memory_allocated if device_type == \"cuda\" else lambda: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077887d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb logging init\n",
    "\n",
    "class DummyWandb:\n",
    "    \"\"\"Useful if we wish to not use wandb but have all the same signatures\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def log(self, *args, **kwargs):\n",
    "        pass\n",
    "    def finish(self):\n",
    "        pass\n",
    "\n",
    "use_dummy_wandb = run == \"dummy\" or not master_process\n",
    "wandb_run = DummyWandb() if use_dummy_wandb else wandb.init(project=\"nanochat\", name=run, config=user_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bytes(device=\"cpu\"):\n",
    "    import torch\n",
    "    base_dir = get_base_dir()\n",
    "    tokenizer_dir = os.path.join(base_dir, \"tokenizer\")\n",
    "    token_bytes_path = os.path.join(tokenizer_dir, \"token_bytes.pt\")\n",
    "    assert os.path.exists(token_bytes_path), f\"Token bytes not found at {token_bytes_path}? It gets written by tok_train.py\"\n",
    "    with open(token_bytes_path, \"rb\") as f:\n",
    "        token_bytes = torch.load(f, map_location=device)\n",
    "    return token_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer will be useful for evaluation, also we need the vocab size\n",
    "tokenizer = get_tokenizer()\n",
    "token_bytes = get_token_bytes(device=device)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print0(f\"Vocab size: {vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model kwargs are derived from the desired depth of the model\n",
    "num_layers = depth\n",
    "model_dim = depth * 64 # aspect ratio 64 (usually this is varied from 64 -> 128 as model size increases)\n",
    "num_heads = max(1, (model_dim + 127) // 128) # head dim 128 (the division here is ceil div)\n",
    "num_kv_heads = num_heads # default is 1:1 GQA (Group Query Attention) ratio (i.e. GQA is disabled)\n",
    "print0(f\"num_layers: {num_layers}\")\n",
    "print0(f\"model_dim: {model_dim}\")\n",
    "print0(f\"num_heads: {num_heads}\")\n",
    "print0(f\"num_kv_heads: {num_kv_heads}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer / data / training length related hyperparameters\n",
    "# figure out the needed gradient accumulation to reach the desired total batch size\n",
    "tokens_per_fwdbwd = device_batch_size * max_seq_len # tokens per iteration for a single rank\n",
    "world_tokens_per_fwdbwd = tokens_per_fwdbwd * ddp_world_size # total tokens per iteration for all ranks\n",
    "assert total_batch_size % world_tokens_per_fwdbwd == 0\n",
    "grad_accum_steps = total_batch_size // world_tokens_per_fwdbwd\n",
    "print0(f\"Tokens / micro-batch / rank: {device_batch_size} x {max_seq_len} = {tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Tokens / micro-batch: {world_tokens_per_fwdbwd:,}\")\n",
    "print0(f\"Total batch size {total_batch_size:,} => gradient accumulation steps: {grad_accum_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7639e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Model\n",
    "model_config_kwargs = dict(sequence_len=max_seq_len, vocab_size=vocab_size, n_layer=num_layers, n_head=num_heads, n_kv_head=num_kv_heads, n_embd=model_dim)\n",
    "with torch.device(\"meta\"):\n",
    "    model_config = GPTConfig(**model_config_kwargs)\n",
    "    model = GPT(model_config)\n",
    "model.to_empty(device=device)\n",
    "model.init_weights()\n",
    "orig_model = model # original, uncompiled model, for saving raw model state_dict\n",
    "model = torch.compile(model, dynamic=False) # TODO: dynamic True/False think through\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print0(f\"Number of parameters: {num_params:,}\")\n",
    "num_flops_per_token = model.estimate_flops()\n",
    "print0(f\"Estimated FLOPs per token: {num_flops_per_token:e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of iterations. Either it is given, or from target flops, or from target data:param ratio (in that order)\n",
    "assert num_iterations > 0 or target_param_data_ratio > 0 or target_flops > 0\n",
    "if num_iterations > 0:\n",
    "    print0(f\"Using user-provided number of iterations: {num_iterations:,}\")\n",
    "elif target_flops > 0:\n",
    "    # calculate the number of iterations from the target flops\n",
    "    num_iterations = round(target_flops / (num_flops_per_token * total_batch_size))\n",
    "    print0(f\"Calculated number of iterations from target FLOPs: {num_iterations:,}\")\n",
    "elif target_param_data_ratio > 0:\n",
    "    # calculate the number of iterations from the target param data ratio\n",
    "    target_tokens = target_param_data_ratio * num_params\n",
    "    num_iterations = target_tokens // total_batch_size\n",
    "    print0(f\"Calculated number of iterations from target data:param ratio: {num_iterations:,}\")\n",
    "else:\n",
    "    raise ValueError(\"No training horizon specified\")\n",
    "total_tokens = total_batch_size * num_iterations\n",
    "print0(f\"Total number of training tokens: {total_tokens:,}\")\n",
    "print0(f\"Tokens : Params ratio: {total_batch_size * num_iterations / num_params:.2f}\") # Chinchilla is ~20\n",
    "print0(f\"Total training FLOPs estimate: {num_flops_per_token * total_tokens:e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74514c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Initialize the Optimizer (Muon for Linear layers, AdamW for embedding and lm_head)\n",
    "optimizers = model.setup_optimizers(unembedding_lr=unembedding_lr, embedding_lr=embedding_lr, matrix_lr=matrix_lr, weight_decay=weight_decay)\n",
    "adamw_optimizer, muon_optimizer = optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataLoaders for train/val\n",
    "base_dir = get_base_dir()\n",
    "tokens_dir = os.path.join(base_dir, \"tokenized_data\")\n",
    "train_loader = tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"train\", device=device)\n",
    "build_val_loader = lambda: tokenizing_distributed_data_loader(device_batch_size, max_seq_len, split=\"val\", device=device)\n",
    "x, y = next(train_loader) # kick off load of the very first batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Set up hyperparameter schedulers\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr_multiplier(it):\n",
    "    warmup_iters = round(warmup_ratio * num_iterations)\n",
    "    warmdown_iters = round(warmdown_ratio * num_iterations)\n",
    "    if it < warmup_iters:\n",
    "        return (it + 1) / warmup_iters\n",
    "    elif it <= num_iterations - warmdown_iters:\n",
    "        return 1.0\n",
    "    else:\n",
    "        progress = (num_iterations - it) / warmdown_iters\n",
    "        return progress * 1.0 + (1 - progress) * final_lr_frac\n",
    "\n",
    "# Momentum scheduler for Muon optimizer\n",
    "def get_muon_momentum(it):\n",
    "    frac = min(it / 300, 1)\n",
    "    momentum = (1 - frac) * 0.85 + frac * 0.95\n",
    "    return momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca4be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bpb(model, batches, steps, token_bytes):\n",
    "    \"\"\"\n",
    "    Instead of the naive 'mean loss', this function returns the bits per byte (bpb),\n",
    "    which is a tokenization vocab size-indepedent metric, meaning you are still comparing\n",
    "    apples:apples if you change the vocab size. The way this works is that instead of just\n",
    "    calculating the average loss as usual, you calculate the sum loss, and indepependently\n",
    "    also the sum bytes (of all the target tokens), and divide. This normalizes the loss by\n",
    "    the number of bytes that the target tokens represent.\n",
    "\n",
    "    The added complexity is so that:\n",
    "    1) All \"normal\" tokens are normalized by the length of the token in bytes\n",
    "    2) No special tokens (e.g. <|bos|>) are included in the metric - they are masked out.\n",
    "    3) No actively masked tokens (using ignore_index of e.g. -1) are included in the metric.\n",
    "\n",
    "    In addition to evaluate_loss, we need the token_bytes tensor:\n",
    "    It is a 1D tensor of shape (vocab_size,), indicating the number of bytes for\n",
    "    each token id, or 0 if the token is to not be counted (e.g. special tokens).\n",
    "    \"\"\"\n",
    "    # record the losses\n",
    "    total_nats = torch.tensor(0.0, dtype=torch.float32, device=model.get_device())\n",
    "    total_bytes = torch.tensor(0, dtype=torch.int64, device=model.get_device())\n",
    "    batch_iter = iter(batches)\n",
    "    for _ in range(steps):\n",
    "        x, y = next(batch_iter)\n",
    "        loss2d = model(x, y, loss_reduction='none') # (B, T)\n",
    "        loss2d = loss2d.view(-1) # flatten\n",
    "        y = y.view(-1) # flatten\n",
    "        if (y.int() < 0).any(): # mps does not currently have kernel for < 0 for int64, only int32\n",
    "            # slightly more complex code path if some target tokens are ignore_index (e.g. -1)\n",
    "            # any target token < 0 is to be ignored: do NOT index token_bytes with negatives\n",
    "            valid = y >= 0\n",
    "            y_safe = torch.where(valid, y, torch.zeros_like(y))\n",
    "            # map valid targets to their byte length; ignored targets contribute 0 bytes\n",
    "            num_bytes2d = torch.where(\n",
    "                valid,\n",
    "                token_bytes[y_safe],\n",
    "                torch.zeros_like(y, dtype=token_bytes.dtype)\n",
    "            )\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "        else:\n",
    "            # fast path: no ignored targets, safe to index directly\n",
    "            num_bytes2d = token_bytes[y]\n",
    "            total_nats += (loss2d * (num_bytes2d > 0)).sum()\n",
    "            total_bytes += num_bytes2d.sum()\n",
    "    # sum reduce across all ranks\n",
    "    world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    if world_size > 1:\n",
    "        dist.all_reduce(total_nats, op=dist.ReduceOp.SUM)\n",
    "        dist.all_reduce(total_bytes, op=dist.ReduceOp.SUM)\n",
    "    # move both to cpu, calculate bpb and return\n",
    "    total_nats = total_nats.item()\n",
    "    total_bytes = total_bytes.item()\n",
    "    if total_bytes == 0:\n",
    "        return float('inf')\n",
    "    bpb = total_nats / (math.log(2) * total_bytes)\n",
    "    return bpb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Training loop\n",
    "min_val_bpb = float(\"inf\")\n",
    "smooth_train_loss = 0 # EMA of training loss\n",
    "ema_beta = 0.9 # EMA decay factor\n",
    "total_training_time = 0 # total wall-clock time of training\n",
    "# note that we run +1 steps only so that we can eval and save at the end\n",
    "# for step in range(num_iterations + 1):\n",
    "for step in range(3):\n",
    "    last_step = step == num_iterations\n",
    "    flops_so_far = num_flops_per_token * total_batch_size * step\n",
    "\n",
    "    # once in a while: evaluate the val bpb (all ranks participate)\n",
    "    if last_step or step % eval_every == 0:\n",
    "        model.eval()\n",
    "        val_loader = build_val_loader()\n",
    "        eval_steps = eval_tokens // (device_batch_size * max_seq_len * ddp_world_size)\n",
    "        with autocast_ctx:\n",
    "            val_bpb = evaluate_bpb(model, val_loader, eval_steps, token_bytes)\n",
    "        print0(f\"Step {step:05d} | Validation bpb: {val_bpb:.4f}\")\n",
    "        if val_bpb < min_val_bpb:\n",
    "            min_val_bpb = val_bpb\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"val/bpb\": val_bpb,\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: estimate the CORE metric (all ranks participate)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    results = {}\n",
    "    if core_metric_every > 0 and (last_step or (step > 0 and step % core_metric_every == 0)):\n",
    "        model.eval()\n",
    "        with autocast_ctx:\n",
    "            results = evaluate_model(orig_model, tokenizer, device, max_per_task=core_metric_max_per_task)\n",
    "        print0(f\"Step {step:05d} | CORE metric: {results['core_metric']:.4f}\")\n",
    "        wandb_run.log({\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"core_metric\": results[\"core_metric\"],\n",
    "            \"centered_results\": results[\"centered_results\"],\n",
    "        })\n",
    "        model.train()\n",
    "\n",
    "    # once in a while: sample from the model (only on master process)\n",
    "    # use the original uncompiled model because the inputs keep changing shape\n",
    "    if master_process and (last_step or (step > 0 and step % sample_every == 0)):\n",
    "        model.eval()\n",
    "        prompts = [\n",
    "            \"The capital of France is\",\n",
    "            \"The chemical symbol of gold is\",\n",
    "            \"If yesterday was Friday, then tomorrow will be\",\n",
    "            \"The opposite of hot is\",\n",
    "            \"The planets of the solar system are:\",\n",
    "            \"My favorite color is\",\n",
    "            \"If 5*x + 3 = 13, then x is\",\n",
    "        ]\n",
    "        engine = Engine(orig_model, tokenizer) # use orig_model to avoid recompilation\n",
    "        for prompt in prompts:\n",
    "            tokens = tokenizer(prompt, prepend=\"<|bos|>\")\n",
    "            with autocast_ctx:\n",
    "                sample, _ = engine.generate_batch(tokens, num_samples=1, max_tokens=16, temperature=0)\n",
    "            print0(tokenizer.decode(sample[0]))\n",
    "        model.train()\n",
    "\n",
    "    # save checkpoint at the end of the run (only on master process)\n",
    "    if master_process and last_step:\n",
    "        output_dirname = model_tag if model_tag else f\"d{depth}\" # e.g. d12\n",
    "        checkpoint_dir = os.path.join(base_dir, \"base_checkpoints\", output_dirname)\n",
    "        save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            step,\n",
    "            orig_model.state_dict(),\n",
    "            [opt.state_dict() for opt in optimizers], # TODO: make sure saving across ranks is done correctly\n",
    "            {\n",
    "                \"step\": step,\n",
    "                \"val_bpb\": val_bpb, # loss at last step\n",
    "                \"model_config\": model_config_kwargs,\n",
    "                \"user_config\": user_config, # inputs to the training script\n",
    "                \"device_batch_size\": device_batch_size,\n",
    "                \"max_seq_len\": max_seq_len,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if last_step:\n",
    "        break\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # single training step\n",
    "    # evaluate the gradient\n",
    "    synchronize()\n",
    "    t0 = time.time()\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        with autocast_ctx:\n",
    "            loss = model(x, y)\n",
    "        train_loss = loss.detach() # for logging\n",
    "        loss = loss / grad_accum_steps # each .backward() is a grad sum => normalize loss here\n",
    "        loss.backward()\n",
    "        x, y = next(train_loader) # prefetch the next batch while the GPU is busy with forward/backward\n",
    "    # gradient clipping\n",
    "    grad_clip_enabled = grad_clip > 0.0\n",
    "    if grad_clip_enabled:\n",
    "        grad_norm_tensor = torch.nn.utils.clip_grad_norm_(orig_model.parameters(), grad_clip)\n",
    "        grad_norm = grad_norm_tensor.item() # GPU tensor -> CPU float (note: cpu-gpu sync point)\n",
    "    # step the optimizers\n",
    "    lrm = get_lr_multiplier(step)\n",
    "    for opt in optimizers:\n",
    "        for group in opt.param_groups:\n",
    "            group[\"lr\"] = group[\"initial_lr\"] * lrm\n",
    "    muon_momentum = get_muon_momentum(step)\n",
    "    for group in muon_optimizer.param_groups:\n",
    "        group[\"momentum\"] = muon_momentum\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    synchronize()\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # logging\n",
    "    smooth_train_loss = ema_beta * smooth_train_loss + (1 - ema_beta) * train_loss.item() # EMA the training loss\n",
    "    debiased_smooth_loss = smooth_train_loss / (1 - ema_beta**(step + 1)) # debias the EMA\n",
    "    pct_done = 100 * step / num_iterations\n",
    "    tok_per_sec = int(total_batch_size / dt)\n",
    "    flops_per_sec = num_flops_per_token * total_batch_size / dt\n",
    "    promised_flops_per_sec_h100 = 989e12 * ddp_world_size # bfloat16 H100 SXM and without 2:4 sparsity\n",
    "    mfu = 100 * flops_per_sec / promised_flops_per_sec_h100 # in %\n",
    "    if step > 10:\n",
    "        total_training_time += dt # only count the time after the first 10 steps\n",
    "    print_grad_norm = f\" grad norm: {grad_norm:.4f} |\" if grad_clip_enabled else \"\"\n",
    "    print0(f\"step {step:05d}/{num_iterations:05d} ({pct_done:.2f}%) | loss: {debiased_smooth_loss:.6f} |{print_grad_norm} lrm: {lrm:.2f} | dt: {dt * 1000:.2f}ms | tok/sec: {tok_per_sec:,} | mfu: {mfu:.2f} | total time: {total_training_time/60:.2f}m\")\n",
    "    if step % 100 == 0:\n",
    "        log_data = {\n",
    "            \"step\": step,\n",
    "            \"total_training_flops\": flops_so_far,\n",
    "            \"total_training_time\": total_training_time,\n",
    "            \"train/loss\": debiased_smooth_loss,\n",
    "            \"train/lrm\": lrm,\n",
    "            \"train/dt\": dt,\n",
    "            \"train/tok_per_sec\": tok_per_sec,\n",
    "            \"train/mfu\": mfu,\n",
    "        }\n",
    "        if grad_clip_enabled:\n",
    "            log_data[\"train/grad_norm\"] = grad_norm\n",
    "        wandb_run.log(log_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
